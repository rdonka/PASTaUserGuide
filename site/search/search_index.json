{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PASTa : P hotometry A nalysis and S ignal Processing T oolbox Welcome to the documentation site for PASTa ( P hotometry A nalysis and S ignal Processing T oolbox)! The PASTa protocol is an open source toolbox and protocol for the preparation, signal processing, and analysis of fiber photometry data. Fiber photometry is a rapidly growing technique to record real-time neural signaling in awake, behaving subjects. However, the processing and analysis of photometry data streams can be complicated, and there is wide divergence in methods across the field. While several open-source signal processing tools exist, platforms can be inflexible in accommodating experimental designs, lack consistency in signal peak detection, and be difficult to use for naive users. To remedy these challenges, we developed PASTa, an open-source MATLAB based toolbox and protocol for the processing and analysis of fiber photometry data. PASTa includes a full analysis pipeline from data preparation through signal processing and transient event detection. Default parameters were selected to provide users with a conservative starting place, with optional inputs to include other commonly used methods and techniques in the field. Additionally, the transient detection protocol adopts options for determining peak detection thresholds and pre-peak baselines to allow more reliable detection of events and characterization of transient kinetics. The pipeline is designed to process multiple sessions at once, automating data processing, analysis, and plot creation to ensure application consistency across experimental sessions. While operating through MATLAB, the code is annotated to be readable, accessible for new users, and adaptable. Here you'll find a detailed user guide for each stage of PASTa, including installation instructions, detailed guidelines for each stage of the signal processing and analysis pipeline, example analyses, and additional details on function inputs and usage to support the documenation available within each function. Navigating the PASTa Repository and Documentation The PASTa code is openly available on GitHub . The toolbox is also available as a toolbox through MATLAB File Exchange . This user guide contains detailed documentation, instructions for function use, and troubleshooting tips. The Getting Started page includes quick start instructions and installation tips. This is a great place to start if you haven't previously downloaded or implemented the PASTa Protocol on your device. User Guide The User Guide page includes detailed instructions and examples for every step of the protocol, including Data Preparation , Signal Processing , and Transient Analysis . This is the best place to start for new users. Each page will walk you through each step, with information about the reccomended settings and any defaults built in to the protocol. The guide also includes examples of each step with fiber photometry recordings with three sensors capturing mesolimbic dopamine dynamics to help users gain an intuition for what each phase of the process is doing to the data streams. Function Documentation For using and troubleshooting individual functions, the Function Documentation page includes even more details about the use of each function in the toolbox, including notes and suggestions for troubleshooting errors. Detailed documentation is also commented in to each function in the source code - just view the help window or open the function source file in MATLAB to read. Example Analyses Additionally, to provide an example of the full pipeline, the Example Analyses page provides users with full examples of experimental analyses. All files needed to perform the example analyses are available for download so users can walk through each step in MATLAB on their own devices to ensure that installation and set up were sucessful, and view outputs of a validated script. Contact Us Please feel free to reach out with questions, feedback, and feature requests. PASTa is actively maintained, and will continue to integrate new features into future versions. Version releases will be continually added to GitHub, and details are also available on the Version Release Notes page. To facilitate community discussion, we are actively maintaining the Discussions Forum via GitHub, which includes channels for announcements, feature requests, troubleshooting, and general discussions. Our individual contact information is also available here .","title":"Home"},{"location":"#pasta-photometry-analysis-and-signal-processing-toolbox","text":"Welcome to the documentation site for PASTa ( P hotometry A nalysis and S ignal Processing T oolbox)! The PASTa protocol is an open source toolbox and protocol for the preparation, signal processing, and analysis of fiber photometry data. Fiber photometry is a rapidly growing technique to record real-time neural signaling in awake, behaving subjects. However, the processing and analysis of photometry data streams can be complicated, and there is wide divergence in methods across the field. While several open-source signal processing tools exist, platforms can be inflexible in accommodating experimental designs, lack consistency in signal peak detection, and be difficult to use for naive users. To remedy these challenges, we developed PASTa, an open-source MATLAB based toolbox and protocol for the processing and analysis of fiber photometry data. PASTa includes a full analysis pipeline from data preparation through signal processing and transient event detection. Default parameters were selected to provide users with a conservative starting place, with optional inputs to include other commonly used methods and techniques in the field. Additionally, the transient detection protocol adopts options for determining peak detection thresholds and pre-peak baselines to allow more reliable detection of events and characterization of transient kinetics. The pipeline is designed to process multiple sessions at once, automating data processing, analysis, and plot creation to ensure application consistency across experimental sessions. While operating through MATLAB, the code is annotated to be readable, accessible for new users, and adaptable. Here you'll find a detailed user guide for each stage of PASTa, including installation instructions, detailed guidelines for each stage of the signal processing and analysis pipeline, example analyses, and additional details on function inputs and usage to support the documenation available within each function.","title":"PASTa: Photometry Analysis and Signal Processing Toolbox"},{"location":"#navigating-the-pasta-repository-and-documentation","text":"The PASTa code is openly available on GitHub . The toolbox is also available as a toolbox through MATLAB File Exchange . This user guide contains detailed documentation, instructions for function use, and troubleshooting tips. The Getting Started page includes quick start instructions and installation tips. This is a great place to start if you haven't previously downloaded or implemented the PASTa Protocol on your device.","title":"Navigating the PASTa Repository and Documentation"},{"location":"#user-guide","text":"The User Guide page includes detailed instructions and examples for every step of the protocol, including Data Preparation , Signal Processing , and Transient Analysis . This is the best place to start for new users. Each page will walk you through each step, with information about the reccomended settings and any defaults built in to the protocol. The guide also includes examples of each step with fiber photometry recordings with three sensors capturing mesolimbic dopamine dynamics to help users gain an intuition for what each phase of the process is doing to the data streams.","title":"User Guide"},{"location":"#function-documentation","text":"For using and troubleshooting individual functions, the Function Documentation page includes even more details about the use of each function in the toolbox, including notes and suggestions for troubleshooting errors. Detailed documentation is also commented in to each function in the source code - just view the help window or open the function source file in MATLAB to read.","title":"Function Documentation"},{"location":"#example-analyses","text":"Additionally, to provide an example of the full pipeline, the Example Analyses page provides users with full examples of experimental analyses. All files needed to perform the example analyses are available for download so users can walk through each step in MATLAB on their own devices to ensure that installation and set up were sucessful, and view outputs of a validated script.","title":"Example Analyses"},{"location":"#contact-us","text":"Please feel free to reach out with questions, feedback, and feature requests. PASTa is actively maintained, and will continue to integrate new features into future versions. Version releases will be continually added to GitHub, and details are also available on the Version Release Notes page. To facilitate community discussion, we are actively maintaining the Discussions Forum via GitHub, which includes channels for announcements, feature requests, troubleshooting, and general discussions. Our individual contact information is also available here .","title":"Contact Us"},{"location":"contactus/","text":"Contact Us PASTa is actively managed by our team, and updates will be continual to add new features, address user feedback, and adapt to the every changing experimental landscape. For more information, or to request new features, please feel free to reach out! For feature requests, help with troubleshooting, and general discussion, please make use of our discussion forums on GitHub. This will allow us to build an open resource center with comments and feedback from all users. Visit the forums under the tab Discussions on our GitHub . To contact our team directly, email us at pastaprotocol@gmail.com","title":"Contact Us"},{"location":"contactus/#contact-us","text":"PASTa is actively managed by our team, and updates will be continual to add new features, address user feedback, and adapt to the every changing experimental landscape. For more information, or to request new features, please feel free to reach out! For feature requests, help with troubleshooting, and general discussion, please make use of our discussion forums on GitHub. This will allow us to build an open resource center with comments and feedback from all users. Visit the forums under the tab Discussions on our GitHub . To contact our team directly, email us at pastaprotocol@gmail.com","title":"Contact Us"},{"location":"exampleanalyses/","text":"Example Analysis: Injection Transients PASTa includes an example analysis with fiber photometry recordings to help users get comfortable with the workflow and observe the effects of each stage of processing on the signal. The analysis script is available on GitHub, and the data are freely accessible on (Box)[https://uofi.app.box.com/s/pqpa8286yi6oia1ocjts7qytb1wetgpv/folder/256973884209] at https://uofi.app.box.com/s/pqpa8286yi6oia1ocjts7qytb1wetgpv/folder/256973884209 under Example Analyses . The subfolder Injection Transients is prepared for the analysis, containing the file and subject keys, as well as folders created for extracted data, analysis, and figure outputs. Raw data blocks collected via Synapse (Tucker Davis Technologies) are nested in the Raw Data folder.The completed analysis output files including extracted data structures, transient quantification, and plots are also available for download. If you have any questions or run into problems accessing the files, please feel free to contact us . Methods Overview Example data are provided from two subjects. Each subject has two fiber photometry sessions conducted on consecutive days: saline and morphine (10mg/kg). Each session consists of a fifteen minute baseline, an intraperitoneal injection, and a 60 minute post-injection recording period. Fiber photometry data were recorded using Tucker Davis Technologies RZ10X processor, with a 465nm excitation wavelength (signal; 'sig' ) and a 405nm isosbestic control wavelength (background, 'baq' ). Injection start and end time points are marked by epochs ( 'injt' ) sent through Med Associates equipment and stored by Synapse as time stamps. Animals Adult male and female Long Evans rats (n = 2) expressing Cre recombinase under the control of the tyrosine hydroxylase promoter (TH:Cre+; [34]) were paired housed within a temperature and humidity-controlled vivarium on a 12:12 hour light-dark cycle with lights on at 7:00 am and maintained on ad libitum food and water for the direction of all experiments. All procedures were conducted in accordance with guidelines set by the National Institutes for Health Guide for the Care and Use of Laboratory Animals and approved by the Animal Care Committee of the University of Illinois Chicago. Recordings were conducted during the light portion of the cycle. Stereotaxic Surgery Procedures Rats were anesthetized via isoflurane (induction: 4%; maintenance: ~2%) and injected with meloxicam (1mg/kg, subcutaneous) prior to the start of stereotaxic surgery. To isolate and record from dopamine neuronal cell bodies, AAV1.hSyn.Flex.GCaMP6f.WPRE.SV40 was targeted to the VTA (Anterior/Posterior (AP): - 5.40mm, Medial/Lateral (ML): +0.70mm, Dorsal/Ventral (DV): -8.15mm, relative to Bregma) of TH:Cre+ rats. Viral injections were performed with a volume of 1\u03bcL at a rate of 0.1\u03bcL/minute with a blunt 30-gauge injection needle mated to a Hamilton syringe via PEEK tubing and connected to a syringe pump. A 5 minute post-injection period followed to ensure complete virus diffusion. An optic fiber (flat 400-\u03bcm core, 0.48 NA, Doric Lenses, Inc.) was implanted above the injection site and secured to three implanted skull screws via metabond and dental cement. Fiber Photometry Data Collection Fiber photometry recordings were performed as in Konanur et al 2020, Hsu et al 2020, and Konanur et al 2024, and Loh et al 2025 [35-38]. Data were collected using data acquisition systems from Tucker Davis Technologies (RZ10X). Implanted fiber optic cannulae of subjects were mated to a patch cord, which was connected through a commutator to a filter cube (FMC4, Doric Lenses). Emitted fluorescence was collected by an external photoreceiver (Visible Femtowatt Photoreceiver Model 2151; Newport) and passed to the data acquisition system for live visualization and recording. LEDs administered 465nm and 405nm wavelengths at a total power output of 30\u03bcW. Light was sinusoidally modulated at 211 and 531 Hz respectively. Lock in amplification was used to demodulate the fluorescence by emission wavelength. Fiber photometry recording sessions were conducted on consecutive days, with the first session consisting of a control saline treatment (1mL/kg, intraperitoneal (i.p.); figure panel A) and the second session consisting of acute morphine treatment (10mg/kg morphine sulfate, i.p.; figure panel B). Each test session consisted of a 15 minute pre-injection baseline, followed by a 60-minute post-injection recording period. Signal processing was performed with PASTa protocol defaults. Subtracted and filtered data streams were normalized to z-score based on the pre-injection baseline period. Transient events were detected at a threshold of 2.6 SDs using PASTa protocol default parameters. Analysis Script The full analysis script is available via the (PASTa GitHub)[https://github.com/rdonka/PASTa] under the subfolder \"Example Analyses\" . Download the file \"main_ExampleAnalysis_InjectionTransients.m\". Data Preparation The first section of the script sets up paths and analysis key inputs. To enable users to switch computers easily, paths are created without the computer and user specific portion. The computer user specific root directory of the path is input to the variable rootdirectory and appended to subsequently needed paths. For users accessing PASTa via cloning the repository, to access files and functions, paths need to be added to MATLAB via the addpath function. genpath is used within addpath to ensure folders and subfolders at the input path are added. Finally, the names of created Subject and File Keys are added to variables subjectkeyname and filekeyname . These keys contain the session specific and subject specific information needed to load the data and analyze the results. For details on keys, see the user guide section on Data Preparation . The keys are loaded using the function createExperimentKey to join the subject specific data to the session specific information contained in the file key. Paths to raw data blocks and extracted output locations are added to string arrays, which are input to the function extractTDTdata . extractTDTdata first extracts the raw data for each file into a data structure, trims the first 5 seconds of the recording, and then saves the MATLAB data structure to extracted output location. Extracted structures for each file are then loaded by the function loadKeydata . After data are loaded, excess samples before the start of the program and after the end of the post-injection period are cropped from the streams. First, session start and end indices are prepared for each file. These indices are input to the function cropFPdata , which removes the excess samples from the signal and background streams, and adjusts the injection and session start time stamp epocs. Signal Processing To control for motion artifacts and photobleaching, the 405 nm channel ( 'baq' ) is subtracted from the 465 nm channel ( 'sig' ) with the function subtractFPdata . The example analysis script uses the default 'frequency' scaling method (see the user guide section (Signal Processing)[https://rdonka.github.io/PASTaUserGuide/userguide/signalprocessing/] for full details and alternative scaling options). First, subtractFPdata scales the background stream to the signal stream. After scaling, the background is subtracted from the signal in the time domain, and filtered with a Butterworth bandpass filter to remove the DC offset as well as high frequency noise (frequencies outside the range of biological interest). The scaling factor ( 'baqscalingfactor' ), subtracted signal ( 'sigsub' ), filtered signal ( 'sigfilt' ) are added to the data structure. Raw, subtracted, and filtered streams for the whole session are visualized and saved to the output folder 'Figures' . Additionally, frequency magnitude plots of the FFTs of the raw, scaled, subtracted, and filtered streams are generated and saved to the 'Figures' folder. Normalization After subtraction, the filtered signal is normalized. To normalize to the entire session, the function normSession is called, outputting the z scored signal streams to 'sigfiltz_normsession' . To normalize to pre injection baseline, the function normBaseline is called, which uses the pre-injection period to normalize the entire session, outputting the z scored streams to 'sigfiltz_normbaseline' . Finally, a for loop is used to create streams with the injection time period removed. For details on the signal processing functions and methods, see the user guide section on Signal Processing . Transient Detection To identify transient events, the findTransients function is used. This function detects relevant transients in the normalized data stream based on an amplitude inclusion criterion (eg, 2.6 SD) relative to the pre-event baseline. The example analysis uses the default pre-event baseline type (baseline window mean; 'blmean' ) and baseline window parameters. See the user guide section (Transient Analysis)[https://rdonka.github.io/PASTaUserGuide/userguide/transientanalysis/] for more details and optional input parameters. Transient events are output to a separate data structure to facilitate analysis. After detection, transients are quantified by frequency, amplitude, half height rise time, half height fall time, half height width, and half height AUC. After detection and quantification, transients are binned into 5 minute (default) increments. The function binTransients identifies and adds the bin of each transient event to the transientquantificiation table. Multiple plots are generated to visualize transient events and figures are saved to the Figures folder. Whole session trace with all transient events: Individual bin traces with identified transients: Overlaid Transient Traces from the Whole Session: Overlaid Transient Traces by Bin: To export the transient events for statistical analysis, the summarizeTransients and summarizeBinTransients functions are used to generate the session means of transient event quantification parameters. Results Overall, morphine administration significantly increased the frequency (figure panel C) and amplitude (figure panel E) of VTA DA activity transients relative to the saline control session. The increase in transient frequency and amplitude was observed approximately 10 minutes post-injection (figure panels D and F), and remained elevated relative to both pre-injection baseline and the matched saline control session through the end of the 60-minute post-injection recording. Transient Frequency and Amplitude Results. Example analysis of VTA DA transient frequency and amplitude after acute morphine administration. Representative examples of pre-injection baseline normalized traces from a single subject before and after A) an injection of saline (control) and B) an injection of morphine (10mg/kg). The vertical dashed line indicates time of injection. C) Mean transient frequency (transients per minute) across the entire session. D) Transient frequency binned in 5-minute time windows. E) Mean transient amplitude (z-score) across the session. F) Transient amplitude binned in 5-minute time windows. Data reflect automated transient detection using PASTa protocol defaults. Additionally, relative to the matched control session, morphine increased VTA DA activity transient rise duration (figure panels A and B) and decreased VTA DA activity transient fall duration (figure panels C and D). Morphine increased the overall AUC of transients (figure panel E). These trends emerged approximately 10 minutes post-injection and returned to pre-treatment baseline and matched control session levels by the end of the post-injection recording period (figure panels B, D, and F). Transient Duration Results . Example analysis of VTA DA transient event duration and AUC following acute saline (blue, control) and morphine (red) administration. A) Mean transient rise duration (milliseconds) across the full session. B) Transient rise duration binned in 5-minute time windows. C) Mean transient fall duration (milliseconds) across the full session. D) Transient fall duration binned in 5-minute time windows. E) Mean area under the curve (AUC) for transients. F) Transient AUC binned in 5-minute time windows. All duration and AUC metrics were calculated at half-height relative to the peak. Vertical dashed lines in B, D, and F indicate the time of saline or morphine injection. Conclusion Together, this example analysis illustrates the complementary shifts in multiple aspects of transient dynamics, providing a more comprehensive evaluation of the effects of morphine on VTA DA activity. This example is provided to allow users to directly apply PASTa to a real data set and ensure that results are replicated across users of the toolbox.","title":"Example Analyses"},{"location":"exampleanalyses/#example-analysis-injection-transients","text":"PASTa includes an example analysis with fiber photometry recordings to help users get comfortable with the workflow and observe the effects of each stage of processing on the signal. The analysis script is available on GitHub, and the data are freely accessible on (Box)[https://uofi.app.box.com/s/pqpa8286yi6oia1ocjts7qytb1wetgpv/folder/256973884209] at https://uofi.app.box.com/s/pqpa8286yi6oia1ocjts7qytb1wetgpv/folder/256973884209 under Example Analyses . The subfolder Injection Transients is prepared for the analysis, containing the file and subject keys, as well as folders created for extracted data, analysis, and figure outputs. Raw data blocks collected via Synapse (Tucker Davis Technologies) are nested in the Raw Data folder.The completed analysis output files including extracted data structures, transient quantification, and plots are also available for download. If you have any questions or run into problems accessing the files, please feel free to contact us .","title":"Example Analysis: Injection Transients"},{"location":"exampleanalyses/#methods","text":"","title":"Methods"},{"location":"exampleanalyses/#overview","text":"Example data are provided from two subjects. Each subject has two fiber photometry sessions conducted on consecutive days: saline and morphine (10mg/kg). Each session consists of a fifteen minute baseline, an intraperitoneal injection, and a 60 minute post-injection recording period. Fiber photometry data were recorded using Tucker Davis Technologies RZ10X processor, with a 465nm excitation wavelength (signal; 'sig' ) and a 405nm isosbestic control wavelength (background, 'baq' ). Injection start and end time points are marked by epochs ( 'injt' ) sent through Med Associates equipment and stored by Synapse as time stamps.","title":"Overview"},{"location":"exampleanalyses/#animals","text":"Adult male and female Long Evans rats (n = 2) expressing Cre recombinase under the control of the tyrosine hydroxylase promoter (TH:Cre+; [34]) were paired housed within a temperature and humidity-controlled vivarium on a 12:12 hour light-dark cycle with lights on at 7:00 am and maintained on ad libitum food and water for the direction of all experiments. All procedures were conducted in accordance with guidelines set by the National Institutes for Health Guide for the Care and Use of Laboratory Animals and approved by the Animal Care Committee of the University of Illinois Chicago. Recordings were conducted during the light portion of the cycle.","title":"Animals"},{"location":"exampleanalyses/#stereotaxic-surgery-procedures","text":"Rats were anesthetized via isoflurane (induction: 4%; maintenance: ~2%) and injected with meloxicam (1mg/kg, subcutaneous) prior to the start of stereotaxic surgery. To isolate and record from dopamine neuronal cell bodies, AAV1.hSyn.Flex.GCaMP6f.WPRE.SV40 was targeted to the VTA (Anterior/Posterior (AP): - 5.40mm, Medial/Lateral (ML): +0.70mm, Dorsal/Ventral (DV): -8.15mm, relative to Bregma) of TH:Cre+ rats. Viral injections were performed with a volume of 1\u03bcL at a rate of 0.1\u03bcL/minute with a blunt 30-gauge injection needle mated to a Hamilton syringe via PEEK tubing and connected to a syringe pump. A 5 minute post-injection period followed to ensure complete virus diffusion. An optic fiber (flat 400-\u03bcm core, 0.48 NA, Doric Lenses, Inc.) was implanted above the injection site and secured to three implanted skull screws via metabond and dental cement.","title":"Stereotaxic Surgery Procedures"},{"location":"exampleanalyses/#fiber-photometry-data-collection","text":"Fiber photometry recordings were performed as in Konanur et al 2020, Hsu et al 2020, and Konanur et al 2024, and Loh et al 2025 [35-38]. Data were collected using data acquisition systems from Tucker Davis Technologies (RZ10X). Implanted fiber optic cannulae of subjects were mated to a patch cord, which was connected through a commutator to a filter cube (FMC4, Doric Lenses). Emitted fluorescence was collected by an external photoreceiver (Visible Femtowatt Photoreceiver Model 2151; Newport) and passed to the data acquisition system for live visualization and recording. LEDs administered 465nm and 405nm wavelengths at a total power output of 30\u03bcW. Light was sinusoidally modulated at 211 and 531 Hz respectively. Lock in amplification was used to demodulate the fluorescence by emission wavelength. Fiber photometry recording sessions were conducted on consecutive days, with the first session consisting of a control saline treatment (1mL/kg, intraperitoneal (i.p.); figure panel A) and the second session consisting of acute morphine treatment (10mg/kg morphine sulfate, i.p.; figure panel B). Each test session consisted of a 15 minute pre-injection baseline, followed by a 60-minute post-injection recording period. Signal processing was performed with PASTa protocol defaults. Subtracted and filtered data streams were normalized to z-score based on the pre-injection baseline period. Transient events were detected at a threshold of 2.6 SDs using PASTa protocol default parameters.","title":"Fiber Photometry Data Collection"},{"location":"exampleanalyses/#analysis-script","text":"The full analysis script is available via the (PASTa GitHub)[https://github.com/rdonka/PASTa] under the subfolder \"Example Analyses\" . Download the file \"main_ExampleAnalysis_InjectionTransients.m\".","title":"Analysis Script"},{"location":"exampleanalyses/#data-preparation","text":"The first section of the script sets up paths and analysis key inputs. To enable users to switch computers easily, paths are created without the computer and user specific portion. The computer user specific root directory of the path is input to the variable rootdirectory and appended to subsequently needed paths. For users accessing PASTa via cloning the repository, to access files and functions, paths need to be added to MATLAB via the addpath function. genpath is used within addpath to ensure folders and subfolders at the input path are added. Finally, the names of created Subject and File Keys are added to variables subjectkeyname and filekeyname . These keys contain the session specific and subject specific information needed to load the data and analyze the results. For details on keys, see the user guide section on Data Preparation . The keys are loaded using the function createExperimentKey to join the subject specific data to the session specific information contained in the file key. Paths to raw data blocks and extracted output locations are added to string arrays, which are input to the function extractTDTdata . extractTDTdata first extracts the raw data for each file into a data structure, trims the first 5 seconds of the recording, and then saves the MATLAB data structure to extracted output location. Extracted structures for each file are then loaded by the function loadKeydata . After data are loaded, excess samples before the start of the program and after the end of the post-injection period are cropped from the streams. First, session start and end indices are prepared for each file. These indices are input to the function cropFPdata , which removes the excess samples from the signal and background streams, and adjusts the injection and session start time stamp epocs.","title":"Data Preparation"},{"location":"exampleanalyses/#signal-processing","text":"To control for motion artifacts and photobleaching, the 405 nm channel ( 'baq' ) is subtracted from the 465 nm channel ( 'sig' ) with the function subtractFPdata . The example analysis script uses the default 'frequency' scaling method (see the user guide section (Signal Processing)[https://rdonka.github.io/PASTaUserGuide/userguide/signalprocessing/] for full details and alternative scaling options). First, subtractFPdata scales the background stream to the signal stream. After scaling, the background is subtracted from the signal in the time domain, and filtered with a Butterworth bandpass filter to remove the DC offset as well as high frequency noise (frequencies outside the range of biological interest). The scaling factor ( 'baqscalingfactor' ), subtracted signal ( 'sigsub' ), filtered signal ( 'sigfilt' ) are added to the data structure. Raw, subtracted, and filtered streams for the whole session are visualized and saved to the output folder 'Figures' . Additionally, frequency magnitude plots of the FFTs of the raw, scaled, subtracted, and filtered streams are generated and saved to the 'Figures' folder.","title":"Signal Processing"},{"location":"exampleanalyses/#normalization","text":"After subtraction, the filtered signal is normalized. To normalize to the entire session, the function normSession is called, outputting the z scored signal streams to 'sigfiltz_normsession' . To normalize to pre injection baseline, the function normBaseline is called, which uses the pre-injection period to normalize the entire session, outputting the z scored streams to 'sigfiltz_normbaseline' . Finally, a for loop is used to create streams with the injection time period removed. For details on the signal processing functions and methods, see the user guide section on Signal Processing .","title":"Normalization"},{"location":"exampleanalyses/#transient-detection","text":"To identify transient events, the findTransients function is used. This function detects relevant transients in the normalized data stream based on an amplitude inclusion criterion (eg, 2.6 SD) relative to the pre-event baseline. The example analysis uses the default pre-event baseline type (baseline window mean; 'blmean' ) and baseline window parameters. See the user guide section (Transient Analysis)[https://rdonka.github.io/PASTaUserGuide/userguide/transientanalysis/] for more details and optional input parameters. Transient events are output to a separate data structure to facilitate analysis. After detection, transients are quantified by frequency, amplitude, half height rise time, half height fall time, half height width, and half height AUC. After detection and quantification, transients are binned into 5 minute (default) increments. The function binTransients identifies and adds the bin of each transient event to the transientquantificiation table. Multiple plots are generated to visualize transient events and figures are saved to the Figures folder. Whole session trace with all transient events: Individual bin traces with identified transients: Overlaid Transient Traces from the Whole Session: Overlaid Transient Traces by Bin: To export the transient events for statistical analysis, the summarizeTransients and summarizeBinTransients functions are used to generate the session means of transient event quantification parameters.","title":"Transient Detection"},{"location":"exampleanalyses/#results","text":"Overall, morphine administration significantly increased the frequency (figure panel C) and amplitude (figure panel E) of VTA DA activity transients relative to the saline control session. The increase in transient frequency and amplitude was observed approximately 10 minutes post-injection (figure panels D and F), and remained elevated relative to both pre-injection baseline and the matched saline control session through the end of the 60-minute post-injection recording. Transient Frequency and Amplitude Results. Example analysis of VTA DA transient frequency and amplitude after acute morphine administration. Representative examples of pre-injection baseline normalized traces from a single subject before and after A) an injection of saline (control) and B) an injection of morphine (10mg/kg). The vertical dashed line indicates time of injection. C) Mean transient frequency (transients per minute) across the entire session. D) Transient frequency binned in 5-minute time windows. E) Mean transient amplitude (z-score) across the session. F) Transient amplitude binned in 5-minute time windows. Data reflect automated transient detection using PASTa protocol defaults. Additionally, relative to the matched control session, morphine increased VTA DA activity transient rise duration (figure panels A and B) and decreased VTA DA activity transient fall duration (figure panels C and D). Morphine increased the overall AUC of transients (figure panel E). These trends emerged approximately 10 minutes post-injection and returned to pre-treatment baseline and matched control session levels by the end of the post-injection recording period (figure panels B, D, and F). Transient Duration Results . Example analysis of VTA DA transient event duration and AUC following acute saline (blue, control) and morphine (red) administration. A) Mean transient rise duration (milliseconds) across the full session. B) Transient rise duration binned in 5-minute time windows. C) Mean transient fall duration (milliseconds) across the full session. D) Transient fall duration binned in 5-minute time windows. E) Mean area under the curve (AUC) for transients. F) Transient AUC binned in 5-minute time windows. All duration and AUC metrics were calculated at half-height relative to the peak. Vertical dashed lines in B, D, and F indicate the time of saline or morphine injection.","title":"Results"},{"location":"exampleanalyses/#conclusion","text":"Together, this example analysis illustrates the complementary shifts in multiple aspects of transient dynamics, providing a more comprehensive evaluation of the effects of morphine on VTA DA activity. This example is provided to allow users to directly apply PASTa to a real data set and ensure that results are replicated across users of the toolbox.","title":"Conclusion"},{"location":"functiondocumentation/","text":"Function Documentation Overview This page contains additional documentation for each function within PASTa, as well as examples of inputs. Detailed documentation is also available within each MATLAB function and can be viewed via the MATLAB help window or by opening the function file. Data Preparation Functions This set of functions is used to prepare raw photometry data, match it with experimental metadata, and load data into a structure in MATLAB. Functions are provided to handle data collected via TDT equipment and software Synapse, or a generic file structure with data streams saved to CSV files. createExperimentKey Combine subject key and file key into a single data structure, appending the provided rootdirectory to create full paths to stored data locations. INPUTS: ROOTDIRECTORY: String. The top-level directory path unique to your system. For example: 'C:\\Users\\rmdon\\'. Must end with a slash/backslash. SUBJECTKEYNAME: String. The name (or full path) of the subject key CSV file (e.g. 'mySubjectKey.csv'). Must contain at least the variable 'SubjectID' . To omit a subject key and only load in the file key, set subjectkeyname = ''. If empty (''), the subject key is skipped and only FILEKEYNAME is loaded. See the user guide Data Preparation for more details. FILEKEYNAME: String. The name (or full path) of the file key CSV file (e.g. 'myFileKey.csv'). Must contain 'SubjectID' , 'BlockFolder' , 'RawFolderPath' , and 'ExtractedFolderPath' columns at minimum. Paths in 'RawFolderPath' and 'ExtractedFolderPath'should each end with a slash/backslash. OUTPUTS: EXPERIMENTKEY: A data structure of the combined file and subject key data. The 'rootdirectory' is prepended to each row's 'RawFolderPath' and 'ExtractedFolderPath' , while the 'BlockFolder' name is appended to both. EXAMPLE: rootdirectory = 'C:\\Users\\rmdon\\'; subjKey = 'subjectKey.csv'; fileKey = 'fileKey.csv'; experimentkey = createExperimentKey(rootdirectory, subjKey, fileKey); % Load keys into a data structure called experimentkey NOTES: FILEKEY must contain at a minimum the fields SubjectID , BlockFolder , RawFolderPath , and ExtractedFolderPath . SUBJECTKEY must contain at a minimum the field SubjectID Folder paths must end with a slash. The subject and file keys are joined based on SubjectID . Subject key must contain every subjects in the file key. If there is a mismatch, you will receive an error message that the right table does not contain all the key variables that are in the left table. The error message will display the unique subject IDs present in each key so you can determine where the mismatch occurred. Fields in subject and file key must be named uniquely. The only field that should be named the same in both keys is Subject. extractTDTdata This function is used to extract TDT data from saved blocks recorded via the software Synapse . For each block, extractTDTdata calls the function \"TDTbin2mat\" (TDT, 2025) and inputs the RawFolderPath to extract fiber photometry data recorded with Synapse . Extracted blocks are parsed it into a single data structure containing all fields, streams, and event epocs. Extracted signal streams are trimmed to remove the first 5 seconds by default (trimming can be adjusted if desired). The function will identify the signal channel by matching the names in the input SIGSTREAMNAMES and the control channel by matching the names in the input BAQSTREAMNAMES. The name inputs can include a list of stream names if channel naming conventions vary by rig. Each block is saved as a separate data structure in a '.mat' file at the location specified by the inputs in extractedfolderpaths . For more details on preparing TDT data blocks, see the user guide section on Data Preparation: Data Organization . INPUTS: RAWFOLDERPATHS: String array of raw TDT block folder paths. The string array should contain one column with each full path in a separate row. If using the createExperimentKey function, this can be created from the experiment key data structure. For example: rawfolderpaths = string({experimentkey.RawFolderPath})'; EXTRACTEDFOLDERPATHS: String array of corresponding output paths where extracted data is saved. If using the createExperimentKey function, this can be created from the experiment key. For example: extractedfolderpaths = string({experimentkey.ExtractedFolderPath})'; SIGSTREAMNAMES: Cell array of possible stream names for the \"signal\" channel (e.g. {'x65A','465A'}). NOTE: Only one stream per file can be treated as signal. If different files have different stream names, include all stream names in the cell array. BAQSTREAMNAMES: Cell array of possible stream names for the \"background\" channel (e.g. {'x05A','405A'}). NOTE: Only one stream per file can be treated as background. A cell array containing strings with the names of the streams to be treated as background. If different files have different background stream names, include all stream names in the cell array. OPTIONAL INPUTS: 'trim': Numeric; Number of seconds to trim from the start and end of each recording. Default: 5 'skipexisting': Numeric (0 or 1; Default: 1); If 1, skip extracting any session for which an output file already exists. If 0, re-extract and overwrite. This allows the user to toggle whether or not to extract every block, or only blocks that have not previously been extracted. If not specified, defaults to skip previously extracted blocks (1). OUTPUTS: Saved .mat data structures for each block in the location specified by extractedfolderpaths . EXAMPLE - DEFAULT: sigstreamnames = {'x65A', '465A'}; % All names of signal streams across files baqstreamnames = {'x05A', '405A'}; % All names of background streams across files rawfolderpaths = string({experimentkey.RawFolderPath})'; % Create string array of raw folder paths extractedfolderpaths = string({experimentkey.ExtractedFolderPath})'; % Create string array of extracted folder paths % Extract and save data structures for each file extractTDTdata(rawfolderpaths,extractedfolderpaths,sigstreamnames,baqstreamnames); EXAMPLE - MANUALLY SPECIFIED TRIM AND SKIPEXISTING: trim = 3; % Set trim to 3 seconds skipexisting = 0; % Extract all blocks sigstreamnames = {'x65A', '465A'}; % All names of signal streams across files baqstreamnames = {'x05A', '405A'}; % All names of background streams across files rawfolderpaths = string({experimentkey.RawFolderPath})'; % Create string array of raw folder paths' extractedfolderpaths = string({experimentkey.ExtractedFolderPath})'; % Create string array of extracted folder paths' % Extract and save data structures for each file extractTDTdata(rawfolderpaths,extractedfolderpaths,sigstreamnames,baqstreamnames,'trim',trim,'skipexisting',skipexisting); extractCSVdata This function is used to extract data collected from non-TDT systems (e.g., Neurophotometrics, Doric). Data must first be prepared in the generic CSV format, with each session saved as a folder containing CSV files of the signal, background, session recording parameters, and event epochs (optional). Note that session recording parameters must at least include the sampling rate as the variable fs . For more details on preparing CSV data files, see the user guide section on Data Preparation: Data Organization . For each block, extractCSVdata extracts the signal and background streams, recording parameters, and any specific event epochs into a single data structure containing all recording parameters, streams, and included event epochs. Extracted signal and background streams are trimmed to remove the first 5 seconds by default (trimming can be adjusted if desired). Each block is saved as a separate data structure in a '.mat' file at the location specified by the inputs in extractedfolderpaths . INPUTS: RAWFOLDERPATHS: String array of raw TDT block folder paths. The string array should contain one column with each full path in a separate row. If using the createExperimentKey function, this can be created from the experiment key data structure. For example: rawfolderpaths = string({experimentkey.RawFolderPath})'; EXTRACTEDFOLDERPATHS: String array of corresponding output paths where extracted data is saved. If using the createExperimentKey function, this can be created from the experiment key. For example: extractedfolderpaths = string({experimentkey.ExtractedFolderPath})'; SIGSTREAMNAME: String; Name of csv files containing the \"signal\" channel (e.g., 'sig' ). NOTE: Only one stream per file can be treated as signal. BAQSTREAMNAME: String; Name of csv files containing the \"background\" channel (e.g. 'baq' ). NOTE: Only one stream per file can be treated as background. OPTIONAL INPUTS: 'loadepocs': Logical; Set to 1 to load event epoch files in the 'Raw Data' session folders. 'epocsnames': Cell array of file names of csv files containing event epochs to be loaded into the data structure. Note: Each type of epoch should be saved to a separate csv file. Pass all event epoch files names to epocsnames . 'trim': Numeric; Number of seconds to trim from the start and end of each recording. Default: 5 'skipexisting': Numeric (0 or 1; Default: 1); If 1, skip extracting any session for which an output file already exists. If 0, re-extract and overwrite. This allows the user to toggle whether or not to extract every block, or only blocks that have not previously been extracted. If not specified, defaults to skip previously extracted blocks (1). OUTPUTS: Saved .mat data structures for each block in the location specified by extractedfolderpaths . EXAMPLE - DEFAULT: sigstreamname = 'sig'; % All names of signal streams across files baqstreamname = 'baq''; % All names of background streams across files rawfolderpaths = string({experimentkey.RawFolderPath})'; % Create string array of raw folder paths extractedfolderpaths = string({experimentkey.ExtractedFolderPath})'; % Create string array of extracted folder paths % Extract and save data structures for each file extractCSVdata(rawfolderpaths,extractedfolderpaths,sigstreamname,baqstreamname); EXAMPLE - WITH EPOCHS: sigstreamname = 'sig'; % All names of signal streams across files baqstreamname = 'baq''; % All names of background streams across files epocsnames = {'strt', 'injt'}; % Prepare names of csv files containing event epochs rawfolderpaths = string({experimentkey.RawFolderPath})'; % Create string array of raw folder paths extractedfolderpaths = string({experimentkey.ExtractedFolderPath})'; % Create string array of extracted folder paths % Extract and save data structures for each file extractCSVdata(rawfolderpaths,extractedfolderpaths,sigstreamname,baqstreamname,'loadepocs',1,'epocsnames',epocsnames); EXAMPLE - MANUALLY SPECIFIED TRIM AND SKIPEXISTING: trim = 3; % Set trim to 3 seconds skipexisting = 0; % Extract all blocks sigstreamname = 'sig'; % All names of signal streams across files baqstreamname = 'baq''; % All names of background streams across files rawfolderpaths = string({experimentkey.RawFolderPath})'; % Create string array of raw folder paths' extractedfolderpaths = string({experimentkey.ExtractedFolderPath})'; % Create string array of extracted folder paths' % Extract and save data structures for each file extractCSVdata(rawfolderpaths,extractedfolderpaths,sigstreamname,baqstreamname,'trim',trim,'skipexisting',skipexisting); loadKeydata For use with previously extracted data. Compatible with data structures extracted by either the extractTDTdata or extractCSVdata functions. loadKeydata loads previously extracted .mat data blocks with individaul session data into a main data structure for further analysis. Each block is one row. The input experiment key must include at a minimum the extracted folder path location of the block. Any additional information about the subject and session in the experiment key will be matched to the extracted data. INPUTS: EXPERIMENTKEY: A prepared data structure with at minimum the ExtractedFolderPath containing the full path to the individual session data structures to be loaded. The EXPERIMENTKEY can be prepared with the LOADKEYS function. OUTPUTS: DATA: the input data structure with each individual extracted block added by row. EXAMPLE: [rawdata] = loadKeydata(experimentkey); % Load data based on the experiment key into the structure 'rawdata' cropFPdata Used to crop data streams to remove portions that are not to be included in analysis (such as the the first two minutes of the session, or the first samples before a hardware control program is initiated). Crops all specified data streams from the index in cropstart to the index in cropend , and adjusts specified event epochs by the amount cropped by cropstart . Users must pre-prepare the crop start and end indexes to specify as inputs for the function. INPUTS: DATA: Data structure array containing at least the specified input fields. CROPSTARTFIELDNAME: String; Field name for cropping start index. For example: 'sessionstart'. CROPENDFIELDNAME: String; Field name for cropping end index. For example: 'sessionend'. STREAMFIELDNAMES: A cell array containing the names (strings) of all the data streams to be cropped. For example: {'sig', 'baq'} OPTIONAL INPUTS: 'epocsfieldnames': A cell array containing the field names of all the event epochs to be adjusted for cropping (event epoch - (start loc - 1)). OUTPUTS: DATA: The data structure with the specified data streams containing the cropped data, and event epochs adjusted to maintain alignment with the data streams. EXAMPLE: WITHOUT EVENT EPOCS cropstartfieldname = 'sessionstart'; % Name of field with session start index cropendfieldname = 'sessionend'; % Name of field with session end index streamfieldnames = {'sig', 'baq','time'}; % Names of all streams to crop % Output cropped data into new structure called data [data] = cropFPdata(rawdata,cropstartfieldname,cropendfieldname,streamfieldnames); EXAMPLE: WITH EVENT EPOCS cropstartfieldname = 'sessionstart'; % Name of field with session start index cropendfieldname = 'sessionend'; % Name of field with session end index streamfieldnames = {'sig', 'baq','time'}; % Names of all streams to crop epocsfieldnames = {'injt','sess'}; % Field names of event epocs to adjust to maintain relative position % Output cropped data into new structure called data [data] = cropFPdata(rawdata,cropstartfieldname,cropendfieldname,streamfieldnames,'epocsfieldnames',epocsfieldnames); Signal Processing Functions This set of functions is used to process raw fiber photometry data prior to further analyses. Functions are available to perform background subtraction, filter, and normalize the data streams. While default options have been chosen to provide users with a conservative starting point, multiple options for signal processing and normalization are available to allow for customization to adapt to experimental needs. For a full discussion of available signal processing methods, see the user guide section Signal Processing . subtractFPdata Subtracts background fluorescence stream (eg, 405nm 'baq') from the signal stream (eg, 465nm 'sig'), converts the subtracted signal to \u0394F/f, and applies a filter to denoise the output. Users must input at a minimum the data structure with the raw data, the names of the fields containing the signal and background streams, and the name of the field containing the sampling rate of the collected data. INPUTS: DATA: Data structure array containing at least the specified input fields (signal and background streams, sampling rate) SIGFIELDNAME: String; name of the field in DATA containing the signal data stream (e.g., 'sig' ). BAQFIELDNAME: String; name of the field in DATA containing the background data stream (e.g., 'baq' ). FSFIELDNAME: String; name of the field in DATA containing the sampling rate of the fiber photometry data streams in Hz (e.g., 'fs' ). Note: The sampling rate must be specified in Hz for filtering to be properly applied. OPTIONAL INPUTS: 'baqscalingtype': A string to specify the type of background scaling to apply. Options are 'frequency' , 'sigmean' , 'OLS' , 'detrendedOLS' , 'smoothedOLS' , 'biexpOLS' , 'biexpQuartFit' , or 'IRLS' . Default: 'frequency' . 'frequency': Default ; Scales the background to the signal channel based on ratio of the power in the specified frequency bands in the FFT (frequency domain) of the channels. 'sigmean': Scales the background to the signal channel based on the ratio of the mean of the signal to the mean of the background (time domain). 'OLS': Uses ordinary least-squares regression to generate scaled background. 'detrendedOLS': Removes the linear trend from signal and background streams prior to using ordinary least-squares regression to generate scaled background. 'smoothedOLS': Applies lowess smoothing to the signal and background streams prior to using ordinary least-squares regression to generate scaled background. 'biexpOLS': Fits and removes a bi-exponential decay function from the raw background and signal streams, then scales the corrected background to the corrected signal with ordinary least-squares regression. 'biQuartFit': Fits and removes a bi-exponential decay function from the raw background and signal streams, then scales the corrected background to the corrected signal based on the interquartile range. 'IRLS': Uses iteratively reweighted least squares regression to generate scaled background. 'baqscalingfreqmin': Only used with 'frequency' scaling. Numeric frequency (Hz) minimum threshold for scaling the background to signal channel. Frequencies above this value will be included in the scaling factor determination. Default: 10 Hz. 'baqscalingfreqmax': Only used with 'frequency' scaling. Numeric frequency (Hz) maximum threshold for scaling the background to signal channel. Frequencies below this value will be included in the scaling factor determination. Default: 100 Hz. 'baqscalingperc': Only used with 'frequency' and 'sigmean' scaling. Adjusts the background scaling factor to be a percent of the derived scaling factor value. Default: 1 (100%). 'subtractionoutput': Output type for the subtracted data. Default: 'dff' 'dff': Outputs subtracted signal as \u0394F/F. ((signal - scaled background) / scaled background) 'df': Outputs subtracted signal as \u0394F. (signal - scaled background) 'artifactremoval' : Logical (0 or 1); set to 1 to detect and remove artifacts from data streams. Default: 0 (false). Note: see removeStreamArtifacts function documentation for more detail. 'filtertype': A string to specify the type of filter to apply after subtraction. Default: 'bandpass'. 'nofilter': No filter will be applied. 'bandpass': A bandpass Butterworth filter will be applied. 'highpass': Only a high pass Butterworth filter will be applied. 'lowpass': Only a low pass Butterworth filter will be applied. 'padding': Defaults to 1, which applies padding prior to filtering. Padding takes the first 10% of the stream, flips it, and appends it to the data before applying the filter to prevent edge effects. Appended data is trimmed after filtering. Set to 0 to turn off padding of data streams. Default: 1. 'paddingperc': Percent of data length to use to determine the number of samples to be appended to the beginning and end of data in padding. Set to minimum of 0.1 (10%). Default: 0.1 (10%). 'filterorder': The order to be used for the chosen Butterworth filter. Default: 3. 'highpasscutoff': The cutoff frequency (Hz) to be used for the high pass Butterworth filter. Default: 2.2860. 'lowpasscutoff': The cutoff frequency (Hz) to be used for the low pass butterworth filter. Default: 0.0051. NOTE: 'bandpass' applies both the high and low pass cutoffs to design the filter. OUTPUTS: DATA: The original data structure with added fields with the scaled background ('baqscaled'), subtracted signal ('sigsub'), and subtracted and filtered signal ('sigfilt'). All inputs and defaults will be added to the data structure under the field 'inputs'. NOTE: If using BAQSCALINGMETHOD 'detrendedOLS' , additional fields containing the detrended signal and background ('sigdetrend' and 'baqdetrend') will be added to the data frame. If using BAQSCALINGMETHOD 'smoothedOLS' , additional fields containing the smoothed signal and background ('sigsmoothed' and 'baqsmoothed') will be added to the data frame. If using BAQSCALINGMETHOD 'biexpOLS' or 'biexpQuartFit' , additional fields containing the bi-exponential decay corrected signal and background ('sigbiexpcorrected' and 'baqbiexpcorrected') will be added to the data frame. EXAMPLE - DEFAULT: sigfieldname = 'sig'; baqfieldname = 'baq'; fsfieldname = 'fs'; data = subtractFPdata(data,sigfieldname,baqfieldname,fsfieldname); EXAMPLE - Frequency Scaling with 20Hz Minimum Threshold and Highpass Filter Only: sigfieldname = 'sig'; baqfieldname = 'baq'; fsfieldname = 'fs'; data = subtractFPdata(data,sigfieldname,baqfieldname,fsfieldname,'baqscalingfreqmin',20,'filtertype,'highpass'); EXAMPLE - OLS Scaling: sigfieldname = 'sig'; baqfieldname = 'baq'; fsfieldname = 'fs'; data = subtractFPdata(data,sigfieldname,baqfieldname,fsfieldname,'baqscalingtype','OLS'); EXAMPLE - IRLS Scaling: sigfieldname = 'sig'; baqfieldname = 'baq'; fsfieldname = 'fs'; data = subtractFPdata(data,sigfieldname,baqfieldname,fsfieldname,'baqscalingtype','IRLS'); removeStreamArtifacts Detects and removes artifacts from fiber photometry data streams. Called by the subtractFPdata function prior to filtering if the optional input removeartifacts is set to 1. INPUTS: DATASTREAM: Numeric array; The signal data for artifact removal. STREAMFIELDNAME: String; Name of the stream being processed. FS: Numeric; Sampling rate of the data stream being processed (Hz). OPTIONAL INPUTS: 'outlierthresholdk': ANumeric, Integer; Multiplier for outlier threshold detection. Default: 3 'artifactremovalwindow': Numeric; Seconds before and after an artifact to replace with NaNs. Default: 0.3 'artifactampthresh_max': Numeric; Standard deviation threshold for high artifacts. Default: 8 'artifactampthresh_min': Numeric; Standard deviation threshold for low artifacts. Default: 8 'bucketsizeSecs': Numeric; Bucket time window length (seconds) for amplitude and mean calculations. OUTPUTS: ARTIFACTREMOVALDATA: Data structure containing: STREAMFIELDNAME_AR: Data stream with artifacts replaced by NaNs STREAMFIELDNAME_AM: Data stream with artifacts replaced by mean values NUMARTIFACTS: Number of detected artifacts ARTIFACTS: Table of artifact details (locations, values, start/end indices) EXAMPLE: Default Parameters datastream = [data(1).sigsub]; % Prepare numeric vector of a stream for one recording session streamfieldname = 'sigsub'; % Field name of the prepared *datastream* input for artifact removal fs = data(1).fs; % Prepare sampling rate of the data stream in Hz % Detect and Remove Artifacts [artifactremovaldata] = removeStreamArtifacts(datastream,streamfieldname,fs); EXAMPLE: Increased Artifact Threshold datastream = [data(1).sigsub]; % Prepare numeric vector of a stream for one recording session streamfieldname = 'sigsub'; % Field name of the prepared *datastream* input for artifact removal fs = data(1).fs; % Prepare sampling rate of the data stream in Hz % Detect and Remove Artifacts [artifactremovaldata] = removeStreamArtifacts(datastream,streamfieldname,fs,'artifactampthresh_max',10,'artifactampthresh_min',10); prepareStreamFFT Helper function to compute the one-sided Fast Fourier Transform (FFT) of a data stream for analysis and plotting. This function is called by the subtractFPdata , plotFFTmag , and plotFFTpower functions. INPUTS: STREAMDATA: Numeric vector; the input data stream to be transformed. FS: Positive scalar; the sampling rate of the data stream in Hz. OUTPUTS: STREAMFFT: Numeric vector; the magnitudes of the one-sided FFT of the input data stream. STREAMF: Numeric vector; the corresponding frequency values in Hz for the FFT magnitudes. EXAMPLE: streamdata = [data(1).sig]; % Prepare numeric vector of the signal stream for one recording session fs = data(1).fs; % Prepare sampling rate of the data stream in Hz % Compute the FFT [streamFFT, streamF] = preparestreamFFT(streamdata, fs); normSession Normalizes a specified data stream to Z-score based on the entire session mean and standard deviation. INPUTS: DATA: Data structure array; each element (row) represents a single recording session and must contain the field specified by STREAMFIELDNAME. STREAMFIELDNAME: String; the name of the field within DATA to be normalized. For example: 'sigfilt' OUTPUTS: DATA: The original data structure with the added field 'data.WHICHSTREAMz_normsession' containing the whole session normalized signal. EXAMPLE: [data] = normSession(data,'sigfilt'); % Outputs whole session z score normBaseline Normalizes a specified data stream to Z-score based on the specified session baseline period. INPUTS: DATA: Data structure array; each element (row) represents a single recording session and must contain the field specified by STREAMFIELDNAME. STREAMFIELDNAME: String; the name of the field within DATA to be normalized. For example: 'sigfilt' BLSTARTFIELDNAME: String; The name of the field containing the index of the start of the baseline period. BLENDFIELDNAME: String; The name of the field containing the index of the end of the baseline period. OUTPUTS: DATA: The original data structure with the added field 'data.WHICHSTREAMz_normbaseline' containing the baseline normalized signal. EXAMPLE: % Prepare baseline start and end indices based on event epoch for eachfile = 1:length(data) data(eachfile).BLstart = 1; data(eachfile).BLend = data(eachfile).injt(1); end % Normalize filtered signal to session baseline mean and SD [data] = normBaseline(data,'sigfilt','BLstart','BLend'); normCustom Normalizes the whole session data stream based on a custom period input as a separate stream field. INPUTS: DATA: Data structure array; each element (row) represents a single recording session and must contain the field specified by STREAMFIELDNAME. FULLSTREAMFIELDNAME: String; the name of the field within DATA to be normalized. For example: 'sigfilt' _CUSTOMSTREAMFIELDNAME:__ String; The name of the field containing the cut data stream to use as the reference for normalization. OUTPUTS: DATA: The original data structure with the added field 'data.WHICHSTREAMz_normcustom' containing the baseline normalized signal. EXAMPLE: % Prepare custom data stream for normalization using the pre-trial and post-trial periods of the session for eachfile = 1:length(data) data(eachfile).customnorm = [data(eachfile).sigfilt(1:data(eachfile).trial(1)), data(eachfile).sigfilt(data(eachfile).trial(30):length(data(eachfile).sigfilt))]; end % Normalize filtered signal to custom session period mean and SD [data] = normCustom(data,'sigfilt','customnorm'); preparestreamFFT Helper function to take the FFT of a data stream and output the magnitudes and corresponding frequencies for analysis, plotting, and diagnostics. This function is called by the plot function plotFFTs but can also be used to prepare FFTs for each stream of data for all sessions with use in a for loop. INPUTS: STREAMDATA: An array containing the values from a single collected data stream. FS: The sampling rate of the data stream. OUTPUTS: STREAMFFT: An array containing the prepared FFT magnitudes of the input stream. STREAMF: An array containing the corresponding frequencies to the prepared FFT magnitudes in STREAMFFT. EXAMPLE OF SINGLE SESSION SIGNAL: [streamFFT,streamF] = preparestreamFFT(data(1).sig,data(eachfile).fs); EXAMPLE OF ALL SESSIONS AND STREAMS: %% Prepare FFTs of all streams streams = {'sig', 'baq', 'baq_scaled','sigsub', 'sigfilt'}; % Frequency scaled background for eachfile = 1:length(data) for eachstream = 1:length(streams) streamname = char(streams(eachstream)); [streamFFT,streamF] = preparestreamFFT(data(eachfile).(streamname),data(eachfile).fs); data(eachfile).(append(streamname,'FFT')) = streamFFT; data(eachfile).(append(streamname,'F')) = streamF; end end Transient Analysis Functions findTransients Main function to detect and quantify transient events in whole session data streams. Users must input the raw data structure containing the data stream, a field with threshold values for transient inclusion, and the sampling rate of the stream. findTransients will output a new data structure ( transientdata ) with any user specified fields from the original data structure, and the field 'transientquantification' containing quantification variables for each detected transient event. The findTransients function will set default values for transient detection and quantification parameters not specific as optional inputs. Optional inputs can be specified to modify transient detection parameters. See the user guide section Transient Analysis for more details. INPUTS: DATA: Data structure array; each element (row) represents a single recording session and must contain the fields specified by ADDVARIABLESFIELDNAMES, STREAMFIELDNAME, THRESHOLDFIELDNAME, and FSFIELDNAME. ADDVARIABLESFIELDNAMES: Cell array; names of the fields in DATA to add to the new data structure. This should include SubjectID and experimentally relevant metadata. For example: {'SubjectID', 'BlockFolder', 'Dose'} WHICHSTREAM: The name (string) of the field containing the stream to be analyzed for transients. WHICHBLTYPE: A string with the type of pre-transient baseline to use for amplitude inclusion and quantification. 'blmean': Pre-transient baselines are set to the mean of the pre-transient window. 'blmin': Pre-transient baselines are set to the minimum value within the pre-transient window. 'localmin': Pre-transient baselines are set to the local minimum directly preceding the transient within the baseline window. WHICHSTREAM: The name (string) of the field containing the stream to be analyzed for transients. WHICHTHRESHOLD: The name (string) of the field containing the prepared numeric threshold values for each stream. For example, 'threshold_3SD'. WHICHFS: The name (string) of the field containing the sampling rate of the raw data collection in Hz. OPTIONAL INPUTS: PREMINSTARTMS: Number of millseconds pre-transient to use as the start of the baseline window. Default: 800 PREMINENDMS: Number of millseconds pre-transient to use as the end of the baseline window. Default: 100 POSTTRANSIENTMS: Number of millseconds post-transient to use for the post peak baseline and trimmed data output. Default: 2000 QUANTIFICATIONHEIGHT: The height at which to characterize rise time, fall time, peak width, and AUC. Must be a number between 0 and 1. Default: 0.5 OUTPUTTRANSIENTDATA: Set to 1 to output cut data streams for each transient event for further analysis or plotting. Set to 0 to skip. Default: 1 OUTPUTS: DATA: The original data structure with sessiontransients_WHICHBLTYPE_THRESHOLDLABEL added in. For more details on individual variables, see the PASTa user guide page Transient Detection . The output contains four nested tables: INPUTS: Includes all required and optional inputs. If optional inputs are not specified, defaults will be applied. TRANSIENTQUANTIFICATION: Includes the quantified variables for each transient, including amplitude, rise time, fall time, width, and AUC. TRANSIENTSTREAMLOCS: Pre-transient baseline, transient peak, rise, and fall locations for each transient to match the cut transient stream data. _TRANSIENTSTREAMDATA: Cut data stream from baseline start to the end of the post-transient period for each transient event. NOTES: Threshold values should be calculated before using the findSessionTransients functions. Typically thresholds are set to 2-3 SDs. If the input data stream is Z scored, this can be the actual SD threshold number. If the input data stream is not Z scored, find the corresponding value to 2-3 SDs for each subject. For transient data outputs, each transient is in a separate row. If OUTPUTTRANSIENTDATA is set to anything other than 1, the TRANSIENTSTREAMLOCS and TRANSIENTSTREAMDATA tables will be skipped and not included in the output. EXAMPLE: % Prepare thresholds - since Z scored streams will be analyzed, input threshold as the desired SD. for eachfile = 1:length(data) data(eachfile).threshold3SD = 3; end % Find session transients based on pre-peak baseline window minimum [data] = findSessionTransients(data,'blmin','sigfiltz_normsession_trimmed','threshold3SD','fs'); % Find session transients based on pre-peak baseline window mean [data] = findSessionTransients(data,'blmean','sigfiltz_normsession_trimmed','threshold3SD','fs','preminstartms',600); % Find session transients based on pre-peak local minimum (last minumum before the peak in the baseline window) [data] = findSessionTransients(data,'localmin','sigfiltz_normsession_trimmed','threshold3SD','fs','quantificationheight',0.25); findSessionTransients_blmean Sub function to detect and quantify transient events in whole session data streams. Transient baselines are defined as the mean of the pre-transient baseline window. This function is called by the main function findSessionTransients , which sets default parameters for transient detection and quantification. If called outside the main function, users must specify all input values manually. INPUTS: DATA: This is a structure that contains at least the data stream you want to analyze, a field with the threshold values, and a field with the sampling rate of the data stream. WHICHSTREAM: The name (string) of the field containing the stream to be analyzed for transients. WHICHTHRESHOLD: The name (string) of the field containing the prepared numeric threshold values for each stream. For example, 'threshold_3SD'. WHICHFS: The name (string) of the field containing the sampling rate of the raw data collection in Hz. PREMINSTARTMS: Number of millseconds pre-transient to use as the start of the baseline window. Reccomended value: 800 PREMINENDMS: Number of millseconds pre-transient to use as the end of the baseline window. Reccomended value: 100 POSTTRANSIENTMS: Number of millseconds post-transient to use for the post peak baseline and trimmed data output. Reccomended value: 2000 QUANTIFICATIONHEIGHT: The height at which to characterize rise time, fall time, peak width, and AUC. Must be a number between 0 and 1. Reccomended value: 0.5 OUTPUTTRANSIENTDATA: Set to 1 to output cut data streams for each transient event for further analysis or plotting. Set to 0 to skip. Reccomended value: 1 OUTPUTS: DATA: The original data structure with sessiontransients_blmean_THRESHOLDLABEL added in. For more details on individual variables, see the PASTa user guide page Transient Detection . The output contains four nested tables: INPUTS: Includes all function inputs. TRANSIENTQUANTIFICATION: Includes the quantified variables for each transient, including amplitude, rise time, fall time, width, and AUC. TRANSIENTSTREAMLOCS: Pre-transient baseline start, pre-transient baseline end, transient peak, rise, and fall locations for each transient to match the cut transient stream data. _TRANSIENTSTREAMDATA: Cut data stream from baseline window start to the end of the post-transient period for each transient event. NOTES: If called outside the main findSessionTransients function, note that users will have to input all parameters manually and no defaults will be applied. As for the main function, threshold values should be calculated before using the findSessionTransients functions. Typically thresholds are set to 2-3 SDs. If the input data stream is Z scored, this can be the actual SD threshold number. If the input data stream is not Z scored, find the corresponding value to 2-3 SDs for each subject. For transient data outputs, each transient is in a separate row. If OUTPUTTRANSIENTDATA is set to anything other than 1, the TRANSIENTSTREAMLOCS and TRANSIENTSTREAMDATA tables will be skipped and not included in the output. EXAMPLE: % Prepare thresholds - since Z scored streams will be analyzed, input threshold as the desired SD. for eachfile = 1:length(data) data(eachfile).threshold3SD = 3; end % Find session transients by directly called in the findSessionTransients_blmean function [data] = findSessionTransients_blmean(data,'sigfiltz_normsession_trimmed','threshold3SD','fs',800,100,2000,0.5,1); findSessionTransients_blmin Sub function to detect and quantify transient events in whole session data streams. Transient baselines are defined as the minimum value within the pre-transient baseline window. This function is called by the main function findSessionTransients , which sets default parameters for transient detection and quantification. If called outside the main function, users must specify all input values manually. INPUTS: DATA: This is a structure that contains at least the data stream you want to analyze, a field with the threshold values, and a field with the sampling rate of the data stream. WHICHSTREAM: The name (string) of the field containing the stream to be analyzed for transients. WHICHTHRESHOLD: The name (string) of the field containing the prepared numeric threshold values for each stream. For example, 'threshold_3SD'. WHICHFS: The name (string) of the field containing the sampling rate of the raw data collection in Hz. PREMINSTARTMS: Number of millseconds pre-transient to use as the start of the baseline window. Reccomended value: 800 PREMINENDMS: Number of millseconds pre-transient to use as the end of the baseline window. Reccomended value: 100 POSTTRANSIENTMS: Number of millseconds post-transient to use for the post peak baseline and trimmed data output. Reccomended value: 2000 QUANTIFICATIONHEIGHT: The height at which to characterize rise time, fall time, peak width, and AUC. Must be a number between 0 and 1. Reccomended value: 0.5 OUTPUTTRANSIENTDATA: Set to 1 to output cut data streams for each transient event for further analysis or plotting. Set to 0 to skip. Reccomended value: 1 OUTPUTS: DATA: The original data structure with sessiontransients_blmin_THRESHOLDLABEL added in. For more details on individual variables, see the PASTa user guide page Transient Detection . The output contains four nested tables: INPUTS: Includes all function inputs. TRANSIENTQUANTIFICATION: Includes the quantified variables for each transient, including amplitude, rise time, fall time, width, and AUC. TRANSIENTSTREAMLOCS: Pre-transient baseline, transient peak, rise, and fall locations for each transient to match the cut transient stream data. TRANSIENTSTREAMDATA: Cut data stream from baseline window start to the end of the post-transient period for each transient event. NOTES: If called outside the main findSessionTransients function, note that users will have to input all parameters manually and no defaults will be applied. As for the main function, threshold values should be calculated before using the findSessionTransients functions. Typically thresholds are set to 2-3 SDs. If the input data stream is Z scored, this can be the actual SD threshold number. If the input data stream is not Z scored, find the corresponding value to 2-3 SDs for each subject. For transient data outputs, each transient is in a separate row. If OUTPUTTRANSIENTDATA is set to anything other than 1, the TRANSIENTSTREAMLOCS and TRANSIENTSTREAMDATA tables will be skipped and not included in the output. EXAMPLE: % Prepare thresholds - since Z scored streams will be analyzed, input threshold as the desired SD. for eachfile = 1:length(data) data(eachfile).threshold3SD = 3; end % Find session transients by directly called in the findSessionTransients_blmin function [data] = findSessionTransients_blmin(data,'sigfiltz_normsession_trimmed','threshold3SD','fs',800,100,2000,0.5,1); findSessionTransients_localmin Sub function to detect and quantify transient events in whole session data streams. Transient baselines are defined as the last local minimum value before the transient peak within the pre-transient baseline window. This function is called by the main function findSessionTransients , which sets default parameters for transient detection and quantification. If called outside the main function, users must specify all input values manually. INPUTS: DATA: This is a structure that contains at least the data stream you want to analyze, a field with the threshold values, and a field with the sampling rate of the data stream. WHICHSTREAM: The name (string) of the field containing the stream to be analyzed for transients. WHICHTHRESHOLD: The name (string) of the field containing the prepared numeric threshold values for each stream. For example, 'threshold_3SD'. WHICHFS: The name (string) of the field containing the sampling rate of the raw data collection in Hz. PREMINSTARTMS: Number of millseconds pre-transient to use as the start of the baseline window. Reccomended value: 800 PREMINENDMS: Number of millseconds pre-transient to use as the end of the baseline window. Reccomended value: 100 POSTTRANSIENTMS: Number of millseconds post-transient to use for the post peak baseline and trimmed data output. Reccomended value: 2000 QUANTIFICATIONHEIGHT: The height at which to characterize rise time, fall time, peak width, and AUC. Must be a number between 0 and 1. Reccomended value: 0.5 OUTPUTTRANSIENTDATA: Set to 1 to output cut data streams for each transient event for further analysis or plotting. Set to 0 to skip. Reccomended value: 1 OUTPUTS: DATA: The original data structure with sessiontransients_localmin_THRESHOLDLABEL added in. For more details on individual variables, see the PASTa user guide page Transient Detection . The output contains four nested tables: INPUTS: Includes all function inputs. TRANSIENTQUANTIFICATION: Includes the quantified variables for each transient, including amplitude, rise time, fall time, width, and AUC. TRANSIENTSTREAMLOCS: Pre-transient baseline (local minimum), transient peak, rise, and fall locations for each transient to match the cut transient stream data. TRANSIENTSTREAMDATA: Cut data stream from baseline window start to the end of the post-transient period for each transient event. NOTES: If called outside the main findSessionTransients function, note that users will have to input all parameters manually and no defaults will be applied. As for the main function, threshold values should be calculated before using the findSessionTransients functions. Typically thresholds are set to 2-3 SDs. If the input data stream is Z scored, this can be the actual SD threshold number. If the input data stream is not Z scored, find the corresponding value to 2-3 SDs for each subject. For transient data outputs, each transient is in a separate row. If OUTPUTTRANSIENTDATA is set to anything other than 1, the TRANSIENTSTREAMLOCS and TRANSIENTSTREAMDATA tables will be skipped and not included in the output. EXAMPLE: % Prepare thresholds - since Z scored streams will be analyzed, input threshold as the desired SD. for eachfile = 1:length(data) data(eachfile).threshold3SD = 3; end % Find session transients by directly called in the findSessionTransients_localmin function [data] = findSessionTransients_localmin(data,'sigfiltz_normsession_trimmed','threshold3SD','fs',800,100,2000,0.5,1); binSessionTransients Used to assign individual transient events to time bins within the session for analysis of changes in transients over time. By default, sessions will be divided into 5 minute bins. Number of bins per session will be calculated by the function. Users have the option to override the defaults to change the bin length and/or manually set the number of bins per session. INPUTS: DATA: This is a structure that contains at least the data stream you want to analyze, a field with the threshold values, and a field with the sampling rate of the data stream. WHICHSTREAM: The name (string) of the field containing the stream to be analyzed for transients. WHICHFS: The name (string) of the field containing the sampling rate of the raw data collection in Hz. WHICHTRANSIENTS: The name (string) of the parent field containing the table of transietns that you want to identify bins for. This is the output of the findSessionTransients function. The name usually follows the convention sessiontransients_WHICHBLTYPE_WHICHTHRESHOLD OPTIONAL INPUTS: * WHICHTRANSIENTSTABLE: The name (string) of the field within WHICHTRANSIENTS that contains the quantification of individual transient events. This input only needs to be specified if not using the format output from the FINDSESSIONTRANSIENTS functions. Default: 'transientquantification' * WHICHMAXLOCS: NThe name (string) of the field containing the transient max locations (indexes) relative to the whole session. This input only needs to be specified if not using the format output from the FINDSESSIONTRANSIENTS functions. Default: 'maxloc' * BINLENGTHMINS: Numeric; Bin length in number of minutes. Default: 5 * NBINSOVERRIDE: Numeric; Manual override to set the number of bins. If set to anything other than 0, users can override the stream-length based calculation of the number of bins per session and set their own number. Default: 0 OUTPUTS: DATA: The original data structure with sessiontransients_localmin_THRESHOLDLABEL added in. For more details on individual variables, see the PASTa user guide page Transient Detection . The output contains four nested tables: INPUTS: Includes all function inputs. TRANSIENTQUANTIFICATION: Includes the quantified variables for each transient, including amplitude, rise time, fall time, width, and AUC. TRANSIENTSTREAMLOCS: Pre-transient baseline (local minimum), transient peak, rise, and fall locations for each transient to match the cut transient stream data. TRANSIENTSTREAMDATA: Cut data stream from baseline window start to the end of the post-transient period for each transient event. NOTES: If called outside the main findSessionTransients function, note that users will have to input all parameters manually and no defaults will be applied. As for the main function, threshold values should be calculated before using the findSessionTransients functions. Typically thresholds are set to 2-3 SDs. If the input data stream is Z scored, this can be the actual SD threshold number. If the input data stream is not Z scored, find the corresponding value to 2-3 SDs for each subject. For transient data outputs, each transient is in a separate row. If OUTPUTTRANSIENTDATA is set to anything other than 1, the TRANSIENTSTREAMLOCS and TRANSIENTSTREAMDATA tables will be skipped and not included in the output. EXAMPLE: % Prepare thresholds - since Z scored streams will be analyzed, input threshold as the desired SD. for eachfile = 1:length(data) data(eachfile).threshold3SD = 3; end % Find session transients by directly called in the findSessionTransients_localmin function [data] = findSessionTransients_localmin(data,'sigfiltz_normsession_trimmed','threshold3SD','fs',800,100,2000,0.5,1); exportSessionTransients Used to create a table of all transient events across all sessions included in the file key and data structure, and exports the table of transient quantification to a csv file. This function depends on the output from findSessionTransients . The creation of a csv file is helpful for users who prefer to run statistical analyses or make plots in other programs than MATLAB, such as R Studio. INPUTS: DATA: This is a structure that contains at least the output from findSessionTransients . WHICHTRANSIENTS: The name (string) of the parent field containing the table of transietns that you want to identify bins for. This is the output of the findSessionTransients function. The name usually follows the convention sessiontransients_WHICHBLTYPE_WHICHTHRESHOLD EXPORTFILEPATH: A string with the path to the folder location where the created table should be saved to. NOTE: The path must end in a forward slash. ADDVARIABLES: A cell array containing any additional variables from the data structure to be added to the transients table. Variables will be added to every row of the output structure. Cell array inputs must be the names of fields in the data structure. At a minimum, this should contain the subject ID. If multiple sessions per subject are included in the data structure, make sure a session ID variable is also included. For example: {'Subject', 'SessionID', 'Treatment'} OPTIONAL INPUTS: * WHICHTRANSIENTSTABLE: The name (string) of the field within WHICHTRANSIENTS that contains the quantification of individual transient events. This input only needs to be specified if not using the format output from the FINDSESSIONTRANSIENTS functions. Default: 'transientquantification' * FILENAME: A string with a custom name for the output csv file. If not specified, the file name will be generated as 'WHICHTRANSIENTS_AllSessionExport_DAY-MONTH-YEAR.csv' OUTPUTS: This function outputs a csv file with all transients for all sessions in the data structure. The file will be output at the specified file path. If the function is called into an object, the table ALLTRANSIENTS will also be saved to an object in the MATLAB workspace. NOTES: The file path must include the full path for the folder including the computer address, and must end in a forward slash. If the file path is not specified properly, the function won't be able to automatically save the csv file. Make sure to add all relevant experimental variables to the ADDVARIABLES list. This includes at a minimum the Subject ID and a session ID, but it's wise to include other key variables to make future analyses easier. Alternatively, you can keep only the Subject ID and a session ID and match the data to the subject and file keys later, which would include all subject and experimental variables. EXAMPLE: % Export transients with added fields for subject and treatment= addvariables = {'Subject','TreatNum','InjType'}; % Prepare variables to append to each transient % To just output the csv file: exportSessionTransients(data,'sessiontransients_blmean_threshold3SD',analysispath,addvariables); % To output the csv file and add the created table to a MATLAB data structure: alltransients = exportSessionTransients(data,'sessiontransients_blmean_threshold3SD',analysispath,addvariables); Visualization and Plotting Functions plotTraces Main function to plot whole session fiber photometry traces. This function will plot the streams sig, baq, baq_scaled, sigsub, and sigfilt for a single session. Use this function in a loop to plot streams for all sessions in the data structure. This plot function plots the data streams with time in minutes on the x axis. To modify the plot or add additional features like session relevant time stamps, use the plot in a loop. Modified plots must be saved in the external loop. If modifications are not neccesaary, then optional inputs can be specified to directly output and save the plot to a filepath location. INPUTS: DATA: This is a structure that contains at least the data stream you want to analyze, a field with the threshold values, and a field with the sampling rate of the data stream. WHICHFILE: The file number to be plotted. MAINTITLE: The main title (string) to be displayed for the overall plot above the individual tiles. For example, '427 - Treatment: Morphine' OPTIONAL INPUTS: SAVEOUTPUT: Set to 1 to automatically save trace plots as png to the plot file path. Default: 0 PLOTFILEPATH: Required if SAVEOUTPUT is set to 1. The specific path to save the plot to. Note that this must be the entire path from computer address to folder ending in the filename for the specific plot. For example: 'C:\\Users\\rmdon\\Box\\Injection Transients\\Figures\\SessionTraces_427_Morphine.png' OUTPUTS: ALLTRACES: A plot object containing subplots for each input stream. EXAMPLE: Single Session %% Plot the first session and automatically save the output maintitle = append(num2str(data(1).Subject),' - Treatment: ',data(1).InjType); % Create title string for current plot plotfilepath = 'C:\\Users\\rmdon\\Box\\Injection Transients\\Figures\\SessionTraces_427_Morphine.png'; % Create plot file path for the current plot plotTraces(data,1,maintitle,'saveoutput',1,'plotfilepath',plotfilepath); EXAMPLE: All Sessions %% Plot whole session streams for each file for eachfile = 1:length(data) maintitle = append(num2str(data(eachfile).Subject),' - Treatment: ',data(eachfile).InjType); % Create title string for current plot alltraces = plotTraces(data,eachfile,maintitle); for eachtile = 1:5 nexttile(eachtile) xline(data(eachfile).injt(1),'--','Injection','Color','#C40300','FontSize',8) xline(data(eachfile).injt(2),'--','Color','#C40300','FontSize',8) end set(gcf, 'Units', 'inches', 'Position', [0, 0, 8, 9]); plotfilepath = append(figurepath,'SessionTraces_',num2str(data(eachfile).Subject),'_',data(eachfile).InjType,'.png'); exportgraphics(gcf,plotfilepath,'Resolution',300) end plotFFTs Main function to plot whole session fiber photometry FFT frequency magnitude plots. This function will plot the streams sig, baq, baq_scaled, sigsub, and sigfilt for a single session. Use this function in a loop to plot streams for all sessions in the data structure. By default, only frequencies up to 20 hz will be plotted but users can specify an optional input to adjust the cutoff lower or higher, or set to the actual max for the stream FFT. To modify the plot or add additional features, use the plot in a loop. Modified plots must be saved in the external loop. If modifications are not neccesaary, then optional inputs can be specified to directly output and save the plot to a filepath location. INPUTS: DATA: This is a structure that contains at least the data stream you want to analyze, a field with the threshold values, and a field with the sampling rate of the data stream. WHICHFILE: The file number to be plotted. MAINTITLE: The main title (string) to be displayed for the overall plot above the individual tiles. For example, '427 - Treatment: Morphine' WHICHFS: The name (string) of the field that contains the sampling rate of the data streams. OPTIONAL INPUTS: XMAX: The desired frequency cutoff for the x axis (hz). Frequencies above this value will be excluded from the plots. To plot all frequencies, set to actual'. Default: 20 SAVEOUTPUT: Set to 1 to automatically save trace plots as png to the plot file path. Default: 0 PLOTFILEPATH: Required if SAVEOUTPUT is set to 1. The specific path to save the plot to. Note that this must be the entire path from computer address to folder ending in the filename for the specific plot. For example: 'C:\\Users\\rmdon\\Box\\Injection Transients\\Figures\\SessionTraces_427_Morphine.png' OUTPUTS: ALLFFTS: A plot object containing subplots for each input stream. EXAMPLE: Single Session %% Plot the first session and automatically save the output maintitle = append(num2str(data(1).Subject),' - Treatment: ',data(1).InjType); % Create title string for current plot plotfilepath = 'C:\\Users\\rmdon\\Box\\Injection Transients\\Figures\\SessionTraces_427_Morphine.png'; % Create plot file path for the current plot plotTraces(data,1,maintitle,'fs','saveoutput',1,'plotfilepath',plotfilepath); EXAMPLE: All Sessions %% Plot stream FFTs for each file for eachfile = 1:length(data) maintitle = append(num2str(data(eachfile).Subject),' - Treatment: ',data(eachfile).InjType); % Create title string for current plot allffts = plotFFTs(data,eachfile,maintitle); set(gcf, 'Units', 'inches', 'Position', [0, 0, 8, 9]); plotfilepath = append(figurepath,'SessionFFTs_',num2str(data(eachfile).Subject),'_',data(eachfile).InjType,'.png'); exportgraphics(gcf,plotfilepath,'Resolution',300) end","title":"Function Documentation"},{"location":"functiondocumentation/#function-documentation-overview","text":"This page contains additional documentation for each function within PASTa, as well as examples of inputs. Detailed documentation is also available within each MATLAB function and can be viewed via the MATLAB help window or by opening the function file.","title":"Function Documentation Overview"},{"location":"functiondocumentation/#data-preparation-functions","text":"This set of functions is used to prepare raw photometry data, match it with experimental metadata, and load data into a structure in MATLAB. Functions are provided to handle data collected via TDT equipment and software Synapse, or a generic file structure with data streams saved to CSV files.","title":"Data Preparation Functions"},{"location":"functiondocumentation/#createexperimentkey","text":"Combine subject key and file key into a single data structure, appending the provided rootdirectory to create full paths to stored data locations. INPUTS: ROOTDIRECTORY: String. The top-level directory path unique to your system. For example: 'C:\\Users\\rmdon\\'. Must end with a slash/backslash. SUBJECTKEYNAME: String. The name (or full path) of the subject key CSV file (e.g. 'mySubjectKey.csv'). Must contain at least the variable 'SubjectID' . To omit a subject key and only load in the file key, set subjectkeyname = ''. If empty (''), the subject key is skipped and only FILEKEYNAME is loaded. See the user guide Data Preparation for more details. FILEKEYNAME: String. The name (or full path) of the file key CSV file (e.g. 'myFileKey.csv'). Must contain 'SubjectID' , 'BlockFolder' , 'RawFolderPath' , and 'ExtractedFolderPath' columns at minimum. Paths in 'RawFolderPath' and 'ExtractedFolderPath'should each end with a slash/backslash. OUTPUTS: EXPERIMENTKEY: A data structure of the combined file and subject key data. The 'rootdirectory' is prepended to each row's 'RawFolderPath' and 'ExtractedFolderPath' , while the 'BlockFolder' name is appended to both. EXAMPLE: rootdirectory = 'C:\\Users\\rmdon\\'; subjKey = 'subjectKey.csv'; fileKey = 'fileKey.csv'; experimentkey = createExperimentKey(rootdirectory, subjKey, fileKey); % Load keys into a data structure called experimentkey NOTES: FILEKEY must contain at a minimum the fields SubjectID , BlockFolder , RawFolderPath , and ExtractedFolderPath . SUBJECTKEY must contain at a minimum the field SubjectID Folder paths must end with a slash. The subject and file keys are joined based on SubjectID . Subject key must contain every subjects in the file key. If there is a mismatch, you will receive an error message that the right table does not contain all the key variables that are in the left table. The error message will display the unique subject IDs present in each key so you can determine where the mismatch occurred. Fields in subject and file key must be named uniquely. The only field that should be named the same in both keys is Subject.","title":"createExperimentKey"},{"location":"functiondocumentation/#extracttdtdata","text":"This function is used to extract TDT data from saved blocks recorded via the software Synapse . For each block, extractTDTdata calls the function \"TDTbin2mat\" (TDT, 2025) and inputs the RawFolderPath to extract fiber photometry data recorded with Synapse . Extracted blocks are parsed it into a single data structure containing all fields, streams, and event epocs. Extracted signal streams are trimmed to remove the first 5 seconds by default (trimming can be adjusted if desired). The function will identify the signal channel by matching the names in the input SIGSTREAMNAMES and the control channel by matching the names in the input BAQSTREAMNAMES. The name inputs can include a list of stream names if channel naming conventions vary by rig. Each block is saved as a separate data structure in a '.mat' file at the location specified by the inputs in extractedfolderpaths . For more details on preparing TDT data blocks, see the user guide section on Data Preparation: Data Organization . INPUTS: RAWFOLDERPATHS: String array of raw TDT block folder paths. The string array should contain one column with each full path in a separate row. If using the createExperimentKey function, this can be created from the experiment key data structure. For example: rawfolderpaths = string({experimentkey.RawFolderPath})'; EXTRACTEDFOLDERPATHS: String array of corresponding output paths where extracted data is saved. If using the createExperimentKey function, this can be created from the experiment key. For example: extractedfolderpaths = string({experimentkey.ExtractedFolderPath})'; SIGSTREAMNAMES: Cell array of possible stream names for the \"signal\" channel (e.g. {'x65A','465A'}). NOTE: Only one stream per file can be treated as signal. If different files have different stream names, include all stream names in the cell array. BAQSTREAMNAMES: Cell array of possible stream names for the \"background\" channel (e.g. {'x05A','405A'}). NOTE: Only one stream per file can be treated as background. A cell array containing strings with the names of the streams to be treated as background. If different files have different background stream names, include all stream names in the cell array. OPTIONAL INPUTS: 'trim': Numeric; Number of seconds to trim from the start and end of each recording. Default: 5 'skipexisting': Numeric (0 or 1; Default: 1); If 1, skip extracting any session for which an output file already exists. If 0, re-extract and overwrite. This allows the user to toggle whether or not to extract every block, or only blocks that have not previously been extracted. If not specified, defaults to skip previously extracted blocks (1). OUTPUTS: Saved .mat data structures for each block in the location specified by extractedfolderpaths . EXAMPLE - DEFAULT: sigstreamnames = {'x65A', '465A'}; % All names of signal streams across files baqstreamnames = {'x05A', '405A'}; % All names of background streams across files rawfolderpaths = string({experimentkey.RawFolderPath})'; % Create string array of raw folder paths extractedfolderpaths = string({experimentkey.ExtractedFolderPath})'; % Create string array of extracted folder paths % Extract and save data structures for each file extractTDTdata(rawfolderpaths,extractedfolderpaths,sigstreamnames,baqstreamnames); EXAMPLE - MANUALLY SPECIFIED TRIM AND SKIPEXISTING: trim = 3; % Set trim to 3 seconds skipexisting = 0; % Extract all blocks sigstreamnames = {'x65A', '465A'}; % All names of signal streams across files baqstreamnames = {'x05A', '405A'}; % All names of background streams across files rawfolderpaths = string({experimentkey.RawFolderPath})'; % Create string array of raw folder paths' extractedfolderpaths = string({experimentkey.ExtractedFolderPath})'; % Create string array of extracted folder paths' % Extract and save data structures for each file extractTDTdata(rawfolderpaths,extractedfolderpaths,sigstreamnames,baqstreamnames,'trim',trim,'skipexisting',skipexisting);","title":"extractTDTdata"},{"location":"functiondocumentation/#extractcsvdata","text":"This function is used to extract data collected from non-TDT systems (e.g., Neurophotometrics, Doric). Data must first be prepared in the generic CSV format, with each session saved as a folder containing CSV files of the signal, background, session recording parameters, and event epochs (optional). Note that session recording parameters must at least include the sampling rate as the variable fs . For more details on preparing CSV data files, see the user guide section on Data Preparation: Data Organization . For each block, extractCSVdata extracts the signal and background streams, recording parameters, and any specific event epochs into a single data structure containing all recording parameters, streams, and included event epochs. Extracted signal and background streams are trimmed to remove the first 5 seconds by default (trimming can be adjusted if desired). Each block is saved as a separate data structure in a '.mat' file at the location specified by the inputs in extractedfolderpaths . INPUTS: RAWFOLDERPATHS: String array of raw TDT block folder paths. The string array should contain one column with each full path in a separate row. If using the createExperimentKey function, this can be created from the experiment key data structure. For example: rawfolderpaths = string({experimentkey.RawFolderPath})'; EXTRACTEDFOLDERPATHS: String array of corresponding output paths where extracted data is saved. If using the createExperimentKey function, this can be created from the experiment key. For example: extractedfolderpaths = string({experimentkey.ExtractedFolderPath})'; SIGSTREAMNAME: String; Name of csv files containing the \"signal\" channel (e.g., 'sig' ). NOTE: Only one stream per file can be treated as signal. BAQSTREAMNAME: String; Name of csv files containing the \"background\" channel (e.g. 'baq' ). NOTE: Only one stream per file can be treated as background. OPTIONAL INPUTS: 'loadepocs': Logical; Set to 1 to load event epoch files in the 'Raw Data' session folders. 'epocsnames': Cell array of file names of csv files containing event epochs to be loaded into the data structure. Note: Each type of epoch should be saved to a separate csv file. Pass all event epoch files names to epocsnames . 'trim': Numeric; Number of seconds to trim from the start and end of each recording. Default: 5 'skipexisting': Numeric (0 or 1; Default: 1); If 1, skip extracting any session for which an output file already exists. If 0, re-extract and overwrite. This allows the user to toggle whether or not to extract every block, or only blocks that have not previously been extracted. If not specified, defaults to skip previously extracted blocks (1). OUTPUTS: Saved .mat data structures for each block in the location specified by extractedfolderpaths . EXAMPLE - DEFAULT: sigstreamname = 'sig'; % All names of signal streams across files baqstreamname = 'baq''; % All names of background streams across files rawfolderpaths = string({experimentkey.RawFolderPath})'; % Create string array of raw folder paths extractedfolderpaths = string({experimentkey.ExtractedFolderPath})'; % Create string array of extracted folder paths % Extract and save data structures for each file extractCSVdata(rawfolderpaths,extractedfolderpaths,sigstreamname,baqstreamname); EXAMPLE - WITH EPOCHS: sigstreamname = 'sig'; % All names of signal streams across files baqstreamname = 'baq''; % All names of background streams across files epocsnames = {'strt', 'injt'}; % Prepare names of csv files containing event epochs rawfolderpaths = string({experimentkey.RawFolderPath})'; % Create string array of raw folder paths extractedfolderpaths = string({experimentkey.ExtractedFolderPath})'; % Create string array of extracted folder paths % Extract and save data structures for each file extractCSVdata(rawfolderpaths,extractedfolderpaths,sigstreamname,baqstreamname,'loadepocs',1,'epocsnames',epocsnames); EXAMPLE - MANUALLY SPECIFIED TRIM AND SKIPEXISTING: trim = 3; % Set trim to 3 seconds skipexisting = 0; % Extract all blocks sigstreamname = 'sig'; % All names of signal streams across files baqstreamname = 'baq''; % All names of background streams across files rawfolderpaths = string({experimentkey.RawFolderPath})'; % Create string array of raw folder paths' extractedfolderpaths = string({experimentkey.ExtractedFolderPath})'; % Create string array of extracted folder paths' % Extract and save data structures for each file extractCSVdata(rawfolderpaths,extractedfolderpaths,sigstreamname,baqstreamname,'trim',trim,'skipexisting',skipexisting);","title":"extractCSVdata"},{"location":"functiondocumentation/#loadkeydata","text":"For use with previously extracted data. Compatible with data structures extracted by either the extractTDTdata or extractCSVdata functions. loadKeydata loads previously extracted .mat data blocks with individaul session data into a main data structure for further analysis. Each block is one row. The input experiment key must include at a minimum the extracted folder path location of the block. Any additional information about the subject and session in the experiment key will be matched to the extracted data. INPUTS: EXPERIMENTKEY: A prepared data structure with at minimum the ExtractedFolderPath containing the full path to the individual session data structures to be loaded. The EXPERIMENTKEY can be prepared with the LOADKEYS function. OUTPUTS: DATA: the input data structure with each individual extracted block added by row. EXAMPLE: [rawdata] = loadKeydata(experimentkey); % Load data based on the experiment key into the structure 'rawdata'","title":"loadKeydata"},{"location":"functiondocumentation/#cropfpdata","text":"Used to crop data streams to remove portions that are not to be included in analysis (such as the the first two minutes of the session, or the first samples before a hardware control program is initiated). Crops all specified data streams from the index in cropstart to the index in cropend , and adjusts specified event epochs by the amount cropped by cropstart . Users must pre-prepare the crop start and end indexes to specify as inputs for the function. INPUTS: DATA: Data structure array containing at least the specified input fields. CROPSTARTFIELDNAME: String; Field name for cropping start index. For example: 'sessionstart'. CROPENDFIELDNAME: String; Field name for cropping end index. For example: 'sessionend'. STREAMFIELDNAMES: A cell array containing the names (strings) of all the data streams to be cropped. For example: {'sig', 'baq'} OPTIONAL INPUTS: 'epocsfieldnames': A cell array containing the field names of all the event epochs to be adjusted for cropping (event epoch - (start loc - 1)). OUTPUTS: DATA: The data structure with the specified data streams containing the cropped data, and event epochs adjusted to maintain alignment with the data streams. EXAMPLE: WITHOUT EVENT EPOCS cropstartfieldname = 'sessionstart'; % Name of field with session start index cropendfieldname = 'sessionend'; % Name of field with session end index streamfieldnames = {'sig', 'baq','time'}; % Names of all streams to crop % Output cropped data into new structure called data [data] = cropFPdata(rawdata,cropstartfieldname,cropendfieldname,streamfieldnames); EXAMPLE: WITH EVENT EPOCS cropstartfieldname = 'sessionstart'; % Name of field with session start index cropendfieldname = 'sessionend'; % Name of field with session end index streamfieldnames = {'sig', 'baq','time'}; % Names of all streams to crop epocsfieldnames = {'injt','sess'}; % Field names of event epocs to adjust to maintain relative position % Output cropped data into new structure called data [data] = cropFPdata(rawdata,cropstartfieldname,cropendfieldname,streamfieldnames,'epocsfieldnames',epocsfieldnames);","title":"cropFPdata"},{"location":"functiondocumentation/#signal-processing-functions","text":"This set of functions is used to process raw fiber photometry data prior to further analyses. Functions are available to perform background subtraction, filter, and normalize the data streams. While default options have been chosen to provide users with a conservative starting point, multiple options for signal processing and normalization are available to allow for customization to adapt to experimental needs. For a full discussion of available signal processing methods, see the user guide section Signal Processing .","title":"Signal Processing Functions"},{"location":"functiondocumentation/#subtractfpdata","text":"Subtracts background fluorescence stream (eg, 405nm 'baq') from the signal stream (eg, 465nm 'sig'), converts the subtracted signal to \u0394F/f, and applies a filter to denoise the output. Users must input at a minimum the data structure with the raw data, the names of the fields containing the signal and background streams, and the name of the field containing the sampling rate of the collected data. INPUTS: DATA: Data structure array containing at least the specified input fields (signal and background streams, sampling rate) SIGFIELDNAME: String; name of the field in DATA containing the signal data stream (e.g., 'sig' ). BAQFIELDNAME: String; name of the field in DATA containing the background data stream (e.g., 'baq' ). FSFIELDNAME: String; name of the field in DATA containing the sampling rate of the fiber photometry data streams in Hz (e.g., 'fs' ). Note: The sampling rate must be specified in Hz for filtering to be properly applied. OPTIONAL INPUTS: 'baqscalingtype': A string to specify the type of background scaling to apply. Options are 'frequency' , 'sigmean' , 'OLS' , 'detrendedOLS' , 'smoothedOLS' , 'biexpOLS' , 'biexpQuartFit' , or 'IRLS' . Default: 'frequency' . 'frequency': Default ; Scales the background to the signal channel based on ratio of the power in the specified frequency bands in the FFT (frequency domain) of the channels. 'sigmean': Scales the background to the signal channel based on the ratio of the mean of the signal to the mean of the background (time domain). 'OLS': Uses ordinary least-squares regression to generate scaled background. 'detrendedOLS': Removes the linear trend from signal and background streams prior to using ordinary least-squares regression to generate scaled background. 'smoothedOLS': Applies lowess smoothing to the signal and background streams prior to using ordinary least-squares regression to generate scaled background. 'biexpOLS': Fits and removes a bi-exponential decay function from the raw background and signal streams, then scales the corrected background to the corrected signal with ordinary least-squares regression. 'biQuartFit': Fits and removes a bi-exponential decay function from the raw background and signal streams, then scales the corrected background to the corrected signal based on the interquartile range. 'IRLS': Uses iteratively reweighted least squares regression to generate scaled background. 'baqscalingfreqmin': Only used with 'frequency' scaling. Numeric frequency (Hz) minimum threshold for scaling the background to signal channel. Frequencies above this value will be included in the scaling factor determination. Default: 10 Hz. 'baqscalingfreqmax': Only used with 'frequency' scaling. Numeric frequency (Hz) maximum threshold for scaling the background to signal channel. Frequencies below this value will be included in the scaling factor determination. Default: 100 Hz. 'baqscalingperc': Only used with 'frequency' and 'sigmean' scaling. Adjusts the background scaling factor to be a percent of the derived scaling factor value. Default: 1 (100%). 'subtractionoutput': Output type for the subtracted data. Default: 'dff' 'dff': Outputs subtracted signal as \u0394F/F. ((signal - scaled background) / scaled background) 'df': Outputs subtracted signal as \u0394F. (signal - scaled background) 'artifactremoval' : Logical (0 or 1); set to 1 to detect and remove artifacts from data streams. Default: 0 (false). Note: see removeStreamArtifacts function documentation for more detail. 'filtertype': A string to specify the type of filter to apply after subtraction. Default: 'bandpass'. 'nofilter': No filter will be applied. 'bandpass': A bandpass Butterworth filter will be applied. 'highpass': Only a high pass Butterworth filter will be applied. 'lowpass': Only a low pass Butterworth filter will be applied. 'padding': Defaults to 1, which applies padding prior to filtering. Padding takes the first 10% of the stream, flips it, and appends it to the data before applying the filter to prevent edge effects. Appended data is trimmed after filtering. Set to 0 to turn off padding of data streams. Default: 1. 'paddingperc': Percent of data length to use to determine the number of samples to be appended to the beginning and end of data in padding. Set to minimum of 0.1 (10%). Default: 0.1 (10%). 'filterorder': The order to be used for the chosen Butterworth filter. Default: 3. 'highpasscutoff': The cutoff frequency (Hz) to be used for the high pass Butterworth filter. Default: 2.2860. 'lowpasscutoff': The cutoff frequency (Hz) to be used for the low pass butterworth filter. Default: 0.0051. NOTE: 'bandpass' applies both the high and low pass cutoffs to design the filter. OUTPUTS: DATA: The original data structure with added fields with the scaled background ('baqscaled'), subtracted signal ('sigsub'), and subtracted and filtered signal ('sigfilt'). All inputs and defaults will be added to the data structure under the field 'inputs'. NOTE: If using BAQSCALINGMETHOD 'detrendedOLS' , additional fields containing the detrended signal and background ('sigdetrend' and 'baqdetrend') will be added to the data frame. If using BAQSCALINGMETHOD 'smoothedOLS' , additional fields containing the smoothed signal and background ('sigsmoothed' and 'baqsmoothed') will be added to the data frame. If using BAQSCALINGMETHOD 'biexpOLS' or 'biexpQuartFit' , additional fields containing the bi-exponential decay corrected signal and background ('sigbiexpcorrected' and 'baqbiexpcorrected') will be added to the data frame. EXAMPLE - DEFAULT: sigfieldname = 'sig'; baqfieldname = 'baq'; fsfieldname = 'fs'; data = subtractFPdata(data,sigfieldname,baqfieldname,fsfieldname); EXAMPLE - Frequency Scaling with 20Hz Minimum Threshold and Highpass Filter Only: sigfieldname = 'sig'; baqfieldname = 'baq'; fsfieldname = 'fs'; data = subtractFPdata(data,sigfieldname,baqfieldname,fsfieldname,'baqscalingfreqmin',20,'filtertype,'highpass'); EXAMPLE - OLS Scaling: sigfieldname = 'sig'; baqfieldname = 'baq'; fsfieldname = 'fs'; data = subtractFPdata(data,sigfieldname,baqfieldname,fsfieldname,'baqscalingtype','OLS'); EXAMPLE - IRLS Scaling: sigfieldname = 'sig'; baqfieldname = 'baq'; fsfieldname = 'fs'; data = subtractFPdata(data,sigfieldname,baqfieldname,fsfieldname,'baqscalingtype','IRLS');","title":"subtractFPdata"},{"location":"functiondocumentation/#removestreamartifacts","text":"Detects and removes artifacts from fiber photometry data streams. Called by the subtractFPdata function prior to filtering if the optional input removeartifacts is set to 1. INPUTS: DATASTREAM: Numeric array; The signal data for artifact removal. STREAMFIELDNAME: String; Name of the stream being processed. FS: Numeric; Sampling rate of the data stream being processed (Hz). OPTIONAL INPUTS: 'outlierthresholdk': ANumeric, Integer; Multiplier for outlier threshold detection. Default: 3 'artifactremovalwindow': Numeric; Seconds before and after an artifact to replace with NaNs. Default: 0.3 'artifactampthresh_max': Numeric; Standard deviation threshold for high artifacts. Default: 8 'artifactampthresh_min': Numeric; Standard deviation threshold for low artifacts. Default: 8 'bucketsizeSecs': Numeric; Bucket time window length (seconds) for amplitude and mean calculations. OUTPUTS: ARTIFACTREMOVALDATA: Data structure containing: STREAMFIELDNAME_AR: Data stream with artifacts replaced by NaNs STREAMFIELDNAME_AM: Data stream with artifacts replaced by mean values NUMARTIFACTS: Number of detected artifacts ARTIFACTS: Table of artifact details (locations, values, start/end indices) EXAMPLE: Default Parameters datastream = [data(1).sigsub]; % Prepare numeric vector of a stream for one recording session streamfieldname = 'sigsub'; % Field name of the prepared *datastream* input for artifact removal fs = data(1).fs; % Prepare sampling rate of the data stream in Hz % Detect and Remove Artifacts [artifactremovaldata] = removeStreamArtifacts(datastream,streamfieldname,fs); EXAMPLE: Increased Artifact Threshold datastream = [data(1).sigsub]; % Prepare numeric vector of a stream for one recording session streamfieldname = 'sigsub'; % Field name of the prepared *datastream* input for artifact removal fs = data(1).fs; % Prepare sampling rate of the data stream in Hz % Detect and Remove Artifacts [artifactremovaldata] = removeStreamArtifacts(datastream,streamfieldname,fs,'artifactampthresh_max',10,'artifactampthresh_min',10);","title":"removeStreamArtifacts"},{"location":"functiondocumentation/#preparestreamfft","text":"Helper function to compute the one-sided Fast Fourier Transform (FFT) of a data stream for analysis and plotting. This function is called by the subtractFPdata , plotFFTmag , and plotFFTpower functions. INPUTS: STREAMDATA: Numeric vector; the input data stream to be transformed. FS: Positive scalar; the sampling rate of the data stream in Hz. OUTPUTS: STREAMFFT: Numeric vector; the magnitudes of the one-sided FFT of the input data stream. STREAMF: Numeric vector; the corresponding frequency values in Hz for the FFT magnitudes. EXAMPLE: streamdata = [data(1).sig]; % Prepare numeric vector of the signal stream for one recording session fs = data(1).fs; % Prepare sampling rate of the data stream in Hz % Compute the FFT [streamFFT, streamF] = preparestreamFFT(streamdata, fs);","title":"prepareStreamFFT"},{"location":"functiondocumentation/#normsession","text":"Normalizes a specified data stream to Z-score based on the entire session mean and standard deviation. INPUTS: DATA: Data structure array; each element (row) represents a single recording session and must contain the field specified by STREAMFIELDNAME. STREAMFIELDNAME: String; the name of the field within DATA to be normalized. For example: 'sigfilt' OUTPUTS: DATA: The original data structure with the added field 'data.WHICHSTREAMz_normsession' containing the whole session normalized signal. EXAMPLE: [data] = normSession(data,'sigfilt'); % Outputs whole session z score","title":"normSession"},{"location":"functiondocumentation/#normbaseline","text":"Normalizes a specified data stream to Z-score based on the specified session baseline period. INPUTS: DATA: Data structure array; each element (row) represents a single recording session and must contain the field specified by STREAMFIELDNAME. STREAMFIELDNAME: String; the name of the field within DATA to be normalized. For example: 'sigfilt' BLSTARTFIELDNAME: String; The name of the field containing the index of the start of the baseline period. BLENDFIELDNAME: String; The name of the field containing the index of the end of the baseline period. OUTPUTS: DATA: The original data structure with the added field 'data.WHICHSTREAMz_normbaseline' containing the baseline normalized signal. EXAMPLE: % Prepare baseline start and end indices based on event epoch for eachfile = 1:length(data) data(eachfile).BLstart = 1; data(eachfile).BLend = data(eachfile).injt(1); end % Normalize filtered signal to session baseline mean and SD [data] = normBaseline(data,'sigfilt','BLstart','BLend');","title":"normBaseline"},{"location":"functiondocumentation/#normcustom","text":"Normalizes the whole session data stream based on a custom period input as a separate stream field. INPUTS: DATA: Data structure array; each element (row) represents a single recording session and must contain the field specified by STREAMFIELDNAME. FULLSTREAMFIELDNAME: String; the name of the field within DATA to be normalized. For example: 'sigfilt' _CUSTOMSTREAMFIELDNAME:__ String; The name of the field containing the cut data stream to use as the reference for normalization. OUTPUTS: DATA: The original data structure with the added field 'data.WHICHSTREAMz_normcustom' containing the baseline normalized signal. EXAMPLE: % Prepare custom data stream for normalization using the pre-trial and post-trial periods of the session for eachfile = 1:length(data) data(eachfile).customnorm = [data(eachfile).sigfilt(1:data(eachfile).trial(1)), data(eachfile).sigfilt(data(eachfile).trial(30):length(data(eachfile).sigfilt))]; end % Normalize filtered signal to custom session period mean and SD [data] = normCustom(data,'sigfilt','customnorm');","title":"normCustom"},{"location":"functiondocumentation/#preparestreamfft_1","text":"Helper function to take the FFT of a data stream and output the magnitudes and corresponding frequencies for analysis, plotting, and diagnostics. This function is called by the plot function plotFFTs but can also be used to prepare FFTs for each stream of data for all sessions with use in a for loop. INPUTS: STREAMDATA: An array containing the values from a single collected data stream. FS: The sampling rate of the data stream. OUTPUTS: STREAMFFT: An array containing the prepared FFT magnitudes of the input stream. STREAMF: An array containing the corresponding frequencies to the prepared FFT magnitudes in STREAMFFT. EXAMPLE OF SINGLE SESSION SIGNAL: [streamFFT,streamF] = preparestreamFFT(data(1).sig,data(eachfile).fs); EXAMPLE OF ALL SESSIONS AND STREAMS: %% Prepare FFTs of all streams streams = {'sig', 'baq', 'baq_scaled','sigsub', 'sigfilt'}; % Frequency scaled background for eachfile = 1:length(data) for eachstream = 1:length(streams) streamname = char(streams(eachstream)); [streamFFT,streamF] = preparestreamFFT(data(eachfile).(streamname),data(eachfile).fs); data(eachfile).(append(streamname,'FFT')) = streamFFT; data(eachfile).(append(streamname,'F')) = streamF; end end","title":"preparestreamFFT"},{"location":"functiondocumentation/#transient-analysis-functions","text":"","title":"Transient Analysis Functions"},{"location":"functiondocumentation/#findtransients","text":"Main function to detect and quantify transient events in whole session data streams. Users must input the raw data structure containing the data stream, a field with threshold values for transient inclusion, and the sampling rate of the stream. findTransients will output a new data structure ( transientdata ) with any user specified fields from the original data structure, and the field 'transientquantification' containing quantification variables for each detected transient event. The findTransients function will set default values for transient detection and quantification parameters not specific as optional inputs. Optional inputs can be specified to modify transient detection parameters. See the user guide section Transient Analysis for more details. INPUTS: DATA: Data structure array; each element (row) represents a single recording session and must contain the fields specified by ADDVARIABLESFIELDNAMES, STREAMFIELDNAME, THRESHOLDFIELDNAME, and FSFIELDNAME. ADDVARIABLESFIELDNAMES: Cell array; names of the fields in DATA to add to the new data structure. This should include SubjectID and experimentally relevant metadata. For example: {'SubjectID', 'BlockFolder', 'Dose'} WHICHSTREAM: The name (string) of the field containing the stream to be analyzed for transients. WHICHBLTYPE: A string with the type of pre-transient baseline to use for amplitude inclusion and quantification. 'blmean': Pre-transient baselines are set to the mean of the pre-transient window. 'blmin': Pre-transient baselines are set to the minimum value within the pre-transient window. 'localmin': Pre-transient baselines are set to the local minimum directly preceding the transient within the baseline window. WHICHSTREAM: The name (string) of the field containing the stream to be analyzed for transients. WHICHTHRESHOLD: The name (string) of the field containing the prepared numeric threshold values for each stream. For example, 'threshold_3SD'. WHICHFS: The name (string) of the field containing the sampling rate of the raw data collection in Hz. OPTIONAL INPUTS: PREMINSTARTMS: Number of millseconds pre-transient to use as the start of the baseline window. Default: 800 PREMINENDMS: Number of millseconds pre-transient to use as the end of the baseline window. Default: 100 POSTTRANSIENTMS: Number of millseconds post-transient to use for the post peak baseline and trimmed data output. Default: 2000 QUANTIFICATIONHEIGHT: The height at which to characterize rise time, fall time, peak width, and AUC. Must be a number between 0 and 1. Default: 0.5 OUTPUTTRANSIENTDATA: Set to 1 to output cut data streams for each transient event for further analysis or plotting. Set to 0 to skip. Default: 1 OUTPUTS: DATA: The original data structure with sessiontransients_WHICHBLTYPE_THRESHOLDLABEL added in. For more details on individual variables, see the PASTa user guide page Transient Detection . The output contains four nested tables: INPUTS: Includes all required and optional inputs. If optional inputs are not specified, defaults will be applied. TRANSIENTQUANTIFICATION: Includes the quantified variables for each transient, including amplitude, rise time, fall time, width, and AUC. TRANSIENTSTREAMLOCS: Pre-transient baseline, transient peak, rise, and fall locations for each transient to match the cut transient stream data. _TRANSIENTSTREAMDATA: Cut data stream from baseline start to the end of the post-transient period for each transient event. NOTES: Threshold values should be calculated before using the findSessionTransients functions. Typically thresholds are set to 2-3 SDs. If the input data stream is Z scored, this can be the actual SD threshold number. If the input data stream is not Z scored, find the corresponding value to 2-3 SDs for each subject. For transient data outputs, each transient is in a separate row. If OUTPUTTRANSIENTDATA is set to anything other than 1, the TRANSIENTSTREAMLOCS and TRANSIENTSTREAMDATA tables will be skipped and not included in the output. EXAMPLE: % Prepare thresholds - since Z scored streams will be analyzed, input threshold as the desired SD. for eachfile = 1:length(data) data(eachfile).threshold3SD = 3; end % Find session transients based on pre-peak baseline window minimum [data] = findSessionTransients(data,'blmin','sigfiltz_normsession_trimmed','threshold3SD','fs'); % Find session transients based on pre-peak baseline window mean [data] = findSessionTransients(data,'blmean','sigfiltz_normsession_trimmed','threshold3SD','fs','preminstartms',600); % Find session transients based on pre-peak local minimum (last minumum before the peak in the baseline window) [data] = findSessionTransients(data,'localmin','sigfiltz_normsession_trimmed','threshold3SD','fs','quantificationheight',0.25);","title":"findTransients"},{"location":"functiondocumentation/#findsessiontransients_blmean","text":"Sub function to detect and quantify transient events in whole session data streams. Transient baselines are defined as the mean of the pre-transient baseline window. This function is called by the main function findSessionTransients , which sets default parameters for transient detection and quantification. If called outside the main function, users must specify all input values manually. INPUTS: DATA: This is a structure that contains at least the data stream you want to analyze, a field with the threshold values, and a field with the sampling rate of the data stream. WHICHSTREAM: The name (string) of the field containing the stream to be analyzed for transients. WHICHTHRESHOLD: The name (string) of the field containing the prepared numeric threshold values for each stream. For example, 'threshold_3SD'. WHICHFS: The name (string) of the field containing the sampling rate of the raw data collection in Hz. PREMINSTARTMS: Number of millseconds pre-transient to use as the start of the baseline window. Reccomended value: 800 PREMINENDMS: Number of millseconds pre-transient to use as the end of the baseline window. Reccomended value: 100 POSTTRANSIENTMS: Number of millseconds post-transient to use for the post peak baseline and trimmed data output. Reccomended value: 2000 QUANTIFICATIONHEIGHT: The height at which to characterize rise time, fall time, peak width, and AUC. Must be a number between 0 and 1. Reccomended value: 0.5 OUTPUTTRANSIENTDATA: Set to 1 to output cut data streams for each transient event for further analysis or plotting. Set to 0 to skip. Reccomended value: 1 OUTPUTS: DATA: The original data structure with sessiontransients_blmean_THRESHOLDLABEL added in. For more details on individual variables, see the PASTa user guide page Transient Detection . The output contains four nested tables: INPUTS: Includes all function inputs. TRANSIENTQUANTIFICATION: Includes the quantified variables for each transient, including amplitude, rise time, fall time, width, and AUC. TRANSIENTSTREAMLOCS: Pre-transient baseline start, pre-transient baseline end, transient peak, rise, and fall locations for each transient to match the cut transient stream data. _TRANSIENTSTREAMDATA: Cut data stream from baseline window start to the end of the post-transient period for each transient event. NOTES: If called outside the main findSessionTransients function, note that users will have to input all parameters manually and no defaults will be applied. As for the main function, threshold values should be calculated before using the findSessionTransients functions. Typically thresholds are set to 2-3 SDs. If the input data stream is Z scored, this can be the actual SD threshold number. If the input data stream is not Z scored, find the corresponding value to 2-3 SDs for each subject. For transient data outputs, each transient is in a separate row. If OUTPUTTRANSIENTDATA is set to anything other than 1, the TRANSIENTSTREAMLOCS and TRANSIENTSTREAMDATA tables will be skipped and not included in the output. EXAMPLE: % Prepare thresholds - since Z scored streams will be analyzed, input threshold as the desired SD. for eachfile = 1:length(data) data(eachfile).threshold3SD = 3; end % Find session transients by directly called in the findSessionTransients_blmean function [data] = findSessionTransients_blmean(data,'sigfiltz_normsession_trimmed','threshold3SD','fs',800,100,2000,0.5,1);","title":"findSessionTransients_blmean"},{"location":"functiondocumentation/#findsessiontransients_blmin","text":"Sub function to detect and quantify transient events in whole session data streams. Transient baselines are defined as the minimum value within the pre-transient baseline window. This function is called by the main function findSessionTransients , which sets default parameters for transient detection and quantification. If called outside the main function, users must specify all input values manually. INPUTS: DATA: This is a structure that contains at least the data stream you want to analyze, a field with the threshold values, and a field with the sampling rate of the data stream. WHICHSTREAM: The name (string) of the field containing the stream to be analyzed for transients. WHICHTHRESHOLD: The name (string) of the field containing the prepared numeric threshold values for each stream. For example, 'threshold_3SD'. WHICHFS: The name (string) of the field containing the sampling rate of the raw data collection in Hz. PREMINSTARTMS: Number of millseconds pre-transient to use as the start of the baseline window. Reccomended value: 800 PREMINENDMS: Number of millseconds pre-transient to use as the end of the baseline window. Reccomended value: 100 POSTTRANSIENTMS: Number of millseconds post-transient to use for the post peak baseline and trimmed data output. Reccomended value: 2000 QUANTIFICATIONHEIGHT: The height at which to characterize rise time, fall time, peak width, and AUC. Must be a number between 0 and 1. Reccomended value: 0.5 OUTPUTTRANSIENTDATA: Set to 1 to output cut data streams for each transient event for further analysis or plotting. Set to 0 to skip. Reccomended value: 1 OUTPUTS: DATA: The original data structure with sessiontransients_blmin_THRESHOLDLABEL added in. For more details on individual variables, see the PASTa user guide page Transient Detection . The output contains four nested tables: INPUTS: Includes all function inputs. TRANSIENTQUANTIFICATION: Includes the quantified variables for each transient, including amplitude, rise time, fall time, width, and AUC. TRANSIENTSTREAMLOCS: Pre-transient baseline, transient peak, rise, and fall locations for each transient to match the cut transient stream data. TRANSIENTSTREAMDATA: Cut data stream from baseline window start to the end of the post-transient period for each transient event. NOTES: If called outside the main findSessionTransients function, note that users will have to input all parameters manually and no defaults will be applied. As for the main function, threshold values should be calculated before using the findSessionTransients functions. Typically thresholds are set to 2-3 SDs. If the input data stream is Z scored, this can be the actual SD threshold number. If the input data stream is not Z scored, find the corresponding value to 2-3 SDs for each subject. For transient data outputs, each transient is in a separate row. If OUTPUTTRANSIENTDATA is set to anything other than 1, the TRANSIENTSTREAMLOCS and TRANSIENTSTREAMDATA tables will be skipped and not included in the output. EXAMPLE: % Prepare thresholds - since Z scored streams will be analyzed, input threshold as the desired SD. for eachfile = 1:length(data) data(eachfile).threshold3SD = 3; end % Find session transients by directly called in the findSessionTransients_blmin function [data] = findSessionTransients_blmin(data,'sigfiltz_normsession_trimmed','threshold3SD','fs',800,100,2000,0.5,1);","title":"findSessionTransients_blmin"},{"location":"functiondocumentation/#findsessiontransients_localmin","text":"Sub function to detect and quantify transient events in whole session data streams. Transient baselines are defined as the last local minimum value before the transient peak within the pre-transient baseline window. This function is called by the main function findSessionTransients , which sets default parameters for transient detection and quantification. If called outside the main function, users must specify all input values manually. INPUTS: DATA: This is a structure that contains at least the data stream you want to analyze, a field with the threshold values, and a field with the sampling rate of the data stream. WHICHSTREAM: The name (string) of the field containing the stream to be analyzed for transients. WHICHTHRESHOLD: The name (string) of the field containing the prepared numeric threshold values for each stream. For example, 'threshold_3SD'. WHICHFS: The name (string) of the field containing the sampling rate of the raw data collection in Hz. PREMINSTARTMS: Number of millseconds pre-transient to use as the start of the baseline window. Reccomended value: 800 PREMINENDMS: Number of millseconds pre-transient to use as the end of the baseline window. Reccomended value: 100 POSTTRANSIENTMS: Number of millseconds post-transient to use for the post peak baseline and trimmed data output. Reccomended value: 2000 QUANTIFICATIONHEIGHT: The height at which to characterize rise time, fall time, peak width, and AUC. Must be a number between 0 and 1. Reccomended value: 0.5 OUTPUTTRANSIENTDATA: Set to 1 to output cut data streams for each transient event for further analysis or plotting. Set to 0 to skip. Reccomended value: 1 OUTPUTS: DATA: The original data structure with sessiontransients_localmin_THRESHOLDLABEL added in. For more details on individual variables, see the PASTa user guide page Transient Detection . The output contains four nested tables: INPUTS: Includes all function inputs. TRANSIENTQUANTIFICATION: Includes the quantified variables for each transient, including amplitude, rise time, fall time, width, and AUC. TRANSIENTSTREAMLOCS: Pre-transient baseline (local minimum), transient peak, rise, and fall locations for each transient to match the cut transient stream data. TRANSIENTSTREAMDATA: Cut data stream from baseline window start to the end of the post-transient period for each transient event. NOTES: If called outside the main findSessionTransients function, note that users will have to input all parameters manually and no defaults will be applied. As for the main function, threshold values should be calculated before using the findSessionTransients functions. Typically thresholds are set to 2-3 SDs. If the input data stream is Z scored, this can be the actual SD threshold number. If the input data stream is not Z scored, find the corresponding value to 2-3 SDs for each subject. For transient data outputs, each transient is in a separate row. If OUTPUTTRANSIENTDATA is set to anything other than 1, the TRANSIENTSTREAMLOCS and TRANSIENTSTREAMDATA tables will be skipped and not included in the output. EXAMPLE: % Prepare thresholds - since Z scored streams will be analyzed, input threshold as the desired SD. for eachfile = 1:length(data) data(eachfile).threshold3SD = 3; end % Find session transients by directly called in the findSessionTransients_localmin function [data] = findSessionTransients_localmin(data,'sigfiltz_normsession_trimmed','threshold3SD','fs',800,100,2000,0.5,1);","title":"findSessionTransients_localmin"},{"location":"functiondocumentation/#binsessiontransients","text":"Used to assign individual transient events to time bins within the session for analysis of changes in transients over time. By default, sessions will be divided into 5 minute bins. Number of bins per session will be calculated by the function. Users have the option to override the defaults to change the bin length and/or manually set the number of bins per session. INPUTS: DATA: This is a structure that contains at least the data stream you want to analyze, a field with the threshold values, and a field with the sampling rate of the data stream. WHICHSTREAM: The name (string) of the field containing the stream to be analyzed for transients. WHICHFS: The name (string) of the field containing the sampling rate of the raw data collection in Hz. WHICHTRANSIENTS: The name (string) of the parent field containing the table of transietns that you want to identify bins for. This is the output of the findSessionTransients function. The name usually follows the convention sessiontransients_WHICHBLTYPE_WHICHTHRESHOLD OPTIONAL INPUTS: * WHICHTRANSIENTSTABLE: The name (string) of the field within WHICHTRANSIENTS that contains the quantification of individual transient events. This input only needs to be specified if not using the format output from the FINDSESSIONTRANSIENTS functions. Default: 'transientquantification' * WHICHMAXLOCS: NThe name (string) of the field containing the transient max locations (indexes) relative to the whole session. This input only needs to be specified if not using the format output from the FINDSESSIONTRANSIENTS functions. Default: 'maxloc' * BINLENGTHMINS: Numeric; Bin length in number of minutes. Default: 5 * NBINSOVERRIDE: Numeric; Manual override to set the number of bins. If set to anything other than 0, users can override the stream-length based calculation of the number of bins per session and set their own number. Default: 0 OUTPUTS: DATA: The original data structure with sessiontransients_localmin_THRESHOLDLABEL added in. For more details on individual variables, see the PASTa user guide page Transient Detection . The output contains four nested tables: INPUTS: Includes all function inputs. TRANSIENTQUANTIFICATION: Includes the quantified variables for each transient, including amplitude, rise time, fall time, width, and AUC. TRANSIENTSTREAMLOCS: Pre-transient baseline (local minimum), transient peak, rise, and fall locations for each transient to match the cut transient stream data. TRANSIENTSTREAMDATA: Cut data stream from baseline window start to the end of the post-transient period for each transient event. NOTES: If called outside the main findSessionTransients function, note that users will have to input all parameters manually and no defaults will be applied. As for the main function, threshold values should be calculated before using the findSessionTransients functions. Typically thresholds are set to 2-3 SDs. If the input data stream is Z scored, this can be the actual SD threshold number. If the input data stream is not Z scored, find the corresponding value to 2-3 SDs for each subject. For transient data outputs, each transient is in a separate row. If OUTPUTTRANSIENTDATA is set to anything other than 1, the TRANSIENTSTREAMLOCS and TRANSIENTSTREAMDATA tables will be skipped and not included in the output. EXAMPLE: % Prepare thresholds - since Z scored streams will be analyzed, input threshold as the desired SD. for eachfile = 1:length(data) data(eachfile).threshold3SD = 3; end % Find session transients by directly called in the findSessionTransients_localmin function [data] = findSessionTransients_localmin(data,'sigfiltz_normsession_trimmed','threshold3SD','fs',800,100,2000,0.5,1);","title":"binSessionTransients"},{"location":"functiondocumentation/#exportsessiontransients","text":"Used to create a table of all transient events across all sessions included in the file key and data structure, and exports the table of transient quantification to a csv file. This function depends on the output from findSessionTransients . The creation of a csv file is helpful for users who prefer to run statistical analyses or make plots in other programs than MATLAB, such as R Studio. INPUTS: DATA: This is a structure that contains at least the output from findSessionTransients . WHICHTRANSIENTS: The name (string) of the parent field containing the table of transietns that you want to identify bins for. This is the output of the findSessionTransients function. The name usually follows the convention sessiontransients_WHICHBLTYPE_WHICHTHRESHOLD EXPORTFILEPATH: A string with the path to the folder location where the created table should be saved to. NOTE: The path must end in a forward slash. ADDVARIABLES: A cell array containing any additional variables from the data structure to be added to the transients table. Variables will be added to every row of the output structure. Cell array inputs must be the names of fields in the data structure. At a minimum, this should contain the subject ID. If multiple sessions per subject are included in the data structure, make sure a session ID variable is also included. For example: {'Subject', 'SessionID', 'Treatment'} OPTIONAL INPUTS: * WHICHTRANSIENTSTABLE: The name (string) of the field within WHICHTRANSIENTS that contains the quantification of individual transient events. This input only needs to be specified if not using the format output from the FINDSESSIONTRANSIENTS functions. Default: 'transientquantification' * FILENAME: A string with a custom name for the output csv file. If not specified, the file name will be generated as 'WHICHTRANSIENTS_AllSessionExport_DAY-MONTH-YEAR.csv' OUTPUTS: This function outputs a csv file with all transients for all sessions in the data structure. The file will be output at the specified file path. If the function is called into an object, the table ALLTRANSIENTS will also be saved to an object in the MATLAB workspace. NOTES: The file path must include the full path for the folder including the computer address, and must end in a forward slash. If the file path is not specified properly, the function won't be able to automatically save the csv file. Make sure to add all relevant experimental variables to the ADDVARIABLES list. This includes at a minimum the Subject ID and a session ID, but it's wise to include other key variables to make future analyses easier. Alternatively, you can keep only the Subject ID and a session ID and match the data to the subject and file keys later, which would include all subject and experimental variables. EXAMPLE: % Export transients with added fields for subject and treatment= addvariables = {'Subject','TreatNum','InjType'}; % Prepare variables to append to each transient % To just output the csv file: exportSessionTransients(data,'sessiontransients_blmean_threshold3SD',analysispath,addvariables); % To output the csv file and add the created table to a MATLAB data structure: alltransients = exportSessionTransients(data,'sessiontransients_blmean_threshold3SD',analysispath,addvariables);","title":"exportSessionTransients"},{"location":"functiondocumentation/#visualization-and-plotting-functions","text":"","title":"Visualization and Plotting Functions"},{"location":"functiondocumentation/#plottraces","text":"Main function to plot whole session fiber photometry traces. This function will plot the streams sig, baq, baq_scaled, sigsub, and sigfilt for a single session. Use this function in a loop to plot streams for all sessions in the data structure. This plot function plots the data streams with time in minutes on the x axis. To modify the plot or add additional features like session relevant time stamps, use the plot in a loop. Modified plots must be saved in the external loop. If modifications are not neccesaary, then optional inputs can be specified to directly output and save the plot to a filepath location. INPUTS: DATA: This is a structure that contains at least the data stream you want to analyze, a field with the threshold values, and a field with the sampling rate of the data stream. WHICHFILE: The file number to be plotted. MAINTITLE: The main title (string) to be displayed for the overall plot above the individual tiles. For example, '427 - Treatment: Morphine' OPTIONAL INPUTS: SAVEOUTPUT: Set to 1 to automatically save trace plots as png to the plot file path. Default: 0 PLOTFILEPATH: Required if SAVEOUTPUT is set to 1. The specific path to save the plot to. Note that this must be the entire path from computer address to folder ending in the filename for the specific plot. For example: 'C:\\Users\\rmdon\\Box\\Injection Transients\\Figures\\SessionTraces_427_Morphine.png' OUTPUTS: ALLTRACES: A plot object containing subplots for each input stream. EXAMPLE: Single Session %% Plot the first session and automatically save the output maintitle = append(num2str(data(1).Subject),' - Treatment: ',data(1).InjType); % Create title string for current plot plotfilepath = 'C:\\Users\\rmdon\\Box\\Injection Transients\\Figures\\SessionTraces_427_Morphine.png'; % Create plot file path for the current plot plotTraces(data,1,maintitle,'saveoutput',1,'plotfilepath',plotfilepath); EXAMPLE: All Sessions %% Plot whole session streams for each file for eachfile = 1:length(data) maintitle = append(num2str(data(eachfile).Subject),' - Treatment: ',data(eachfile).InjType); % Create title string for current plot alltraces = plotTraces(data,eachfile,maintitle); for eachtile = 1:5 nexttile(eachtile) xline(data(eachfile).injt(1),'--','Injection','Color','#C40300','FontSize',8) xline(data(eachfile).injt(2),'--','Color','#C40300','FontSize',8) end set(gcf, 'Units', 'inches', 'Position', [0, 0, 8, 9]); plotfilepath = append(figurepath,'SessionTraces_',num2str(data(eachfile).Subject),'_',data(eachfile).InjType,'.png'); exportgraphics(gcf,plotfilepath,'Resolution',300) end","title":"plotTraces"},{"location":"functiondocumentation/#plotffts","text":"Main function to plot whole session fiber photometry FFT frequency magnitude plots. This function will plot the streams sig, baq, baq_scaled, sigsub, and sigfilt for a single session. Use this function in a loop to plot streams for all sessions in the data structure. By default, only frequencies up to 20 hz will be plotted but users can specify an optional input to adjust the cutoff lower or higher, or set to the actual max for the stream FFT. To modify the plot or add additional features, use the plot in a loop. Modified plots must be saved in the external loop. If modifications are not neccesaary, then optional inputs can be specified to directly output and save the plot to a filepath location. INPUTS: DATA: This is a structure that contains at least the data stream you want to analyze, a field with the threshold values, and a field with the sampling rate of the data stream. WHICHFILE: The file number to be plotted. MAINTITLE: The main title (string) to be displayed for the overall plot above the individual tiles. For example, '427 - Treatment: Morphine' WHICHFS: The name (string) of the field that contains the sampling rate of the data streams. OPTIONAL INPUTS: XMAX: The desired frequency cutoff for the x axis (hz). Frequencies above this value will be excluded from the plots. To plot all frequencies, set to actual'. Default: 20 SAVEOUTPUT: Set to 1 to automatically save trace plots as png to the plot file path. Default: 0 PLOTFILEPATH: Required if SAVEOUTPUT is set to 1. The specific path to save the plot to. Note that this must be the entire path from computer address to folder ending in the filename for the specific plot. For example: 'C:\\Users\\rmdon\\Box\\Injection Transients\\Figures\\SessionTraces_427_Morphine.png' OUTPUTS: ALLFFTS: A plot object containing subplots for each input stream. EXAMPLE: Single Session %% Plot the first session and automatically save the output maintitle = append(num2str(data(1).Subject),' - Treatment: ',data(1).InjType); % Create title string for current plot plotfilepath = 'C:\\Users\\rmdon\\Box\\Injection Transients\\Figures\\SessionTraces_427_Morphine.png'; % Create plot file path for the current plot plotTraces(data,1,maintitle,'fs','saveoutput',1,'plotfilepath',plotfilepath); EXAMPLE: All Sessions %% Plot stream FFTs for each file for eachfile = 1:length(data) maintitle = append(num2str(data(eachfile).Subject),' - Treatment: ',data(eachfile).InjType); % Create title string for current plot allffts = plotFFTs(data,eachfile,maintitle); set(gcf, 'Units', 'inches', 'Position', [0, 0, 8, 9]); plotfilepath = append(figurepath,'SessionFFTs_',num2str(data(eachfile).Subject),'_',data(eachfile).InjType,'.png'); exportgraphics(gcf,plotfilepath,'Resolution',300) end","title":"plotFFTs"},{"location":"gettingstarted/","text":"Getting Started with PASTa Here you'll find the quick start guide to set up PASTa with instructions on installation and basic use of the toolbox. Once your device is set up, move to the User Guide for detailed documentation on the use of the PASTa Protocol for analysis. Set Up and Installation PASTa is an open-source MATLAB-based toolbox, containing a series of functions to extract, process, and analyze photometry data. Users must first install MATLAB (version 2020a or later) and add the Signal Processing Toolbox add-on. Following MATLAB installation, users can install PASTa three ways: 1) MATLAB Add-On Explorer, 2) downloading the PASTa MATLAB toolbox installation file from the PASTa GitHub, or 3) local installation of the PASTa GitHub repository. Install MATLAB First, users must ensure that MATLAB is installed on the local computer used for analysis. The Signal Processing Toolbox is a required dependency of PASTa. To install MATLAB, visit the MathWorks site . Multiple license types are available, including Academic ($275/year) and student ($99/year). For more info, see MATLAB Pricing . Licensing may also be available through your university. After installing MATLAB, ensure the Signal Processing Toolbox is installed as an Add-On. To install the Add-On: At the top of the MATLAB window, select the Home tab Under Environment , select Add-Ons . Click Get Add-Ons In the Search bar, search for \"Signal Processing Toolbox\" Install the Signal Processing Toolbox Add-On Install PASTa Option 1: MATLAB Add-On Explorer PASTa installation as a MATLAB toolbox through the MATLAB Add-On Explorer is the recommended method to install the toolbox to ensure MATLAB is set up to properly access the functions. This guarantees version stability and accessible for newer users of MATLAB as functions will be installed and accessed similarly to built-in MATLAB functions. To install PASTa through the MATLAB Add-On Explorer: At the top of the MATLAB window, select the Home tab Under Environment , select Add-Ons . Click Get Add-Ons In the Search bar, search for \"PASTa\" Install the PASTa toolbox Add-On Option 2: MATLAB Toolbox Installation File on GitHub Installation via the MATLAB toolbox file in the PASTa GitHub is another option and may be necessary for MATLAB users with older versions that are not compatible with the Add-On explorer or are not tied to a current MATLAB license subscription. To install PASTa through the MATLAB Toolbox file on GitHub: Navigate to the PASTa GitHub repository: https://github.com/rdonka/PASTa Download the file \"PASTa.mltbx\" Open the file \"PASTa.mltbx\" . The toolbox will be automatically installed to MATLAB. Option 3: Clone the PASTa GitHub Repository To access PASTa, users can also copy the toolbox functions to a local folder of the user's choice and then add that folder to the MATLAB path. If desired, users can create a local copy of PASTa by cloning the repository via GitHub desktop. The use of GitHub desktop allows users to easily fetch updates to the repository. Installation of PASTa through cloning of the GitHub repository is more dynamic and may facilitate integration of version updates as they are released. New versions of PASTa will continue to include integration of newly developed signal processing methods, bug fixes, and analysis features as they are released. To download and use PASTa directly via the GitHub repository: Download GitHub and GitHub Desktop . For detailed instructions on GitHub setup, see https://docs.github.com/en/desktop/installing-and-authenticating-to-github-desktop/setting-up-github-desktop Clone the PASTa repository. To make it easy to locate repositories, we recommend making a folder on your desktop called \"GitHubRepositories\" and cloning all repositories there. For detailed instructions on cloning repositories, see https://docs.github.com/en/repositories/creating-and-managing-repositories/cloning-a-repository Add the path to the cloned PASTa repository to MATLAB at the start of each analysis session. For instructions to add folders to the search path, see https://www.mathworks.com/help/matlab/ref/addpath.html Note: The file main_ExampleAnalysis_Transients.m in the Example Analyses folder serves as a template for the main script to run functions described in the protocol. An example of adding the cloned PASTa repository folder to path is included at the beginning of the script. For an overview of how to use GitHub and best practices: https://www.freecodecamp.org/news/introduction-to-git-and-github/ Get Started with an Example Analysis To help users get started with PASTa, the repository includes a full example analysis. The example includes all stages of the pipeline from data preparation through signal processing and transient event detection and analysis. For full details, see the Example Analyses page. Learning Resources For users unfamiliar with MATLAB, we have compiled some helpful introductory resources on the Learning Resources page. Time Considerations The initial installation of PASTa is easy and quick when done through the MATLAB Add-On Explorer, or via the packaged toolbox file included in the PASTa GitHub repository. This is recommended for users less familiar with MATLAB or GitHub. For users who are comfortable with GitHub and GitHub Desktop, cloning the repository is straightforward. For newer users seeking to access the toolbox by cloning the GitHub repository, expect a few hours to get acquainted with both GitHub and the addition of the toolbox to MATLAB. Data preparation time for initial users of PASTa will vary depending on existing data organization practices. While we recommend a specific data organization structure for initial users of PASTa (Basic Protocol 2), PASTa functions are written to be flexible to alternative data organization practices. Allot 2 hours for the data organization process. Once data organization is complete, the remaining protocols should take limited time to apply if using PASTa default options. A full example of an analysis from data organization to signal processing and transient detection is included in the repository. Adapting the example analysis to the user\u2019s experiment will typically take approximately 1-2 hours. Allot additional time if the user is new to MATLAB, or if experimental design requires more custom modifications. Together, we anticipate that a new user of PASTa should be able to complete all protocols within approximately 4-5 hours. After initial set up, running the full pipeline including data extraction, signal processing, transient detection and analysis, and plot generation typically takes approximately 1 minute for each fiber photometry session. Contact Us Please feel free to reach out with questions, feedback, and feature requests. PASTa is actively maintained, and will continue to integrate new features into future versions. Version releases will be continually added to GitHub, and details are also available on the Version Release Notes page. To facilitate community discussion, we are actively maintaining the Discussions Forum via GitHub, which includes channels for announcements, feature requests, troubleshooting, and general discussions. Our individual contact information is also available here .","title":"Getting Started"},{"location":"gettingstarted/#getting-started-with-pasta","text":"Here you'll find the quick start guide to set up PASTa with instructions on installation and basic use of the toolbox. Once your device is set up, move to the User Guide for detailed documentation on the use of the PASTa Protocol for analysis.","title":"Getting Started with PASTa"},{"location":"gettingstarted/#set-up-and-installation","text":"PASTa is an open-source MATLAB-based toolbox, containing a series of functions to extract, process, and analyze photometry data. Users must first install MATLAB (version 2020a or later) and add the Signal Processing Toolbox add-on. Following MATLAB installation, users can install PASTa three ways: 1) MATLAB Add-On Explorer, 2) downloading the PASTa MATLAB toolbox installation file from the PASTa GitHub, or 3) local installation of the PASTa GitHub repository.","title":"Set Up and Installation"},{"location":"gettingstarted/#install-matlab","text":"First, users must ensure that MATLAB is installed on the local computer used for analysis. The Signal Processing Toolbox is a required dependency of PASTa. To install MATLAB, visit the MathWorks site . Multiple license types are available, including Academic ($275/year) and student ($99/year). For more info, see MATLAB Pricing . Licensing may also be available through your university. After installing MATLAB, ensure the Signal Processing Toolbox is installed as an Add-On. To install the Add-On: At the top of the MATLAB window, select the Home tab Under Environment , select Add-Ons . Click Get Add-Ons In the Search bar, search for \"Signal Processing Toolbox\" Install the Signal Processing Toolbox Add-On","title":"Install MATLAB"},{"location":"gettingstarted/#install-pasta","text":"","title":"Install PASTa"},{"location":"gettingstarted/#option-1-matlab-add-on-explorer","text":"PASTa installation as a MATLAB toolbox through the MATLAB Add-On Explorer is the recommended method to install the toolbox to ensure MATLAB is set up to properly access the functions. This guarantees version stability and accessible for newer users of MATLAB as functions will be installed and accessed similarly to built-in MATLAB functions. To install PASTa through the MATLAB Add-On Explorer: At the top of the MATLAB window, select the Home tab Under Environment , select Add-Ons . Click Get Add-Ons In the Search bar, search for \"PASTa\" Install the PASTa toolbox Add-On","title":"Option 1: MATLAB Add-On Explorer"},{"location":"gettingstarted/#option-2-matlab-toolbox-installation-file-on-github","text":"Installation via the MATLAB toolbox file in the PASTa GitHub is another option and may be necessary for MATLAB users with older versions that are not compatible with the Add-On explorer or are not tied to a current MATLAB license subscription. To install PASTa through the MATLAB Toolbox file on GitHub: Navigate to the PASTa GitHub repository: https://github.com/rdonka/PASTa Download the file \"PASTa.mltbx\" Open the file \"PASTa.mltbx\" . The toolbox will be automatically installed to MATLAB.","title":"Option 2: MATLAB Toolbox Installation File on GitHub"},{"location":"gettingstarted/#option-3-clone-the-pasta-github-repository","text":"To access PASTa, users can also copy the toolbox functions to a local folder of the user's choice and then add that folder to the MATLAB path. If desired, users can create a local copy of PASTa by cloning the repository via GitHub desktop. The use of GitHub desktop allows users to easily fetch updates to the repository. Installation of PASTa through cloning of the GitHub repository is more dynamic and may facilitate integration of version updates as they are released. New versions of PASTa will continue to include integration of newly developed signal processing methods, bug fixes, and analysis features as they are released. To download and use PASTa directly via the GitHub repository: Download GitHub and GitHub Desktop . For detailed instructions on GitHub setup, see https://docs.github.com/en/desktop/installing-and-authenticating-to-github-desktop/setting-up-github-desktop Clone the PASTa repository. To make it easy to locate repositories, we recommend making a folder on your desktop called \"GitHubRepositories\" and cloning all repositories there. For detailed instructions on cloning repositories, see https://docs.github.com/en/repositories/creating-and-managing-repositories/cloning-a-repository Add the path to the cloned PASTa repository to MATLAB at the start of each analysis session. For instructions to add folders to the search path, see https://www.mathworks.com/help/matlab/ref/addpath.html Note: The file main_ExampleAnalysis_Transients.m in the Example Analyses folder serves as a template for the main script to run functions described in the protocol. An example of adding the cloned PASTa repository folder to path is included at the beginning of the script. For an overview of how to use GitHub and best practices: https://www.freecodecamp.org/news/introduction-to-git-and-github/","title":"Option 3: Clone the PASTa GitHub Repository"},{"location":"gettingstarted/#get-started-with-an-example-analysis","text":"To help users get started with PASTa, the repository includes a full example analysis. The example includes all stages of the pipeline from data preparation through signal processing and transient event detection and analysis. For full details, see the Example Analyses page.","title":"Get Started with an Example Analysis"},{"location":"gettingstarted/#learning-resources","text":"For users unfamiliar with MATLAB, we have compiled some helpful introductory resources on the Learning Resources page.","title":"Learning Resources"},{"location":"gettingstarted/#time-considerations","text":"The initial installation of PASTa is easy and quick when done through the MATLAB Add-On Explorer, or via the packaged toolbox file included in the PASTa GitHub repository. This is recommended for users less familiar with MATLAB or GitHub. For users who are comfortable with GitHub and GitHub Desktop, cloning the repository is straightforward. For newer users seeking to access the toolbox by cloning the GitHub repository, expect a few hours to get acquainted with both GitHub and the addition of the toolbox to MATLAB. Data preparation time for initial users of PASTa will vary depending on existing data organization practices. While we recommend a specific data organization structure for initial users of PASTa (Basic Protocol 2), PASTa functions are written to be flexible to alternative data organization practices. Allot 2 hours for the data organization process. Once data organization is complete, the remaining protocols should take limited time to apply if using PASTa default options. A full example of an analysis from data organization to signal processing and transient detection is included in the repository. Adapting the example analysis to the user\u2019s experiment will typically take approximately 1-2 hours. Allot additional time if the user is new to MATLAB, or if experimental design requires more custom modifications. Together, we anticipate that a new user of PASTa should be able to complete all protocols within approximately 4-5 hours. After initial set up, running the full pipeline including data extraction, signal processing, transient detection and analysis, and plot generation typically takes approximately 1 minute for each fiber photometry session.","title":"Time Considerations"},{"location":"gettingstarted/#contact-us","text":"Please feel free to reach out with questions, feedback, and feature requests. PASTa is actively maintained, and will continue to integrate new features into future versions. Version releases will be continually added to GitHub, and details are also available on the Version Release Notes page. To facilitate community discussion, we are actively maintaining the Discussions Forum via GitHub, which includes channels for announcements, feature requests, troubleshooting, and general discussions. Our individual contact information is also available here .","title":"Contact Us"},{"location":"learningresources/","text":"Learning Resources This page is a compilation of additional resources that may be useful to users that are in the earlier phases of learning to interact with code. While PASTa is written to be approachable and user friendly, learning to interact with coding environments and write new scripts can be challenging. If you have other resources to reccomend, please let us know and we will update this page. Git, GitHub, and GitHub Desktop Getting Started with Git and GitHub Desktop , Article by Codeacademy: https://www.codecademy.com/article/what-is-git-and-github-desktop An Intro to Git and GitHub for Beginners (Tutorial) , Article by HubSpot: https://product.hubspot.com/blog/git-and-github-tutorial-for-beginners Git, GitHub, & GitHub Desktop for beginners , Video by Coder Coder: https://www.youtube.com/watch?v=8Dd7KRpKeaE","title":"Resources"},{"location":"learningresources/#learning-resources","text":"This page is a compilation of additional resources that may be useful to users that are in the earlier phases of learning to interact with code. While PASTa is written to be approachable and user friendly, learning to interact with coding environments and write new scripts can be challenging. If you have other resources to reccomend, please let us know and we will update this page.","title":"Learning Resources"},{"location":"learningresources/#git-github-and-github-desktop","text":"Getting Started with Git and GitHub Desktop , Article by Codeacademy: https://www.codecademy.com/article/what-is-git-and-github-desktop An Intro to Git and GitHub for Beginners (Tutorial) , Article by HubSpot: https://product.hubspot.com/blog/git-and-github-tutorial-for-beginners Git, GitHub, & GitHub Desktop for beginners , Video by Coder Coder: https://www.youtube.com/watch?v=8Dd7KRpKeaE","title":"Git, GitHub, and GitHub Desktop"},{"location":"about/citepasta/","text":"How to Cite PASTa Information on citing the PASTa Protocol coming soon, thanks for checking!","title":"How to Cite PASTa"},{"location":"about/citepasta/#how-to-cite-pasta","text":"Information on citing the PASTa Protocol coming soon, thanks for checking!","title":"How to Cite PASTa"},{"location":"about/license/","text":"License Copyright (C) 2024 Rachel Donka. The PASTa Protocol software and source code is licensed under the GNU General Public License v3. This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. Disclaimer of Warranty: THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \u201cAS IS\u201d WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION. Limitation of Liability: IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. For more information about the GNU General Public License, see the full license at [ https://www.gnu.org/licenses/gpl-3.0.html ]","title":"License"},{"location":"about/license/#license","text":"Copyright (C) 2024 Rachel Donka. The PASTa Protocol software and source code is licensed under the GNU General Public License v3. This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. Disclaimer of Warranty: THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \u201cAS IS\u201d WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION. Limitation of Liability: IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. For more information about the GNU General Public License, see the full license at [ https://www.gnu.org/licenses/gpl-3.0.html ]","title":"License"},{"location":"about/pastateam/","text":"PASTa Team PASTa was developed by the Roitman laboratories at the University of Illinois Chicago. Development Team Rachel Donka , Graduate Student, J Roitman Lab Dr. Maxine Loh , Post Doctoral Fellow, M Roitman Lab Dr. Vaibhav Konanur , Former Graduate Student, M Roitman Lab Dr. Jamie Roitman , Principal Investigator Dr. Mitchell Roitman Maintenance Team Rachel Donka , Graduate Student, J Roitman Lab Dr. Maxine Loh , Post Doctoral Fellow, M Roitman Lab","title":"PASTa Team"},{"location":"about/pastateam/#pasta-team","text":"PASTa was developed by the Roitman laboratories at the University of Illinois Chicago.","title":"PASTa Team"},{"location":"about/pastateam/#development-team","text":"Rachel Donka , Graduate Student, J Roitman Lab Dr. Maxine Loh , Post Doctoral Fellow, M Roitman Lab Dr. Vaibhav Konanur , Former Graduate Student, M Roitman Lab Dr. Jamie Roitman , Principal Investigator Dr. Mitchell Roitman","title":"Development Team"},{"location":"about/pastateam/#maintenance-team","text":"Rachel Donka , Graduate Student, J Roitman Lab Dr. Maxine Loh , Post Doctoral Fellow, M Roitman Lab","title":"Maintenance Team"},{"location":"about/references/","text":"References Sherathiya, N.V., et al., GuPPy, a Python toolbox for the analysis of fiber photometry data. Scientific Reports, 2021. 11(1). Bridge, F.M., et al., FiPhA: an open-source platform for fiber photometry analysis. Neurophotonics, 2024. 11(01). Murphy, Z.K., et al., PhAT: A flexible open-source GUI-driven toolkit for photometry analysis. 2023. Akam, T. and E.M. Walton, pyPhotometry: Open source Python based hardware and software for fiber photometry data acquisition. Scientific Reports, 2019. 9(1). Conlisk, D., et al., Integrating operant behavior and fiber photometry with the open-source python library Pyfiber. Scientific Reports, 2023. 13(1). Bruno, A.C., et al., pMAT: An open-source software suite for the analysis of fiber photometry data. Pharmacology Biochemistry and Behavior, 2021. 201: p. 173093. Keevers, J.L. and P. Jean-Richard-Dit-Bressel, Obtaining artifact-corrected signals in fiber photometry via isosbestic signals, robust regression, and dF/F calculations. Neurophotonics, 2025. 12(02).","title":"References"},{"location":"about/references/#references","text":"Sherathiya, N.V., et al., GuPPy, a Python toolbox for the analysis of fiber photometry data. Scientific Reports, 2021. 11(1). Bridge, F.M., et al., FiPhA: an open-source platform for fiber photometry analysis. Neurophotonics, 2024. 11(01). Murphy, Z.K., et al., PhAT: A flexible open-source GUI-driven toolkit for photometry analysis. 2023. Akam, T. and E.M. Walton, pyPhotometry: Open source Python based hardware and software for fiber photometry data acquisition. Scientific Reports, 2019. 9(1). Conlisk, D., et al., Integrating operant behavior and fiber photometry with the open-source python library Pyfiber. Scientific Reports, 2023. 13(1). Bruno, A.C., et al., pMAT: An open-source software suite for the analysis of fiber photometry data. Pharmacology Biochemistry and Behavior, 2021. 201: p. 173093. Keevers, J.L. and P. Jean-Richard-Dit-Bressel, Obtaining artifact-corrected signals in fiber photometry via isosbestic signals, robust regression, and dF/F calculations. Neurophotonics, 2025. 12(02).","title":"References"},{"location":"about/releasenotes/","text":"Version Release Notes Version 0.0 (2024-11-30) Version detailed in pre-print Biorxiv manuscript.","title":"Version Release Notes"},{"location":"about/releasenotes/#version-release-notes","text":"","title":"Version Release Notes"},{"location":"about/releasenotes/#version-00-2024-11-30","text":"Version detailed in pre-print Biorxiv manuscript.","title":"Version 0.0 (2024-11-30)"},{"location":"userguide/datapreparation/","text":"Data Preparation The first stage in the PASTa Protocol the organization, extraction, and loading of fiber photometry data and associated metadata into MATLAB prior to signal processing and analysis. To streamline the analysis workflow, PASTa recommends a standardized data structure that facilitates compatibility and minimizes user error. Organizing data using the subject and file keys allows users to easily merge photometry data with experimental variables across multiple sessions, while reducing manual errors that can occur from repeated copying and pasting. This metadata management approach ensures consistent handling of subject identifiers and experimental conditions, which is essential for batch processing and reproducible analysis. To import fiber photometry data into MATLAB, PASTa includes built-in functions to extract data collected with TDT systems or data collected with other systems via a generic csv format option. Pre-extracting photometry signals into individual MATLAB structures greatly reduces processing time across multiple analysis sessions, as the raw data extraction from TDT data blocks or the generic CSV data block format is often the most computationally-intensive step in the workflow. Preparing and organizing data in this format ahead of analysis ensures smooth integration with PASTa\u2019s functionality and improves overall analysis workflow efficiency. However, if neither extraction option is compatible with user data, users can adapt the existing extraction functions or create their own to accommodate other file structures and import data into MATLAB. The signal processing and transient detection functions are written to be flexible to alternative data structure organizations, providing the required fields are available in the primary data structure. Data Organization To accommodate a variety of file organization structures, users first create two csv files containing the information necessary to access raw photometry data and the relevant experimental metadata for each subject and recording session. MATLAB can access raw data folders stored either locally or in a cloud-based storage app like Box or Dropbox. Prepare Folder Structure While the functions included in PASTa are written to be flexible and usable with differing data organization practices, we recommend users organize the data for each analysis in a parent folder with subfolders for Raw Data, Extracted Data, Analysis, and Figures. Create the project parent folder with the subfolders \"Raw Data\" , \"Extracted Data\" , \"Analysis\" , and \"Figures\" . The \u201cRaw Data\u201d folder should include subfolders containing the individual fiber photometry session blocks. PASTa currently includes two options for fiber photometry data: 1) Synapse (TDT) Format, and 2) CSV Format. If your data doesn't match the available load options, please feel free to reach out! Option 1: Synapse (TDT) Format Fiber photometry data collected through the software Synapse (Tucker Davis Technologies) is stored in tanks and blocks . Tanks are parent folders created by Synapse for each experiment. Blocks are individual folders for each session containing the actual output data. Stored data cannot be accessed directly via the folder, but rather must be extracted via MATLAB. By default, the tank path is: C:\\TDT\\Synapse\\Tanks . Synapse recognizes experiments and subjects as key categories of information that play a special role in managing data storage and retrieval. Thus, when running the session, it is critical to ensure the correct Experiment profile is selected, and the correct Subject identifier is input for each session. By default, Synapse names data tanks automatically based on experiment name and the start time of the first recording (e.g., {ExperimentName}-{yymmdd}-{hhmmss}). Blocks of data are named based on subject (e.g., {SubjectName}-{yymmdd}-{hhmmss}) for each recording session and the start time. Synapse will save a new Tank for every day unless you change the default setting. Click Menu at the top of the bar, then Preferences. Under the Data Saving tab, make sure \"New Tank Each Day\" is unchecked. If data are collected with Synapse (Tucker Davis Technologies), raw block folders can be placed directly in the Raw Data folder. Copy the data blocks output by Synapse for each session into Raw Data . If blocks are nested in tanks, we recommend to unnest the blocks for ease of file key creation. Option 2: CSV Format If data are collected with other systems (e.g., Neurophotometics, Doric), users must first format the data files into CSV files. For each individual recording session, create a folder. The folder name should be unique and include at a minimum the subject ID and recording date (e.g., \u201cSubject001_04032025\u201d). Add separate CSV files with the session recording parameters (\u201crecordingparams.csv\u201d), streams (\u201cstreams.csv\u201d), and optionally any desired event epochs (\u201cepocs.csv\u201d). REQUIRED FILES: recordingparams.csv: A csv file containing fiber photometry recording parameters. Each parameter should be a separate column, with the name of the parameter in the first row and the value in the second row. At a minimum, this must include \u2018fs\u2019, containing the sampling rate the data streams were recorded at. sig.csv: A csv file containing the signal stream values. Each value should be in a separate row with the first value starting in row 1. baq.csv: A csv file containing the background stream values. Each value should be in a separate row with the first value starting in row 1. OPTIONAL FILES: Event epoch files: Individual csv files containing any desired event or behavioral epochs to be included in the analysis. Each epoch type should be saved as a separate csv file (e.g., \u2018epoc_injt.csv\u2019). Each epoch value should be in a separate row with the first value starting in row 1. Note: Epochs must be specified as sample number relative to the recorded data streams. Epochs specified as time values must be converted to sample indexes prior to extracting the data. Other relevant data streams: Additional csv files with other relevant data streams, such as time indexes of each sample. After prepared individual session folders are created, copy the blocks to the Raw Data folder in the project parent folder. Create the Subject and File Keys To accomodate a variety of file organization structures, users can first create two csv files containing the information necessary to access raw data, and experimental metadata to match to raw photometry data. MATLAB can access raw data folders stored either locally or in a cloud-based storage app like Box or Dropbox. To faciliate analysis, users can create two keys as csv files: a subject key and a file key. In the PASTa protocol, these files will be knit together to pair subject specific information with each individual session of data, preventing the need for manual repeated entry of subject specific information and reduce the time burden of properly maintaining and including experimental metadata factors like subject traits, treatments, and experimental equipment. The subject key is a csv file with the experimentally relevant subject metadata that is consistent for a subject across all recording sessions (e.g., sex, DOB, implant location, sensor, experimental group, etc). The file key is a csv file that lists the names of folders containing data files and the file paths to locate raw data and store extracted data. In the PASTa protocol, the subject key information will be joined to the individual session data to pair subject specific information with each individual recording. The subject key file prevents the need for manual repeated entry of subject specific information, a process that is error-prone. Further, a subject key file reduces the time burden of properly maintaining and including factors like subject traits, treatments, and experimental equipment. At minimum, the subject key must contain the field \u201cSubjectID\u201d . Any additional fields can be added as columns in the csv file, and fields can contain any format of data (numeric, string, etc). Create and Save the Subject Key The subject key should contain information about each subject that is constant and unchanging such as SubjectID, sex, date of birth, fiber location, sensor, experimental group, and any other user specificed information. For example: 1. Create a .csv file with a column labeled \u201cSubjectID\u201d. Enter the unique SubjectID for each subject included in the experiment. 2. Optional but recommended: Add additional columns for relevant metadata, such as \u201cSex\u201d , \u201cDOB\u201d , \u201cFiberPlacement\u201d , \u201cFiberSide\u201d , \u201cSensor\u201d , \u201cGroup\u201d , or other experimental variables. These fields will be carried forward into the final data structure and are useful for downstream analyses. Create and Save the File Key In the file key, each row should correspond to one fiber photometry session. For each session, the key must include at least the SubjectID, path to the raw data block, and path to where the extracted MATLAB structure should be saved. All file paths should be specified in the file key without the specific root directory portion of the path. For example , a file saved to a specific device at the path \"C:\\Users\\rmdon\\Box\\RawData\\\" should be specified as \"Box\\RawData\\\" . This facilitates analysis across multiple devices or cloud storage solutions without manual edits to the file key. Ensure the specified path ends in a forward slash. 1. Create a csv file with the columns \u201cSubjectID\u201d, \u201cBlockFolder\u201d, \u201cRawFolderPath\u201d, and \u201cExtractedFolderPath\u201d. 2. : For each recording session, add a new row to the file: - In the \u201cSubjectID\u201d column, enter the corresponding \u201cSubjectID\u201d for the session. Ensure the File Key SubjectIDs match exactly with the IDs used in the Subject Key. In the \u201cBlockFolder\u201d column, enter the name of the block folder that contains the raw fiber photometry data for that session. In the \u201cRawFolderPath\u201d column, specify the relative path to the parent directory that contains the block folder listed in the \u201cBlockFolder\u201d column. End the path with a forward slash (e.g., '\\'). In the \u201cExtractedFolderPath\u201d column, enter the relative path to the parent directory where the extracted data structure should be saved. This should also end in a forward slash. The extracted data location should be separate from the location of the raw data blocks. Note: All folder paths should be specified in the file key without the system-specific root directory portion of the path. This facilitates analysis across multiple devices or cloud storage solutions without manual edits to the file key. Ensure the specified path ends in a forward slash. For example, for a file where the extracted data structure should be saved to the path \"C:\\Users\\rmdon\\Box\\ExtractedData\\\", the ExtractedFolderPath should be specified as \"Box\\ExtractedData\\\". Note: For Mac users, all paths should be set using the convention of forward slashes in path directories. 3. Optional but recommended: - Include session-specific variables as additional columns in the File Key. These may include information such as equipment used, recording power, session condition, drug treatments or injection volumes, body weight, or other relevant details unique to each recording session. Use field names without spaces or special characters to ensure compatibility with downstream analyses. Note: The only field name that should be included in both the Subject Key and File Key is \u201cSubjectID\u201d. Ensure all additional fields have unique and non-duplicated names. For example: Prepare MATLAB for analysis Prior to loading and processing fiber photometry data, the MATLAB environment is prepared by adding customized functions and data locations to the MATLAB path and creating helper variables to aid in accessing files. The subject key and file key are joined to create the overall experimentkey , which is used to locate, extract, and load fiber photometry data into MATLAB. Prior to beginning analysis, individual session data is extracted and saved as MATLAB data structures. This makes the process of loading data at the start of each analysis session significantly faster. Prepare the MATLAB environment Define the variable rootdirectory to store the system-specific portion of the file path. This typically corresponds to the user\u2019s home directory or another consistent root location on the local machine (e.g., \u2018C:\\Users\\rmdon\\\u2019). Add the directories containing the subject key, file key, and PASTa function scripts to the MATLAB search path. This can be done manually through the MATLAB interface or in the main script using the addpath function. Code example: Create the Experiment Key Use the createExperimentKey function to create the experimentkey data structure. This function merges the individual subject information from the subject key with the session information from the file key. It also appends the rootdirectory and folder names from the field \u2018Folder\u2019 to the \u2018RawFolderPath\u2019 and \u2018ExtractedFolderPath\u2019 fields. This creates the full paths for each session. The experimentkey is output as a MATLAB data structure. REQUIRED INPUTS: rootdirectory: The top level path unique to the user\u2019s system to be appended to RawFolderPath and ExtractedFolderPath subjectkeyname: The filename of the prepared Subject Key csv file (string; e.g., \u2018subjectkey.csv\u2019 ). If no subject key is necessary (i.e., the experiment only involves one session per subject), set the subjectkeyname to empty (e.g., \u2018\u2019). filekeyname: The filename of the prepared File Key csv file (string). OUTPUT: experimentkey : A data structure with each row in the file key matched to the subject key, with the full path from root directory to folder name in the \u2018RawFolderPath\u2019 and \u2018ExtractedFolderPath\u2019 fields. Code example: Extract fiber photometry data Extract the fiber photometry data and save the resulting MATLAB structure for each session. One MATLAB structure will be created and saved per session. Saving data in this format facilitates efficient data loading across multiple sessions during downstream analysis. Pre-extracting and saving session-level structures significantly reduces processing time when accessing large datasets or performing batch analyses. When the raw data is extracted, trimming will be applied by default. By default, this removes the first 5 seconds of the session to remove large fluctuations in output signal that occur when the hardware is turned on and off. The number of seconds trimmed can be adjusted by overriding the default. Trimming example. First 30 seconds of a fiber photometry recording session (Ventral Tegmental Area (VTA) dopamine activity, GCaMP6f), showing A) the raw signal from the 465nm channel and B) the raw background from the 405nm channel. Both panels illustrate the initial large artifact immediately following the start of the recording. To mitigate undesired effects on signal processing, PASTa by default extracts the data and trims the first 5 seconds of each session. To extract fiber photometry data and save MATLAB structures for each session, two functions are available depending on the hardware used for data collection. Option 1: Synapse (TDT) Format For data collected with Synapse (Tucker Davis Technologies) software, use the extractTDTdata function to extract and save a MATLAB structure for each session. REQUIRED INPUTS: rawfolderpaths: A string array containing the full path from root directory to folder name for all sessions to be extracted (i.e., the paths contained in the experimentkey field \u2018RawFolderPath\u2019 ). extractedfolderpaths: A string array containing the full path from root directory to folder name for all sessions to be extracted (e.g., the paths contained in the experimentkey field \u2018ExtractedFolderPath\u2019 ). Note: the experimentkey can be used to prepare the string arrays of the rawfolderpaths and the extractedfolderpaths. For each array, subset the field in the experimentkey into a cell array and convert the values to strings. See the \u2018Example Analysis\u2019 scripts and the code below for an example. sigstreamnames: A cell array containing all possible names of the signal stream in the raw data blocks (e.g., {\u2018465\u2019, \u2018x465\u2019}). baqstreamnames: A cell array containing all possible names of the background or control stream in the raw data blocks (e.g., {\u2018405\u2019, \u2018x405\u2019}). OPTIONAL INPUTS: \u2018trim\u2019: Specify the number of seconds to trim from the start of the session. By default, the first 5 seconds are trimmed from each stream (see trimming example figure above). \u2018skipexisting\u2019: By default, the function will skip any sessions that already have an extracted MATLAB structure saved to the specified ExtractedFolderPath. To reprocess and overwrite existing structures, set \u2018skipexisting\u2019 to 0. OUTPUT: Individual MATLAB structures with extracted data for each session, saved to the location specified by the extractedfolderpaths . Each structure includes session date, session time, session length, the sampling rate (fs), fiber photometry signal and background streams (sig and baq), and any event epochs. Note: The names of the streams in the raw data blocks are set in the program Synapse. To determine what stream names were set, check the \u2018StoresListing.txt\u2019 file in the raw data block folders. Stream names may be stored with an \u2018x\u2019 in front of the numbers, and the \u2018x\u2019 may not appear in stores listing. As naming conventions may vary by fiber photometry rig, the inputs allow users to include multiple naming conventions. The function will identify which streams exist in the stored data and load the stores into the sig (signal) and baq (background or control) fields respectively. Code example: Option 2: CSV Format For data collected with other systems (e.g., Neurophotometrics, Doric), use the extractCSVdata function to extract and save a MATLAB structure for each session. REQUIRED INPUTS: rawfolderpaths: A string array containing the full path from root directory to folder name for all sessions to be extracted (i.e., the paths contained in the experimentkey field \u2018RawFolderPath\u2019 ). extractedfolderpaths: A string array containing the full path from root directory to folder name for all sessions to be extracted (e.g., the paths contained in the experimentkey field \u2018ExtractedFolderPath\u2019 ). Note: the experimentkey can be used to prepare the string arrays of the rawfolderpaths and the extractedfolderpaths. For each array, subset the field in the experimentkey into a cell array and convert the values to strings. See the \u2018Example Analysis\u2019 scripts and the code below for an example. sigstreamname: A string containing the name of csv file of the signal stream in the raw data block folders (e.g., \u2018465\u2019). baqstreamname: A string containing the name of csv file of the background or control stream stream in the raw data block folders (e.g., \u2018405\u2019). OPTIONAL INPUTS: 'loadepocs\u2019: Set to 1 to load files containing event epochs. \u2018epocsnames\u2019: A cell array containing all names of the csv files in the raw data block folders containing event epochs to be extracted and added to the data structure (e.g., {\u2018injt\u2019, \u2018strt\u2019}). \u2018trim\u2019: Specify the number of seconds to trim from the start of the session. By default, the first 5 seconds are trimmed from each stream (see trimming example figure above). \u2018skipexisting\u2019: By default, the function will skip any sessions that already have an extracted MATLAB structure saved to the specified ExtractedFolderPath. To reprocess and overwrite existing structures, set \u2018skipexisting\u2019 to 0. OUTPUT: Individual MATLAB structures with extracted data for each session, saved to the location specified by the extractedfolderpaths . Each structure includes session date, session time, session length, the sampling rate (fs), fiber photometry signal and background streams (sig and baq), and any event epochs. Code example: Load Fiber Photometry Data To facilitate the processing of fiber photometry data and transient event detection, the PASTa protocol utilizes MATLAB data structures to organize the inputs and outputs to and from PASTa functions. The prepared data structure should contain each fiber photometry recording session as a row, with all subject and session specific metadata in addition to fiber photometry data streams, sampling rate, and event epochs. Load data into a MATLAB structure Use the loadKeydata function to load the extracted fiber photometry data for all sessions in the experimentkey . This function matches each session in the experiment key to its corresponding extracted MATLAB structure and appends fiber photometry data fields. Regardless of hardware set up and which data extraction function is used, all extracted files can be loaded with the function loadKeydata . REQUIRED INPUTS: experimentkey: The experimentkey object created by the createExperimentKey function. Output: data: The combined data structure containing all fields in the experiment key appended with the extracted fiber photometry data for each session. All subject and session specific metadata are included alongside the fiber photometry data for streamlined analysis. Note: If desired, users can create and use their own pipeline to organize and load fiber photometry data into MATLAB. To be compatible with PASTa functions, the data preparation process should result in a MATLAB data structure with each fiber photometry session as a row with fields containing at least a SubjectID variable, the photometry signal stream (e.g., 465nm stream), and the photometry control or background stream (e.g., 405nm stream). Code example: Optional: Crop fiber photometry data In cases where fiber photometry recordings begin or end outside the experimentally relevant period \u2013 for example, when hardware is manually triggered before or after the behavioral task begins or ends \u2013 users may wish to crop the data streams to remove the beginning and end of each session. Additionally, users may wish to exclude the initial portion of the recording session (e.g., the first few minutes), where photobleaching is typically more pronounced and nonlinear before signal stabilization. Crop data example: Fiber photometry (VTA dopamine activity, GCaMP6f) raw A) signal (465nm) and B) background (405nm) data streams each fit with an exponential decay function (red). Both streams show a steeper decay slope in the first two minutes, reflecting an initially faster rate of photobleaching that stabilizes over time. To account for this, the PASTa protocol recommends cropping the data streams to exclude the first two minutes of each session prior to further analysis. If cropping is desired, users must first prepare the start and end indexes of the range of the data streams to be included. Any samples outside the start and stop indexes will be remove. If event epochs are included in the analysis, users should specify an optional input to adjust the epochs by the crop start index to maintain spatial aligned with data streams. 1. Prepare session start and end indexes. Add fields to the data structure specifying the desired session start and end sample indexes for each session. Ensure these values are integers. Note: Start and end points can typically be calculated using time and sampling rate or can be anchored to a known event epoch such as the start of a behavioral paradigm. Examples of start and end index calculation are included in the Example Analysis files. Code example: 2. Use the cropFPdata function to crop fiber photometry streams based on user-defined session start and end sample indexes. This function also adjusts event epochs to maintain correct temporal alignment. REQUIRED INPUTS: data: Data structure created by the LoadKeyData function. Each session should be a separate row. The data structure must containing at least the fields specified in the additional inputs. REQUIRED INPUTS: data: The full data structure containing all session data. cropstartfieldname: The name (string) of the field in the data structure containing the prepared start indexes for cropping (e.g., \u2018sessionstart\u2019). cropendfieldname: A string containing the name of the field with the locations of the session end indices. Everything after the end index will be cropped. streamfieldnames: A cell array containing the names of the streams to be cropped. Typically, this includes the fields containing the signal and background streams (e.g., {\u2018sig\u2019, \u2018baq\u2019}). OPTIONAL INPUTS: 'epocsfieldnames': A cell array containing the names of all event epoch fields that should be adjusted in accordance with the cropping. Epoch fields should contain timestamps in sample number for each event. Each timestamp will be offset by the number of samples removed at the start of the session. Any number of event fields can be adjusted depending on the needs of the experimental paradigm (e.g., {\u2018strt\u2019, \u2018injt\u2019}). OUTPUT: data: The data structure with the specified fields cropped and any input epochs adjusted to maintain alignment. Note: The cropFPdata function directly modifies the fields in the input MATLAB data structure by overwriting the specific fiber photometry streams and any event epochs selected for adjusted. To maintain data integrity and traceability, consider loading the original fiber photometry data into a separate structure (e.g., rawdata) and saving the cropped output to a new structure (e.g., data). This approach helps preserve the unaltered data and prevent accidental double cropping during analysis. Code example of cropping: After data preparation is complete, continue to Signal Processing .","title":"Data Preparation"},{"location":"userguide/datapreparation/#data-preparation","text":"The first stage in the PASTa Protocol the organization, extraction, and loading of fiber photometry data and associated metadata into MATLAB prior to signal processing and analysis. To streamline the analysis workflow, PASTa recommends a standardized data structure that facilitates compatibility and minimizes user error. Organizing data using the subject and file keys allows users to easily merge photometry data with experimental variables across multiple sessions, while reducing manual errors that can occur from repeated copying and pasting. This metadata management approach ensures consistent handling of subject identifiers and experimental conditions, which is essential for batch processing and reproducible analysis. To import fiber photometry data into MATLAB, PASTa includes built-in functions to extract data collected with TDT systems or data collected with other systems via a generic csv format option. Pre-extracting photometry signals into individual MATLAB structures greatly reduces processing time across multiple analysis sessions, as the raw data extraction from TDT data blocks or the generic CSV data block format is often the most computationally-intensive step in the workflow. Preparing and organizing data in this format ahead of analysis ensures smooth integration with PASTa\u2019s functionality and improves overall analysis workflow efficiency. However, if neither extraction option is compatible with user data, users can adapt the existing extraction functions or create their own to accommodate other file structures and import data into MATLAB. The signal processing and transient detection functions are written to be flexible to alternative data structure organizations, providing the required fields are available in the primary data structure.","title":"Data Preparation"},{"location":"userguide/datapreparation/#data-organization","text":"To accommodate a variety of file organization structures, users first create two csv files containing the information necessary to access raw photometry data and the relevant experimental metadata for each subject and recording session. MATLAB can access raw data folders stored either locally or in a cloud-based storage app like Box or Dropbox.","title":"Data Organization"},{"location":"userguide/datapreparation/#prepare-folder-structure","text":"While the functions included in PASTa are written to be flexible and usable with differing data organization practices, we recommend users organize the data for each analysis in a parent folder with subfolders for Raw Data, Extracted Data, Analysis, and Figures. Create the project parent folder with the subfolders \"Raw Data\" , \"Extracted Data\" , \"Analysis\" , and \"Figures\" . The \u201cRaw Data\u201d folder should include subfolders containing the individual fiber photometry session blocks. PASTa currently includes two options for fiber photometry data: 1) Synapse (TDT) Format, and 2) CSV Format. If your data doesn't match the available load options, please feel free to reach out!","title":"Prepare Folder Structure"},{"location":"userguide/datapreparation/#option-1-synapse-tdt-format","text":"Fiber photometry data collected through the software Synapse (Tucker Davis Technologies) is stored in tanks and blocks . Tanks are parent folders created by Synapse for each experiment. Blocks are individual folders for each session containing the actual output data. Stored data cannot be accessed directly via the folder, but rather must be extracted via MATLAB. By default, the tank path is: C:\\TDT\\Synapse\\Tanks . Synapse recognizes experiments and subjects as key categories of information that play a special role in managing data storage and retrieval. Thus, when running the session, it is critical to ensure the correct Experiment profile is selected, and the correct Subject identifier is input for each session. By default, Synapse names data tanks automatically based on experiment name and the start time of the first recording (e.g., {ExperimentName}-{yymmdd}-{hhmmss}). Blocks of data are named based on subject (e.g., {SubjectName}-{yymmdd}-{hhmmss}) for each recording session and the start time. Synapse will save a new Tank for every day unless you change the default setting. Click Menu at the top of the bar, then Preferences. Under the Data Saving tab, make sure \"New Tank Each Day\" is unchecked. If data are collected with Synapse (Tucker Davis Technologies), raw block folders can be placed directly in the Raw Data folder. Copy the data blocks output by Synapse for each session into Raw Data . If blocks are nested in tanks, we recommend to unnest the blocks for ease of file key creation.","title":"Option 1: Synapse (TDT) Format"},{"location":"userguide/datapreparation/#option-2-csv-format","text":"If data are collected with other systems (e.g., Neurophotometics, Doric), users must first format the data files into CSV files. For each individual recording session, create a folder. The folder name should be unique and include at a minimum the subject ID and recording date (e.g., \u201cSubject001_04032025\u201d). Add separate CSV files with the session recording parameters (\u201crecordingparams.csv\u201d), streams (\u201cstreams.csv\u201d), and optionally any desired event epochs (\u201cepocs.csv\u201d). REQUIRED FILES: recordingparams.csv: A csv file containing fiber photometry recording parameters. Each parameter should be a separate column, with the name of the parameter in the first row and the value in the second row. At a minimum, this must include \u2018fs\u2019, containing the sampling rate the data streams were recorded at. sig.csv: A csv file containing the signal stream values. Each value should be in a separate row with the first value starting in row 1. baq.csv: A csv file containing the background stream values. Each value should be in a separate row with the first value starting in row 1. OPTIONAL FILES: Event epoch files: Individual csv files containing any desired event or behavioral epochs to be included in the analysis. Each epoch type should be saved as a separate csv file (e.g., \u2018epoc_injt.csv\u2019). Each epoch value should be in a separate row with the first value starting in row 1. Note: Epochs must be specified as sample number relative to the recorded data streams. Epochs specified as time values must be converted to sample indexes prior to extracting the data. Other relevant data streams: Additional csv files with other relevant data streams, such as time indexes of each sample. After prepared individual session folders are created, copy the blocks to the Raw Data folder in the project parent folder.","title":"Option 2: CSV Format"},{"location":"userguide/datapreparation/#create-the-subject-and-file-keys","text":"To accomodate a variety of file organization structures, users can first create two csv files containing the information necessary to access raw data, and experimental metadata to match to raw photometry data. MATLAB can access raw data folders stored either locally or in a cloud-based storage app like Box or Dropbox. To faciliate analysis, users can create two keys as csv files: a subject key and a file key. In the PASTa protocol, these files will be knit together to pair subject specific information with each individual session of data, preventing the need for manual repeated entry of subject specific information and reduce the time burden of properly maintaining and including experimental metadata factors like subject traits, treatments, and experimental equipment. The subject key is a csv file with the experimentally relevant subject metadata that is consistent for a subject across all recording sessions (e.g., sex, DOB, implant location, sensor, experimental group, etc). The file key is a csv file that lists the names of folders containing data files and the file paths to locate raw data and store extracted data. In the PASTa protocol, the subject key information will be joined to the individual session data to pair subject specific information with each individual recording. The subject key file prevents the need for manual repeated entry of subject specific information, a process that is error-prone. Further, a subject key file reduces the time burden of properly maintaining and including factors like subject traits, treatments, and experimental equipment. At minimum, the subject key must contain the field \u201cSubjectID\u201d . Any additional fields can be added as columns in the csv file, and fields can contain any format of data (numeric, string, etc).","title":"Create the Subject and File Keys"},{"location":"userguide/datapreparation/#create-and-save-the-subject-key","text":"The subject key should contain information about each subject that is constant and unchanging such as SubjectID, sex, date of birth, fiber location, sensor, experimental group, and any other user specificed information. For example: 1. Create a .csv file with a column labeled \u201cSubjectID\u201d. Enter the unique SubjectID for each subject included in the experiment. 2. Optional but recommended: Add additional columns for relevant metadata, such as \u201cSex\u201d , \u201cDOB\u201d , \u201cFiberPlacement\u201d , \u201cFiberSide\u201d , \u201cSensor\u201d , \u201cGroup\u201d , or other experimental variables. These fields will be carried forward into the final data structure and are useful for downstream analyses.","title":"Create and Save the Subject Key"},{"location":"userguide/datapreparation/#create-and-save-the-file-key","text":"In the file key, each row should correspond to one fiber photometry session. For each session, the key must include at least the SubjectID, path to the raw data block, and path to where the extracted MATLAB structure should be saved. All file paths should be specified in the file key without the specific root directory portion of the path. For example , a file saved to a specific device at the path \"C:\\Users\\rmdon\\Box\\RawData\\\" should be specified as \"Box\\RawData\\\" . This facilitates analysis across multiple devices or cloud storage solutions without manual edits to the file key. Ensure the specified path ends in a forward slash. 1. Create a csv file with the columns \u201cSubjectID\u201d, \u201cBlockFolder\u201d, \u201cRawFolderPath\u201d, and \u201cExtractedFolderPath\u201d. 2. : For each recording session, add a new row to the file: - In the \u201cSubjectID\u201d column, enter the corresponding \u201cSubjectID\u201d for the session. Ensure the File Key SubjectIDs match exactly with the IDs used in the Subject Key. In the \u201cBlockFolder\u201d column, enter the name of the block folder that contains the raw fiber photometry data for that session. In the \u201cRawFolderPath\u201d column, specify the relative path to the parent directory that contains the block folder listed in the \u201cBlockFolder\u201d column. End the path with a forward slash (e.g., '\\'). In the \u201cExtractedFolderPath\u201d column, enter the relative path to the parent directory where the extracted data structure should be saved. This should also end in a forward slash. The extracted data location should be separate from the location of the raw data blocks. Note: All folder paths should be specified in the file key without the system-specific root directory portion of the path. This facilitates analysis across multiple devices or cloud storage solutions without manual edits to the file key. Ensure the specified path ends in a forward slash. For example, for a file where the extracted data structure should be saved to the path \"C:\\Users\\rmdon\\Box\\ExtractedData\\\", the ExtractedFolderPath should be specified as \"Box\\ExtractedData\\\". Note: For Mac users, all paths should be set using the convention of forward slashes in path directories. 3. Optional but recommended: - Include session-specific variables as additional columns in the File Key. These may include information such as equipment used, recording power, session condition, drug treatments or injection volumes, body weight, or other relevant details unique to each recording session. Use field names without spaces or special characters to ensure compatibility with downstream analyses. Note: The only field name that should be included in both the Subject Key and File Key is \u201cSubjectID\u201d. Ensure all additional fields have unique and non-duplicated names. For example:","title":"Create and Save the File Key"},{"location":"userguide/datapreparation/#prepare-matlab-for-analysis","text":"Prior to loading and processing fiber photometry data, the MATLAB environment is prepared by adding customized functions and data locations to the MATLAB path and creating helper variables to aid in accessing files. The subject key and file key are joined to create the overall experimentkey , which is used to locate, extract, and load fiber photometry data into MATLAB. Prior to beginning analysis, individual session data is extracted and saved as MATLAB data structures. This makes the process of loading data at the start of each analysis session significantly faster.","title":"Prepare MATLAB for analysis"},{"location":"userguide/datapreparation/#prepare-the-matlab-environment","text":"Define the variable rootdirectory to store the system-specific portion of the file path. This typically corresponds to the user\u2019s home directory or another consistent root location on the local machine (e.g., \u2018C:\\Users\\rmdon\\\u2019). Add the directories containing the subject key, file key, and PASTa function scripts to the MATLAB search path. This can be done manually through the MATLAB interface or in the main script using the addpath function. Code example:","title":"Prepare the MATLAB environment"},{"location":"userguide/datapreparation/#create-the-experiment-key","text":"Use the createExperimentKey function to create the experimentkey data structure. This function merges the individual subject information from the subject key with the session information from the file key. It also appends the rootdirectory and folder names from the field \u2018Folder\u2019 to the \u2018RawFolderPath\u2019 and \u2018ExtractedFolderPath\u2019 fields. This creates the full paths for each session. The experimentkey is output as a MATLAB data structure. REQUIRED INPUTS: rootdirectory: The top level path unique to the user\u2019s system to be appended to RawFolderPath and ExtractedFolderPath subjectkeyname: The filename of the prepared Subject Key csv file (string; e.g., \u2018subjectkey.csv\u2019 ). If no subject key is necessary (i.e., the experiment only involves one session per subject), set the subjectkeyname to empty (e.g., \u2018\u2019). filekeyname: The filename of the prepared File Key csv file (string). OUTPUT: experimentkey : A data structure with each row in the file key matched to the subject key, with the full path from root directory to folder name in the \u2018RawFolderPath\u2019 and \u2018ExtractedFolderPath\u2019 fields. Code example:","title":"Create the Experiment Key"},{"location":"userguide/datapreparation/#extract-fiber-photometry-data","text":"Extract the fiber photometry data and save the resulting MATLAB structure for each session. One MATLAB structure will be created and saved per session. Saving data in this format facilitates efficient data loading across multiple sessions during downstream analysis. Pre-extracting and saving session-level structures significantly reduces processing time when accessing large datasets or performing batch analyses. When the raw data is extracted, trimming will be applied by default. By default, this removes the first 5 seconds of the session to remove large fluctuations in output signal that occur when the hardware is turned on and off. The number of seconds trimmed can be adjusted by overriding the default. Trimming example. First 30 seconds of a fiber photometry recording session (Ventral Tegmental Area (VTA) dopamine activity, GCaMP6f), showing A) the raw signal from the 465nm channel and B) the raw background from the 405nm channel. Both panels illustrate the initial large artifact immediately following the start of the recording. To mitigate undesired effects on signal processing, PASTa by default extracts the data and trims the first 5 seconds of each session. To extract fiber photometry data and save MATLAB structures for each session, two functions are available depending on the hardware used for data collection.","title":"Extract fiber photometry data"},{"location":"userguide/datapreparation/#option-1-synapse-tdt-format_1","text":"For data collected with Synapse (Tucker Davis Technologies) software, use the extractTDTdata function to extract and save a MATLAB structure for each session. REQUIRED INPUTS: rawfolderpaths: A string array containing the full path from root directory to folder name for all sessions to be extracted (i.e., the paths contained in the experimentkey field \u2018RawFolderPath\u2019 ). extractedfolderpaths: A string array containing the full path from root directory to folder name for all sessions to be extracted (e.g., the paths contained in the experimentkey field \u2018ExtractedFolderPath\u2019 ). Note: the experimentkey can be used to prepare the string arrays of the rawfolderpaths and the extractedfolderpaths. For each array, subset the field in the experimentkey into a cell array and convert the values to strings. See the \u2018Example Analysis\u2019 scripts and the code below for an example. sigstreamnames: A cell array containing all possible names of the signal stream in the raw data blocks (e.g., {\u2018465\u2019, \u2018x465\u2019}). baqstreamnames: A cell array containing all possible names of the background or control stream in the raw data blocks (e.g., {\u2018405\u2019, \u2018x405\u2019}). OPTIONAL INPUTS: \u2018trim\u2019: Specify the number of seconds to trim from the start of the session. By default, the first 5 seconds are trimmed from each stream (see trimming example figure above). \u2018skipexisting\u2019: By default, the function will skip any sessions that already have an extracted MATLAB structure saved to the specified ExtractedFolderPath. To reprocess and overwrite existing structures, set \u2018skipexisting\u2019 to 0. OUTPUT: Individual MATLAB structures with extracted data for each session, saved to the location specified by the extractedfolderpaths . Each structure includes session date, session time, session length, the sampling rate (fs), fiber photometry signal and background streams (sig and baq), and any event epochs. Note: The names of the streams in the raw data blocks are set in the program Synapse. To determine what stream names were set, check the \u2018StoresListing.txt\u2019 file in the raw data block folders. Stream names may be stored with an \u2018x\u2019 in front of the numbers, and the \u2018x\u2019 may not appear in stores listing. As naming conventions may vary by fiber photometry rig, the inputs allow users to include multiple naming conventions. The function will identify which streams exist in the stored data and load the stores into the sig (signal) and baq (background or control) fields respectively. Code example:","title":"Option 1: Synapse (TDT) Format"},{"location":"userguide/datapreparation/#option-2-csv-format_1","text":"For data collected with other systems (e.g., Neurophotometrics, Doric), use the extractCSVdata function to extract and save a MATLAB structure for each session. REQUIRED INPUTS: rawfolderpaths: A string array containing the full path from root directory to folder name for all sessions to be extracted (i.e., the paths contained in the experimentkey field \u2018RawFolderPath\u2019 ). extractedfolderpaths: A string array containing the full path from root directory to folder name for all sessions to be extracted (e.g., the paths contained in the experimentkey field \u2018ExtractedFolderPath\u2019 ). Note: the experimentkey can be used to prepare the string arrays of the rawfolderpaths and the extractedfolderpaths. For each array, subset the field in the experimentkey into a cell array and convert the values to strings. See the \u2018Example Analysis\u2019 scripts and the code below for an example. sigstreamname: A string containing the name of csv file of the signal stream in the raw data block folders (e.g., \u2018465\u2019). baqstreamname: A string containing the name of csv file of the background or control stream stream in the raw data block folders (e.g., \u2018405\u2019). OPTIONAL INPUTS: 'loadepocs\u2019: Set to 1 to load files containing event epochs. \u2018epocsnames\u2019: A cell array containing all names of the csv files in the raw data block folders containing event epochs to be extracted and added to the data structure (e.g., {\u2018injt\u2019, \u2018strt\u2019}). \u2018trim\u2019: Specify the number of seconds to trim from the start of the session. By default, the first 5 seconds are trimmed from each stream (see trimming example figure above). \u2018skipexisting\u2019: By default, the function will skip any sessions that already have an extracted MATLAB structure saved to the specified ExtractedFolderPath. To reprocess and overwrite existing structures, set \u2018skipexisting\u2019 to 0. OUTPUT: Individual MATLAB structures with extracted data for each session, saved to the location specified by the extractedfolderpaths . Each structure includes session date, session time, session length, the sampling rate (fs), fiber photometry signal and background streams (sig and baq), and any event epochs. Code example:","title":"Option 2: CSV Format"},{"location":"userguide/datapreparation/#load-fiber-photometry-data","text":"To facilitate the processing of fiber photometry data and transient event detection, the PASTa protocol utilizes MATLAB data structures to organize the inputs and outputs to and from PASTa functions. The prepared data structure should contain each fiber photometry recording session as a row, with all subject and session specific metadata in addition to fiber photometry data streams, sampling rate, and event epochs.","title":"Load Fiber Photometry Data"},{"location":"userguide/datapreparation/#load-data-into-a-matlab-structure","text":"Use the loadKeydata function to load the extracted fiber photometry data for all sessions in the experimentkey . This function matches each session in the experiment key to its corresponding extracted MATLAB structure and appends fiber photometry data fields. Regardless of hardware set up and which data extraction function is used, all extracted files can be loaded with the function loadKeydata . REQUIRED INPUTS: experimentkey: The experimentkey object created by the createExperimentKey function. Output: data: The combined data structure containing all fields in the experiment key appended with the extracted fiber photometry data for each session. All subject and session specific metadata are included alongside the fiber photometry data for streamlined analysis. Note: If desired, users can create and use their own pipeline to organize and load fiber photometry data into MATLAB. To be compatible with PASTa functions, the data preparation process should result in a MATLAB data structure with each fiber photometry session as a row with fields containing at least a SubjectID variable, the photometry signal stream (e.g., 465nm stream), and the photometry control or background stream (e.g., 405nm stream). Code example:","title":"Load data into a MATLAB structure"},{"location":"userguide/datapreparation/#optional-crop-fiber-photometry-data","text":"In cases where fiber photometry recordings begin or end outside the experimentally relevant period \u2013 for example, when hardware is manually triggered before or after the behavioral task begins or ends \u2013 users may wish to crop the data streams to remove the beginning and end of each session. Additionally, users may wish to exclude the initial portion of the recording session (e.g., the first few minutes), where photobleaching is typically more pronounced and nonlinear before signal stabilization. Crop data example: Fiber photometry (VTA dopamine activity, GCaMP6f) raw A) signal (465nm) and B) background (405nm) data streams each fit with an exponential decay function (red). Both streams show a steeper decay slope in the first two minutes, reflecting an initially faster rate of photobleaching that stabilizes over time. To account for this, the PASTa protocol recommends cropping the data streams to exclude the first two minutes of each session prior to further analysis. If cropping is desired, users must first prepare the start and end indexes of the range of the data streams to be included. Any samples outside the start and stop indexes will be remove. If event epochs are included in the analysis, users should specify an optional input to adjust the epochs by the crop start index to maintain spatial aligned with data streams. 1. Prepare session start and end indexes. Add fields to the data structure specifying the desired session start and end sample indexes for each session. Ensure these values are integers. Note: Start and end points can typically be calculated using time and sampling rate or can be anchored to a known event epoch such as the start of a behavioral paradigm. Examples of start and end index calculation are included in the Example Analysis files. Code example: 2. Use the cropFPdata function to crop fiber photometry streams based on user-defined session start and end sample indexes. This function also adjusts event epochs to maintain correct temporal alignment. REQUIRED INPUTS: data: Data structure created by the LoadKeyData function. Each session should be a separate row. The data structure must containing at least the fields specified in the additional inputs. REQUIRED INPUTS: data: The full data structure containing all session data. cropstartfieldname: The name (string) of the field in the data structure containing the prepared start indexes for cropping (e.g., \u2018sessionstart\u2019). cropendfieldname: A string containing the name of the field with the locations of the session end indices. Everything after the end index will be cropped. streamfieldnames: A cell array containing the names of the streams to be cropped. Typically, this includes the fields containing the signal and background streams (e.g., {\u2018sig\u2019, \u2018baq\u2019}). OPTIONAL INPUTS: 'epocsfieldnames': A cell array containing the names of all event epoch fields that should be adjusted in accordance with the cropping. Epoch fields should contain timestamps in sample number for each event. Each timestamp will be offset by the number of samples removed at the start of the session. Any number of event fields can be adjusted depending on the needs of the experimental paradigm (e.g., {\u2018strt\u2019, \u2018injt\u2019}). OUTPUT: data: The data structure with the specified fields cropped and any input epochs adjusted to maintain alignment. Note: The cropFPdata function directly modifies the fields in the input MATLAB data structure by overwriting the specific fiber photometry streams and any event epochs selected for adjusted. To maintain data integrity and traceability, consider loading the original fiber photometry data into a separate structure (e.g., rawdata) and saving the cropped output to a new structure (e.g., data). This approach helps preserve the unaltered data and prevent accidental double cropping during analysis. Code example of cropping: After data preparation is complete, continue to Signal Processing .","title":"Optional: Crop fiber photometry data"},{"location":"userguide/signalprocessing/","text":"Signal Processing PASTa is designed with accessibility and flexibility in mind, aiming to make code-based signal processing tools available to users with minimal experience. Each function includes carefully chosen default parameters that performed reliably and conservatively across a range of sensors. If desired, users can easily override any defaults using optional inputs, allowing full control over the analysis. After raw fiber photometry data is loaded in to MATLAB (see Data Preparation , signal processing is conducted to account for photobleaching, motion artifacts, and other sources of \"noise\". The signal processing functions are written to be as flexible to differing streams and naming conventions as possible, but if the functions don't match your data, please reach out and let us know and we will update. Background Scaling and Subtraction Methods Subtraction of the background stream from the signal stream controls for the overall rate of photobleaching, sources of noise common to both channels, and motion artifacts. The resultant subtracted stream is then filtered to reduce high frequency noise. To subtract the background stream from the control stream, the background first must be scaled to the signal. PASTa has set default parameters for background scaling, subtraction, and filtering. Unless modified, default parameters will be applied. However, optional inputs for each parameter are available to provide users with full control over every aspect of the signal processing. By default, PASTa scales the background control stream to the signal stream, performs background subtraction, and filters the signal, outputting the processed stream as \u0394F/F. Default scaling applies a frequency domain-based approach, determining a constant scaling factor from the ratio of the power in the frequency band from 10 Hz to 100 Hz between the background and signal streams. To determine the scaling factor, both signal and background streams are centered at 0 and Fast Fourier Transform (FFT) is used to convert both centered streams to the frequency domain (figure panel B). This allows a constant scaling factor to be determined by the components of the streams that should be weighted equally across signal and background. The raw background stream is then multiplied by the scaling factor, and the mean of the signal channel is added to produce the scaled background stream (figure panel C). This range of frequencies was selected as they are typically higher than the possible kinetics of the fluorescent sensor (e.g., noise) [13, 19, 29], and thus should be equally represented in the signal and background streams. This approach is advantageous in that it ensures that the background is not under or over scaled relative to the signal (figure panel D) and preserves the shape of the background stream through scaling. This approach also preserves the use of the background channel for artifact and photobleaching removal even when the background channel wavelength is not a true isosbestic point for the sensor, as motion artifacts will remain correlated and photobleaching is still constant in the absence of true channel independence (see below for a comparison of background scaling methods across multiple sensors). After scaling, the scaled background is then subtracted from the signal, and the result is divided by the scaled background multiplied by 100 to output the subtracted signal in \uf044F/F (figure panel E). Finally, the subtracted signal is filtered with a 3rd order bandpass Butterworth filter with a high-pass cutoff of 2.286 Hz and a low-pass cutoff of 0.0051 Hz to remove the DC offset of the subtracted signal and attenuate high frequency noise (figure panels F-H). Prior to filtering, the data stream will be padded by 10% to prevent edge effects. Padding is removed from the final output. Figure: Background scaling, subtraction, and filtering example. Representative example of default signal processing steps for a single recording session (VTA dopamine activity, GCaMP6f). A) Raw signal (465nm) and raw background (405nm) streams. B) FFT power spectrum of the raw signal and raw background streams with the power across frequencies from 0 to 100 Hz plotted in logarithmic scale. The scaling factor constant is determined in the frequency domain as the ratio of the power of the signal stream to the power of the background stream. By default, the power in all frequencies between 10 and 100 Hz (gray bar) is used to determine the scaling factor by which to multiply the background stream. C) Raw signal (465nm) overlaid with scaled background (scaled 405nm). The background stream is multiplied by the scaling factor in the time domain and centered around the mean of the raw signal. D) Power spectrum of the FFT of raw signal and scaled background streams. E) Subtracted signal (raw signal \u2013 scaled background) output as \u0394F/F. F) Zoomed in example of overlaid subtracted signal and subtracted and filtered signal. Filtering reduces high frequency fluctuations without significantly altering the shape of the signal. G) Subtracted and filtered signal trace. H) FFT of the subtracted signal before and after filtering. By default, a bandpass Butterworth filter is applied with a low pass cutoff of 0.0051 Hz and a high pass cutoff of 2.286 Hz (shaded). Note: Variance in the frequencies of interest depending on the sensor, recording region, hardware capability, and experimental question may require users to adjust the range of frequencies used in scaling. Users can easily adjust the range of frequencies used in scaling with optional function inputs. Examples of Frequency Background Scaling and Subtraction with VTA GCaMP6f, NAcLS dLight1.3b, and NAcLS GRABDA2H Background scaling and subtraction examples for VTA GCaMP6f, NAcLS dLight1.3b, and NAcLS GRABDA2H. A) Traces of 465nm (signal) and 405nm (background) raw streams. B) Magnitude frequency plots of the Fast Fourier Transform (FFT) of raw, overlaid raw, and overlaid raw 465 and scaled 405nm streams. C) Trace of raw 465nm stream with overlaid scaled 405nm stream. D) Subtracted signal output as dF/F. Overall, frequency background scaling aligns the background with the signal, preventing over or under scaling while preserving the shape of both signal streams. Alternative Background Scaling Methods In our hands, frequency scaling typically performs better than other scaling approaches. Advantages are particularly notable for sensors such as GRABDA2H, for which 405nm is not a perfect isosbestic control. Use of frequency scaling rescues the use of the 405nm stream to control for photobleaching and motion artifacts. This may be particularly useful as new sensors are continually in development, not all of which have an isosbestic or commercially available control wavelength for use in photometry systems. Users may need to adjust signal processing parameters depending on the chosen biosensor, region, and dynamics of the neural system of interest. PASTa includes alternative options for background scaling and subtraction commonly used in fiber photometry publications and other open-source and proprietary software tools. Regression-based approaches are commonly used in the field, which scale the background to the signal through ordinary least squares regression (OLS) [14]. The fitted background is then subtracted from the signal. Modifications of OLS scaling include smoothing prior to OLS regression scaling and subtraction [20] or linearly detrending both the background and signal prior to OLS regression, which are also available in PASTa. OLS regression approaches may not be advantageous for all fiber photometry applications, as they assume that all differences between the control or background stream and the signal stream are noise. As the fluorescence in the signal channel is dependent on neural activity, divergence between the streams can reduce the accuracy of artifact correction and downstream \u0394F/F calculations. Newer approaches have utilized iteratively reweighted least squares regression (IRLS) to fit the background channel to the signal. IRLS applies weights to reduce the influence of outliers during model fitting, which down-weights meaningful divergences between the background and signal streams to better capture artifacts while avoiding issues with overfitting that occur with OLS regression approaches [23]. Alternatively, some methods first fit the background and signal streams with a biexponential decay function to capture non-linear dynamics in photobleaching. The resulting fit is used to remove the non-linear decay dynamics, after which the background is scaled to the signal with either OLS regression or with a constant factor derived from the interquartile range of the signal [17]. This approach may better remove the effects of photobleaching but may not fully correct for motion artifacts or non-decay-based sources of noise. The ability to acquire the sensor\u2019s true isosbestic wavelength impacts the data during scaling and subtraction. In cases where the background channel is a true isosbestic (e.g., 405nm wavelength with GCaMP6f), the subtraction method will likely not severely impact the resultant data stream. However, in cases where the control channel is not entirely independent, OLS regression-based scaling approaches are not as reliable as they overcorrect the shape and typically under-scale the background, resulting in significantly worse performance at removing motion artifact and photobleaching. IRLS or biexponential decay fitting methods may be more robust. In testing and validation of PASTa, we found that frequency-domain scaling produced reliable results without altering the shape or significantly under-scaling the background stream and effectively removed motion artifacts and photobleach from the resultant subtracted signal. Background Scaling and Subtraction Method Comparison. Background scaling, subtraction examples and magnitude frequency plots for VTA GCaMP6f, NAcLS dLight1.3b, and NAcLS GRABDA2H with differing background scaling methods. A) Frequency Scaling: PASTa Protocol default method; The raw background is scaled to the signal based on a constant scaling factor determined by the ratio of the background to the signal in the frequency domain for frequencies greater than 10 Hz. B) OLS Regression Scaling: Commonly used method where the raw background is scaled to the signal with ordinary least squares regression, using the model fit to generate the scaled background stream. C) Linear Detrend and OLS Regression: Prior to scaling, both the raw signal and the raw background streams are detrended to remove the linear component of the stream (this is usually the decay of the signal over time). Ordinary least squares regression is then used with the detrended signals to generate the scaled background stream. D) Lowess Smoothing and OLS Regression: Prior to scaling, both the raw signal and the raw background streams are smoothed via Lowess smoothing. Ordinary least squares regression is then used with the smoothed signals to generate the scaled background stream. E) IRLS Regression: Relatively newer method where waw background is scaled to the signal with iteratively reweighted least squares regression to generate the scaled background stream. Background Scaling and Subtraction Method Correlations. Samples of scaled background and subtracted signal across scaling methods and sensors. Overall, correlations indicate that while most methods are similar for GCaMP6f and dLight1.3b, for GRABDA2H frequency scaling preserves the shape of the background and prevents underscaling relative to other methods. PASTa Background Scaling and Subtraction Options The PASTa toolbox subtraction function includes options to use OLS regression, detrending and OLS regression, Lowess Smoothing and OLS Regression, biexponential decay fitting with OLS regression or interquartile range scaling, and IRLS regression to scale the background prior to subtraction. See the References page for full citations of publications implementing each of these approaches for more detail. Filtering Considerations In addition to subtraction method, users can adjust filter parameters for the subtracted signal to better isolate the signal components that are of biological interest. By default, PASTa employs a 3rd order bandpass Butterworth filter with a high pass cutoff of 0.0051 Hz and a low pass cutoff of 2.286 Hz. The type and order of the filter were selected to best preserve the integrity of the frequencies of interest in the pass band. The cutoff frequencies were chosen based on the typical temporal dynamics of fluorescent sensors and population-level neural dynamics of interest. We primarily tested these parameters on recordings of GCaMP6f in dopamine (DA) cell bodies in the ventral tegmental area (VTA), and recordings of DA release in the nucleus accumbens (NAc) with GRABDA2H and dLight1.3b. If sensor kinetics or the range of frequencies of biological interest vary significantly from these, users may need to adjust the filtering parameters. Use of the frequency domain plot functions ( plotFFTpower and plotFFTmag ) may be helpful in visualizing the frequency components of the signal and performance of the filter relative to experimental goals. When performing the subtraction and filtering, filter parameters can be easily adjusted using optional function inputs. Filtering example. Examples of subtracted and filtered data for for VTA GCaMP6f, NAcLS dLight1.3b, and NAcLS GRABDA2H. A) Magnitude frequency plots of the Fast Fourier Transform (FFT) of the subtracted and filtered streams. Note that the Butterworth bandpass filter is overlaid (grey dashed line) on the pre-filtering subtracted stream magnitude frequency plots. B) Trace of the subtracted stream (signal - scaled background). C) Trace of the subtracted and filtered stream. D) Zoomed in overlaid example of the subtracted and filtered streams. Note that the filtered stream is in the darker shade. While the PASTa Protocol has default filter settings, users can override these as required by sensor or experimental design to modify the filter type (band-pass, high-pass, low-pass), order, and cutoff frequencies. Subtracted and Filtered Stream Output By default, PASTa outputs the subtracted and filtered signal streams in \u0394F/F, which represents the signal as the proportional change in fluorescence relative to baseline, effectively normalizing the range of the fluorescence. This is advantageous as it makes the values of the signal stream scale-invariant, controlling for variability in sensor expression, fiber placement, and signal amplitude, and enabling comparison across trials, individual recording sessions, and subjects. In cases where baseline values are unstable or experimenters prefer to analyze absolute changes in fluorescence within a session, analysis of the uncorrected \u0394F values may be preferable. Users can alter the output of the subtraction and filtering function to \u0394F using an optional input to the function. Overall Takeaways Ultimately, each signal processing method may have advantages and disadvantages, and the ideal methodology may heavily depend on the fluorescent sensor, properties of the control or background stream, and design of the experiment. PASTa allows users to apply and compare a wide variety of background scaling and subtraction methods, integrating commonly used approaches in fiber photometry signal processing into a central toolbox. Subtract and Filter Fiber Photometry Data Use the subtractFPdata function to perform signal processing including scaling the background stream, subtracting the background from the signal stream, and filtering the subtracted signal. Original raw data fields in the data structure are preserved. Processed data fields with the scaling background, subtracted signal, and filtered signal are appended. REQUIRED INPUTS: data: The full data structure containing all session data. sigfieldname: The name of the field (string) containing the signal stream (e.g., \u2018sig\u2019 ). baqfieldname: The name of the field (string) containing the background stream (e.g., \u2018baq\u2019 ). fsfieldname: The name of the field (string) containing the sampling rate of the data (e.g., \u2018fs\u2019 ). OPTIONAL INPUTS: baqscalingtype: Manually set an alternative background scaling method. Available background scaling methods are: 'frequency' (default): The raw background is scaled to the signal based on a constant scaling factor determined by the ratio of the power of the background to the power of the signal in the frequency domain for frequencies greater than 10 Hz and less than 100 Hz. 'sigmean': Constant scaling method where the raw background is centered around the signal mean but otherwise unadjusted. 'OLS': Commonly used method where the raw background is scaled to the signal with ordinary least squares (OLS) regression, using the model fit to generate the scaled background stream ( Sherathiya et al, 2021 ). 'detrendedOLS': Prior to scaling, both the raw signal and the raw background streams are detrended to remove the linear component of the stream (this is usually the decay of the signal over time). OLS regression is then used with the detrended signals to generate the scaled background stream. 'smoothedOLS': Prior to scaling, both the raw signal and the raw background streams are smoothed via Lowess smoothing. OLS regression is then used with the smoothed signals to generate the scaled background stream ( Bruno et al, 2021 ). \u2018biexpOLS\u2019: Prior to scaling, both the raw signal and raw background streams are fit with a biexponential decay and divided by the fitted curve to linearize the signal. Ordinary least squares regression is then used to fit the linearized background to the linearized signal to generate the scaled background stream ( Murphy et al, 2024 ). 'biexpQuartFit\u2019: Prior to scaling, both the raw signal and raw background streams are fit with a biexponential decay and divided by the fitted curve to linearize the signal. The background stream is then scaled to the signal stream by multiplying the background by the interquartile range of the signal. The quartile fit background is then adjusted so that the background median value is the same as the signal median ( Murphy et al, 2024 ). 'IRLS': Relatively newer method using iteratively reweighted least squares regression to fit the background to the signal stream to generate the scaled background ( Keevers et al, 2025 ). OPTIONAL INPUTS: Optional for 'frequency' scaling only: Adjust 'frequency' scaling parameters using optional inputs. 'baqscalingfreqmin': Specify the minimum frequency (Hz) threshold for scaling (Default: 10 Hz). We recommend the minimum frequency should be well outside the range of interest, which is determined by your sensor and region of interest. 'baqscalingfreqmax': Specify the maximum frequency (Hz) threshold for scaling (Default: 100 Hz). The maximum is set to avoid over scaling due to high frequency bands well outside the range of interest. The maximum cannot be set higher than half the sampling rate of your recordings (fs). 'baqscalingperc': Adjust the scaling factor by a percent. By default, the scaling factor is unadjusted (set to 1, or 100%). Background scaling can be reduced if desired by setting the baqscalingperc to less than 1. subtractionoutput: Manually specify the output type for the subtracted data streams. Options are \\Delta$F/F ( 'dff' ; default) or \\Delta$F ('df'). 'dff': Subtracted signal is output as \\Delta$ F/F. \\Delta$F/F = (Signal - Scaled Background) / Scaled Background 'df': Subtracted signal is output as \\Delta$ F. \\Delta$F = Signal - Scaled Background 'artifactremoval': Automatically detect and remove artifacts from the subtracted data stream. Set to 1 (Default: 0) to automatically detect and remove large artifacts from the subtracted data prior to filtering. Note: If \u2018artifactremoval\u2019 is set to 1, identified artifacts will be added to a table with artifact indexes. In the subtracted signal stream, artifacts will be replaced with the mean of the signal in the subtracted data stream for compatibility with filtering. An additional field with artifacts replaced by NaNs will be added to the data structure. An additional field with the non-adjusted subtracted signal (sigsubraw) will also be appended to the data structure. Optional filter parameters: Manually adjust the filter to be applied to the subtracted data stream. 'filtertype': Specify the filter type to apply to the data stream. Options are 'bandpass' (default), 'highpass', 'lowpass', or 'nofilter'. 'padding': By default, padding will be applied to the data stream prior to filtering. To override and skip padding, set to 0 (default: 1). 'paddingperc': Specify the percentage of data length to be used for padding. By default, 10% (0.1) of the data stream will be used for padding. Note that the padding percentage must be set to a minimum of 10% and a maximum of 100%. 'filterorder': Order to be used for the applied filter. By default, the order is set to 3. 'highpasscutoff': Frequency in Hz to use for the high-pass filter cutoff. Default: 2.286. 'lowpasscutoff': Frequency in Hz to use for the low-pass filter cutoff. Default: 0.0051. OUTPUTS: data: The data structure appended with fields for: params.subtractFPdata : All parameters used by the function. baqscaled : The scaled background stream. baqscalingfactor : The background scaling factor. sigsub : The subtracted signal. sigfilt : The subtracted and filtered signal. Code example: Subtraction with default scaling and filter parameters Plot Fiber Photometry Data Streams Visualization of the raw and processed data streams is essential to validate the quality of data for every animal and recording session. Further, visual inspection is critical to confirm the consistency and efficacy of your signal processing parameters. Plot Traces To facilitate visualization across all stages of signal processing, PASTa includes a basic plot function that generates a figure with the raw signal (figure panel A), raw background (figure panel B), raw signal with overlaid scaled background (figure panel C), subtracted signal (figure panel D), and subtracted and filtered signal traces (figure panel E). This function can be used in a loop to customize plots and save outputs for each recording session. plotTraces. Example output of the plotTraces function, which generates a tiled layout with the following fiber photometry data streams for a single recording session (VTA dopamine activity, GCaMP6f): A) raw signal (465nm), B) raw background (405nm), C) raw signal overlaid with scaled background, D) subtracted signal, and E) subtracted and filtered signal. Use the function plotTraces to generate the tiled layout with raw signal, raw background, raw signal with overlaid scaled background, subtracted signal, and subtracted and filtered signal for each session. REQUIRED INPUTS: When calling the function, users must specify: - data: The full data structure containing all session data output from the subtractFPdata function. fileindex: The file number to plot. Specify as an integer corresponding to the row in the data structure with the session to be plotted. maintitle: The main title to be displayed on the figure. OPTIONAL INPUTS: Automatically save the plot to a specified file path. 'saveoutput': Set to 1 to automatically save the created figure (default: 0). 'outputfiletype': Manually specify the file type of the saved figure. Available options are png, jpg, tiff, eps, and pdf (default: \u2018png\u2019). 'plotfilepath': Specify the path to save the figure to, including the full path from root directory to the figure folder and ending in the specific filename. OUTPUT: A tiled layout figure containing plots of the raw signal (sig), raw background (baq), raw signal with overlaid scaled background (sig and baqscaled), subtracted (sigsub), and subtracted and filtered signal (sigfilt). Optional: Use the plotTraces function in a loop to automatically plot all sessions, modify plots to add relevant experimental markets, and output the plots to the desired location. For each session in the data structure, generate the maintitle for the plot. Call the function plotTraces and assign the output figure to an object. Add any desired markers (e.g., session start indicator or injection time point line) to each plot tile, adjust the size of the plot, prepare a unique file path, and save the plot to the specified plot file path. Code example: plotTraces Review Trace Plots Inspect the stream plots for data quality, including the individual stages of signal processing (background scaling, subtraction, subtraction and filtering). - Identify any quality issues in the raw data stream and visually confirm a strong signal to noise ratio. - Inspect the raw data streams for large artifacts and observe the efficacy of the subtraction and filtration for removing artifacts. - Identify anomalies with signal processing and visually inspect the efficacy of the background scaling method, subtraction, and filtering for maintaining the shape of the signal stream while removing common artifacts and photobleaching effects. Plot FFTs To facilitate inspection of signal quality and signal processing effects, PASTa includes two functions for visualizing the frequency spectra of raw and processed data streams \u2013 plotFFTpower and plotFFTmag (see figure below). Examining the frequency domain may help diagnose noise sources, evaluate filter performance, and assess the preservation of biologically relevant signal content. plotFFTs. Example output of the frequency domain representation of fiber photometry streams for a single recording session (VTA GCaMP6f) created by the functions plotFFTpower and plotFFTmag . Top to bottom: FFT power spectral density (left) and magnitude plots (right) show the frequency content on a log scale of: A) raw signal (465nm channel), B) raw background (405nm channel; control), C) overlaid raw signal and raw background, D) overlaid raw signal with scaled background, E) background subtracted signal, and F) background subtracted and filtered signal. These plots visualize how preprocessing steps reduce high frequency noise while preserving low frequency neural signal components. Use the function plotFFTpower to plot the frequency-power plots and/or the function plotFFTmag to plot the frequency-magnitude plots for the FFTs of the raw signal, raw background, raw signal with overlaid scaled background, subtracted signal, and subtracted and filtered signal streams. REQUIRED INPUTS: When calling the function, users must specify: - data: The full data structure containing all session data output from the subtractFPdata function. - fileindex: The file number to plot. Specify as an integer corresponding to the row in the data structure with the session to be plotted. - maintitle: The main title to be displayed on the figure. - fsfieldname: The name of the field containing the sampling rate of the streams (e.g., 'fs' ). OPTIONAL INPUTS: - 'xmax': Adjust the x-axis (frequency) max value to aid visualization. Set 'xmax' to the desired frequency in Hz, or to 'actual' to plot all available frequencies. By default, plotFFTpower and plotFFTmag will plot power by frequency up to 100 Hz, excluding all values for frequencies above 100 Hz. Optional: Automatically save the plot to a specified file path. - 'saveoutput': Set to 1 to automatically save the created figure (default: 0). - 'outputfiletype': Manually specify the file type of the saved figure. Available options are png, jpg, tiff, eps, and pdf (default: 'png' ). - 'plotfilepath': Specify the path to save the figure to, including the full path from root directory to the figure folder and ending in the specific filename. Optional: Use the plotFFTpower or plotFFTmag function in a loop to automatically plot the FFTs of all sessions and output the plots to the desired location. - For each session in the data structure, generate the maintitle for the plot. Call the function plotFFTpower or plotFFTmag and assign the output figure to an object. Adjust the size of the plot, prepare a unique file path, and save the plot to the specified plot file path. Code example: plotFFTpower and plotFFTmag Review FFT Plots Inspect the FFT stream plots for data quality, including the individual stages of signal processing (background scaling, subtraction, subtraction and filtering). - Identify any quality issues in the raw data stream and visually confirm a strong signal to noise ratio. - Inspect the raw data streams for large artifacts and observe the efficacy of the subtraction and filtration for removing artifacts. - Identify anomalies with signal processing and visually inspect the efficacy of the background scaling method, subtraction, and filtering for maintaining the shape of the signal stream while removing common artifacts and photobleaching effects. Normalization Normalization converts the filtered signal to Z score. While normalization is optional, it often facilitates comparisons across sessions and between treatment groups. The best method of normalization will depend heavily on the design of the experiment and individual recording session structure. Normalization of fiber photometry data is a critical step that facilitates meaningful comparisons across time, within and between sessions, and between subjects. Choosing an appropriate normalization strategy can strongly influence both the shape and interpretation of neural signals. One widely used approach is z-score normalization, which converts \u0394F/F traces into units of standard deviation (SD) from the mean. This standardization allows for intuitive interpretation of the data (e.g., effect size relative to variability) and enables cross-session and cross-subject comparisons by placing the processed signal on a common scale. However, z-score normalization is not without tradeoffs and can alter the shape and detectability of signals depending on the method of application ( Wallace et al, 2025 ). Because it is a linear transformation, the effects on the signal depend on how the mean and SD are defined. Whole-session z-scoring, a commonly used approach, centers the entire stream at zero and scales it based on total session variance. While this facilitates comparisons within and between sessions, it can obscure absolute changes in fluorescence. For instance, a pharmacological treatment that increases the frequency or amplitude of transient events may inflate session-wide variance, leading to a compression of z-scored signals and masking of event-related dynamics. Conversely, treatments that suppress activity may reduce session variance, exaggerating minor fluctuations in the normalized data stream. Similarly, trial-based paradigms with strong evoked responses may see signal distortion if those large events dominate the normalization parameters. To address these potential issues, normalization can instead be based on a stable pre-treatment or pre-trial baseline period within the session, preserving sensitivity to treatment or task-induced changes in the signal. Alternatively, users may select a custom normalization window (such as the first and last few minutes of a session) to avoid early session transients or anchor the analysis to a consistent resting state. In some cases, using raw \u0394F or \u0394F/F values without additional z-scoring may be preferable, particularly if the baseline is unavailable or unstable. PASTa provides multiple normalization options, including whole-session, baseline, and fully custom z-score methods, enabling users to tailor normalization to their experimental design. To support transparent decision making, PASTa also includes a visualization function that outputs a tiled layout of multiple normalized traces from a session to help users evaluate the impact of different normalization strategies. Importantly, transient event detection functions in PASTa are compatible with both \u0394F and \u0394F/F streams, giving users flexibility in how they process and analyze their data while preserving interpretability. PASTa includes multiple normalization options. Whole session normalization uses the mean and SD of the whole session. Session baseline uses the mean and SD from a specified session baseline, which may be useful in cases where a drug is delivered mid session, experimental events occur mid session, or other treatments may result in a sustained shift that could bias signal normalization. If additional options are required, please let us know! Normalization examples. A) Normalization (Z Score) to whole session mean and SD. B) Normalization (Z Score) to a 3-min session baseline mean and SD. Option 1: Whole Session Z-Score Normalize (Z-score) the subtracted and filtered signal to whole session mean and standard deviation. Use the normSession function to Z-score the subtracted and filtered signal stream. REQUIRED INPUTS: When calling the function, specify: data: The full data structure containing all session data output from the subtractFPdata function. streamfieldname: The name of the field containing the stream to be normalized (e.g., 'sigfilt' ). OUTPUT: data: The full data structure with the normalized data appended to an additional field. The field name will be specified as the streamfieldname concatenated with 'z_normsession'. For example, if 'sigfilt' is the specified stream, the output normalized stream will be named 'sigfiltz_normsession'. Option 2: Session Baseline Z-Score Normalize (Z-score) to session baseline mean and standard deviation. First, prepare fields in the data structure containing indexes for the desired baseline period start and end points for each session. These indexes should be in sample number. Use the normBaseline function to Z-score the subtracted and filtered signal stream to the mean and standard deviation of the desired baseline period. REQUIRED INPUTS: When calling the function, specify: data: The full data structure containing all session data output from the subtractFPdata function. streamfieldname: The name of the field containing the stream to be normalized (e.g., 'sigfilt' ). BLstartfieldname: The name of the field containing the baseline period start indices for each session. BLendfieldname: The name of the field containing the baseline period end indices for each session. OUTPUT: data: The full data structure with the normalized data appended to an additional field. The field name will be specified as the streamfieldname concatenated with 'z_normbaseline'. For example, if 'sigfilt' is the specified stream, the output normalized stream will be named 'sigfiltz_normbaseline'. Option 3: Custom Session Period Z-Score Normalize (Z-score) to custom session period mean and standard deviation Prepare a field in the data structure containing the portions of the stream to be used for the calculating of the mean and standard deviation for normalization. This could include any portions of the stream desired by the user and should be one combined array. For example, users may want to normalize both pre-trial and post-trial baselines. The pre- and post-trial stream data can be appended to one array and used for normalization of the whole session. Use the normCustom function to Z-score the subtracted and filtered signal stream to the mean and standard deviation of the custom period. REQUIRED INPUTS: When calling the function, specify: data: The full data structure containing all session data output from the subtractFPdata function. streamfieldname: The name of the field containing the stream to be normalized (e.g., 'sigfilt'). customstreamfieldname: The name of the field containing the custom cut portion of the full stream to be used for the mean and standard deviation in Z-score calculations. OUTPUT: data: The full data structure with the normalized data appended to an additional field. The field name will be specified as the fullstreamfieldname concatenated with 'z_normcustom'. For example, if 'sigfilt' is the specified stream, the output normalized stream will be named 'sigfiltz_normcustom'. CODE EXAMPLE: Plot Normalized Data Streams Plot the normalized data streams using the plotNormTraces function. As normalization can affect the shape of the signal depending on the session and/or experimental design, it may be useful to plot and compare multiple methods of normalization, such as whole session vs session baseline. plotNormTraces Normalized trace plot of a single recording session of VTA dopamine activity (GCaMP6f). Subtracted and filtered data are Z-scored to the whole session mean and standard deviation. Use the plotNormTraces function to generate a figure with plots of all normalized data streams. As many streams as desired can be input to allow for easy visual comparison. REQUIRED INPUTS: When calling the function, specify: data: The full data structure containing all session data including all normalized streams to be plotted. fileindex: The file number to plot. Specify as an integer corresponding to the row in the data structure with the session to be plotted. streamfieldnames: A cell array containing the names of the normalized streams to be plotted (e.g., {'sigfiltz_normsession', 'sigfiltz_normbaseline'}). fsfieldname: The name of the field containing the sampling rate of the streams (e.g., 'fs'). maintitle: The main title to be displayed on the figure. streamtitles: A cell array containing the desired titles for each subplot, corresponding to each stream in the streamfieldnames input. The titles should indicate which method of normalization was used for each stream provided (e.g., {'Normalized to Whole Session', 'Normalized to Session Baseline'}). OPTIONAL INPUTS: Automatically save the plot to a specified file path. - Set the optional input 'saveoutput' to 1. - Manually specify the file type to save the figure as (outputfiletype; default 'png'). Available options are png, jpg, tiff, eps, and pdf. - Use the optional input 'plotfilepath' to provide the full path for the figure including the full path from root directory to plot folder, ending in the specific filename. CODE EXAMPLE: After signal processing is complete, continue to Transient Analysis .","title":"Signal Processing"},{"location":"userguide/signalprocessing/#signal-processing","text":"PASTa is designed with accessibility and flexibility in mind, aiming to make code-based signal processing tools available to users with minimal experience. Each function includes carefully chosen default parameters that performed reliably and conservatively across a range of sensors. If desired, users can easily override any defaults using optional inputs, allowing full control over the analysis. After raw fiber photometry data is loaded in to MATLAB (see Data Preparation , signal processing is conducted to account for photobleaching, motion artifacts, and other sources of \"noise\". The signal processing functions are written to be as flexible to differing streams and naming conventions as possible, but if the functions don't match your data, please reach out and let us know and we will update.","title":"Signal Processing"},{"location":"userguide/signalprocessing/#background-scaling-and-subtraction-methods","text":"Subtraction of the background stream from the signal stream controls for the overall rate of photobleaching, sources of noise common to both channels, and motion artifacts. The resultant subtracted stream is then filtered to reduce high frequency noise. To subtract the background stream from the control stream, the background first must be scaled to the signal. PASTa has set default parameters for background scaling, subtraction, and filtering. Unless modified, default parameters will be applied. However, optional inputs for each parameter are available to provide users with full control over every aspect of the signal processing. By default, PASTa scales the background control stream to the signal stream, performs background subtraction, and filters the signal, outputting the processed stream as \u0394F/F. Default scaling applies a frequency domain-based approach, determining a constant scaling factor from the ratio of the power in the frequency band from 10 Hz to 100 Hz between the background and signal streams. To determine the scaling factor, both signal and background streams are centered at 0 and Fast Fourier Transform (FFT) is used to convert both centered streams to the frequency domain (figure panel B). This allows a constant scaling factor to be determined by the components of the streams that should be weighted equally across signal and background. The raw background stream is then multiplied by the scaling factor, and the mean of the signal channel is added to produce the scaled background stream (figure panel C). This range of frequencies was selected as they are typically higher than the possible kinetics of the fluorescent sensor (e.g., noise) [13, 19, 29], and thus should be equally represented in the signal and background streams. This approach is advantageous in that it ensures that the background is not under or over scaled relative to the signal (figure panel D) and preserves the shape of the background stream through scaling. This approach also preserves the use of the background channel for artifact and photobleaching removal even when the background channel wavelength is not a true isosbestic point for the sensor, as motion artifacts will remain correlated and photobleaching is still constant in the absence of true channel independence (see below for a comparison of background scaling methods across multiple sensors). After scaling, the scaled background is then subtracted from the signal, and the result is divided by the scaled background multiplied by 100 to output the subtracted signal in \uf044F/F (figure panel E). Finally, the subtracted signal is filtered with a 3rd order bandpass Butterworth filter with a high-pass cutoff of 2.286 Hz and a low-pass cutoff of 0.0051 Hz to remove the DC offset of the subtracted signal and attenuate high frequency noise (figure panels F-H). Prior to filtering, the data stream will be padded by 10% to prevent edge effects. Padding is removed from the final output. Figure: Background scaling, subtraction, and filtering example. Representative example of default signal processing steps for a single recording session (VTA dopamine activity, GCaMP6f). A) Raw signal (465nm) and raw background (405nm) streams. B) FFT power spectrum of the raw signal and raw background streams with the power across frequencies from 0 to 100 Hz plotted in logarithmic scale. The scaling factor constant is determined in the frequency domain as the ratio of the power of the signal stream to the power of the background stream. By default, the power in all frequencies between 10 and 100 Hz (gray bar) is used to determine the scaling factor by which to multiply the background stream. C) Raw signal (465nm) overlaid with scaled background (scaled 405nm). The background stream is multiplied by the scaling factor in the time domain and centered around the mean of the raw signal. D) Power spectrum of the FFT of raw signal and scaled background streams. E) Subtracted signal (raw signal \u2013 scaled background) output as \u0394F/F. F) Zoomed in example of overlaid subtracted signal and subtracted and filtered signal. Filtering reduces high frequency fluctuations without significantly altering the shape of the signal. G) Subtracted and filtered signal trace. H) FFT of the subtracted signal before and after filtering. By default, a bandpass Butterworth filter is applied with a low pass cutoff of 0.0051 Hz and a high pass cutoff of 2.286 Hz (shaded). Note: Variance in the frequencies of interest depending on the sensor, recording region, hardware capability, and experimental question may require users to adjust the range of frequencies used in scaling. Users can easily adjust the range of frequencies used in scaling with optional function inputs.","title":"Background Scaling and Subtraction Methods"},{"location":"userguide/signalprocessing/#examples-of-frequency-background-scaling-and-subtraction-with-vta-gcamp6f-nacls-dlight13b-and-nacls-grabda2h","text":"Background scaling and subtraction examples for VTA GCaMP6f, NAcLS dLight1.3b, and NAcLS GRABDA2H. A) Traces of 465nm (signal) and 405nm (background) raw streams. B) Magnitude frequency plots of the Fast Fourier Transform (FFT) of raw, overlaid raw, and overlaid raw 465 and scaled 405nm streams. C) Trace of raw 465nm stream with overlaid scaled 405nm stream. D) Subtracted signal output as dF/F. Overall, frequency background scaling aligns the background with the signal, preventing over or under scaling while preserving the shape of both signal streams.","title":"Examples of Frequency Background Scaling and Subtraction with VTA GCaMP6f, NAcLS dLight1.3b, and NAcLS GRABDA2H"},{"location":"userguide/signalprocessing/#alternative-background-scaling-methods","text":"In our hands, frequency scaling typically performs better than other scaling approaches. Advantages are particularly notable for sensors such as GRABDA2H, for which 405nm is not a perfect isosbestic control. Use of frequency scaling rescues the use of the 405nm stream to control for photobleaching and motion artifacts. This may be particularly useful as new sensors are continually in development, not all of which have an isosbestic or commercially available control wavelength for use in photometry systems. Users may need to adjust signal processing parameters depending on the chosen biosensor, region, and dynamics of the neural system of interest. PASTa includes alternative options for background scaling and subtraction commonly used in fiber photometry publications and other open-source and proprietary software tools. Regression-based approaches are commonly used in the field, which scale the background to the signal through ordinary least squares regression (OLS) [14]. The fitted background is then subtracted from the signal. Modifications of OLS scaling include smoothing prior to OLS regression scaling and subtraction [20] or linearly detrending both the background and signal prior to OLS regression, which are also available in PASTa. OLS regression approaches may not be advantageous for all fiber photometry applications, as they assume that all differences between the control or background stream and the signal stream are noise. As the fluorescence in the signal channel is dependent on neural activity, divergence between the streams can reduce the accuracy of artifact correction and downstream \u0394F/F calculations. Newer approaches have utilized iteratively reweighted least squares regression (IRLS) to fit the background channel to the signal. IRLS applies weights to reduce the influence of outliers during model fitting, which down-weights meaningful divergences between the background and signal streams to better capture artifacts while avoiding issues with overfitting that occur with OLS regression approaches [23]. Alternatively, some methods first fit the background and signal streams with a biexponential decay function to capture non-linear dynamics in photobleaching. The resulting fit is used to remove the non-linear decay dynamics, after which the background is scaled to the signal with either OLS regression or with a constant factor derived from the interquartile range of the signal [17]. This approach may better remove the effects of photobleaching but may not fully correct for motion artifacts or non-decay-based sources of noise. The ability to acquire the sensor\u2019s true isosbestic wavelength impacts the data during scaling and subtraction. In cases where the background channel is a true isosbestic (e.g., 405nm wavelength with GCaMP6f), the subtraction method will likely not severely impact the resultant data stream. However, in cases where the control channel is not entirely independent, OLS regression-based scaling approaches are not as reliable as they overcorrect the shape and typically under-scale the background, resulting in significantly worse performance at removing motion artifact and photobleaching. IRLS or biexponential decay fitting methods may be more robust. In testing and validation of PASTa, we found that frequency-domain scaling produced reliable results without altering the shape or significantly under-scaling the background stream and effectively removed motion artifacts and photobleach from the resultant subtracted signal. Background Scaling and Subtraction Method Comparison. Background scaling, subtraction examples and magnitude frequency plots for VTA GCaMP6f, NAcLS dLight1.3b, and NAcLS GRABDA2H with differing background scaling methods. A) Frequency Scaling: PASTa Protocol default method; The raw background is scaled to the signal based on a constant scaling factor determined by the ratio of the background to the signal in the frequency domain for frequencies greater than 10 Hz. B) OLS Regression Scaling: Commonly used method where the raw background is scaled to the signal with ordinary least squares regression, using the model fit to generate the scaled background stream. C) Linear Detrend and OLS Regression: Prior to scaling, both the raw signal and the raw background streams are detrended to remove the linear component of the stream (this is usually the decay of the signal over time). Ordinary least squares regression is then used with the detrended signals to generate the scaled background stream. D) Lowess Smoothing and OLS Regression: Prior to scaling, both the raw signal and the raw background streams are smoothed via Lowess smoothing. Ordinary least squares regression is then used with the smoothed signals to generate the scaled background stream. E) IRLS Regression: Relatively newer method where waw background is scaled to the signal with iteratively reweighted least squares regression to generate the scaled background stream. Background Scaling and Subtraction Method Correlations. Samples of scaled background and subtracted signal across scaling methods and sensors. Overall, correlations indicate that while most methods are similar for GCaMP6f and dLight1.3b, for GRABDA2H frequency scaling preserves the shape of the background and prevents underscaling relative to other methods.","title":"Alternative Background Scaling Methods"},{"location":"userguide/signalprocessing/#pasta-background-scaling-and-subtraction-options","text":"The PASTa toolbox subtraction function includes options to use OLS regression, detrending and OLS regression, Lowess Smoothing and OLS Regression, biexponential decay fitting with OLS regression or interquartile range scaling, and IRLS regression to scale the background prior to subtraction. See the References page for full citations of publications implementing each of these approaches for more detail.","title":"PASTa Background Scaling and Subtraction Options"},{"location":"userguide/signalprocessing/#filtering-considerations","text":"In addition to subtraction method, users can adjust filter parameters for the subtracted signal to better isolate the signal components that are of biological interest. By default, PASTa employs a 3rd order bandpass Butterworth filter with a high pass cutoff of 0.0051 Hz and a low pass cutoff of 2.286 Hz. The type and order of the filter were selected to best preserve the integrity of the frequencies of interest in the pass band. The cutoff frequencies were chosen based on the typical temporal dynamics of fluorescent sensors and population-level neural dynamics of interest. We primarily tested these parameters on recordings of GCaMP6f in dopamine (DA) cell bodies in the ventral tegmental area (VTA), and recordings of DA release in the nucleus accumbens (NAc) with GRABDA2H and dLight1.3b. If sensor kinetics or the range of frequencies of biological interest vary significantly from these, users may need to adjust the filtering parameters. Use of the frequency domain plot functions ( plotFFTpower and plotFFTmag ) may be helpful in visualizing the frequency components of the signal and performance of the filter relative to experimental goals. When performing the subtraction and filtering, filter parameters can be easily adjusted using optional function inputs. Filtering example. Examples of subtracted and filtered data for for VTA GCaMP6f, NAcLS dLight1.3b, and NAcLS GRABDA2H. A) Magnitude frequency plots of the Fast Fourier Transform (FFT) of the subtracted and filtered streams. Note that the Butterworth bandpass filter is overlaid (grey dashed line) on the pre-filtering subtracted stream magnitude frequency plots. B) Trace of the subtracted stream (signal - scaled background). C) Trace of the subtracted and filtered stream. D) Zoomed in overlaid example of the subtracted and filtered streams. Note that the filtered stream is in the darker shade. While the PASTa Protocol has default filter settings, users can override these as required by sensor or experimental design to modify the filter type (band-pass, high-pass, low-pass), order, and cutoff frequencies.","title":"Filtering Considerations"},{"location":"userguide/signalprocessing/#subtracted-and-filtered-stream-output","text":"By default, PASTa outputs the subtracted and filtered signal streams in \u0394F/F, which represents the signal as the proportional change in fluorescence relative to baseline, effectively normalizing the range of the fluorescence. This is advantageous as it makes the values of the signal stream scale-invariant, controlling for variability in sensor expression, fiber placement, and signal amplitude, and enabling comparison across trials, individual recording sessions, and subjects. In cases where baseline values are unstable or experimenters prefer to analyze absolute changes in fluorescence within a session, analysis of the uncorrected \u0394F values may be preferable. Users can alter the output of the subtraction and filtering function to \u0394F using an optional input to the function.","title":"Subtracted and Filtered Stream Output"},{"location":"userguide/signalprocessing/#overall-takeaways","text":"Ultimately, each signal processing method may have advantages and disadvantages, and the ideal methodology may heavily depend on the fluorescent sensor, properties of the control or background stream, and design of the experiment. PASTa allows users to apply and compare a wide variety of background scaling and subtraction methods, integrating commonly used approaches in fiber photometry signal processing into a central toolbox.","title":"Overall Takeaways"},{"location":"userguide/signalprocessing/#subtract-and-filter-fiber-photometry-data","text":"Use the subtractFPdata function to perform signal processing including scaling the background stream, subtracting the background from the signal stream, and filtering the subtracted signal. Original raw data fields in the data structure are preserved. Processed data fields with the scaling background, subtracted signal, and filtered signal are appended. REQUIRED INPUTS: data: The full data structure containing all session data. sigfieldname: The name of the field (string) containing the signal stream (e.g., \u2018sig\u2019 ). baqfieldname: The name of the field (string) containing the background stream (e.g., \u2018baq\u2019 ). fsfieldname: The name of the field (string) containing the sampling rate of the data (e.g., \u2018fs\u2019 ). OPTIONAL INPUTS: baqscalingtype: Manually set an alternative background scaling method. Available background scaling methods are: 'frequency' (default): The raw background is scaled to the signal based on a constant scaling factor determined by the ratio of the power of the background to the power of the signal in the frequency domain for frequencies greater than 10 Hz and less than 100 Hz. 'sigmean': Constant scaling method where the raw background is centered around the signal mean but otherwise unadjusted. 'OLS': Commonly used method where the raw background is scaled to the signal with ordinary least squares (OLS) regression, using the model fit to generate the scaled background stream ( Sherathiya et al, 2021 ). 'detrendedOLS': Prior to scaling, both the raw signal and the raw background streams are detrended to remove the linear component of the stream (this is usually the decay of the signal over time). OLS regression is then used with the detrended signals to generate the scaled background stream. 'smoothedOLS': Prior to scaling, both the raw signal and the raw background streams are smoothed via Lowess smoothing. OLS regression is then used with the smoothed signals to generate the scaled background stream ( Bruno et al, 2021 ). \u2018biexpOLS\u2019: Prior to scaling, both the raw signal and raw background streams are fit with a biexponential decay and divided by the fitted curve to linearize the signal. Ordinary least squares regression is then used to fit the linearized background to the linearized signal to generate the scaled background stream ( Murphy et al, 2024 ). 'biexpQuartFit\u2019: Prior to scaling, both the raw signal and raw background streams are fit with a biexponential decay and divided by the fitted curve to linearize the signal. The background stream is then scaled to the signal stream by multiplying the background by the interquartile range of the signal. The quartile fit background is then adjusted so that the background median value is the same as the signal median ( Murphy et al, 2024 ). 'IRLS': Relatively newer method using iteratively reweighted least squares regression to fit the background to the signal stream to generate the scaled background ( Keevers et al, 2025 ). OPTIONAL INPUTS: Optional for 'frequency' scaling only: Adjust 'frequency' scaling parameters using optional inputs. 'baqscalingfreqmin': Specify the minimum frequency (Hz) threshold for scaling (Default: 10 Hz). We recommend the minimum frequency should be well outside the range of interest, which is determined by your sensor and region of interest. 'baqscalingfreqmax': Specify the maximum frequency (Hz) threshold for scaling (Default: 100 Hz). The maximum is set to avoid over scaling due to high frequency bands well outside the range of interest. The maximum cannot be set higher than half the sampling rate of your recordings (fs). 'baqscalingperc': Adjust the scaling factor by a percent. By default, the scaling factor is unadjusted (set to 1, or 100%). Background scaling can be reduced if desired by setting the baqscalingperc to less than 1. subtractionoutput: Manually specify the output type for the subtracted data streams. Options are \\Delta$F/F ( 'dff' ; default) or \\Delta$F ('df'). 'dff': Subtracted signal is output as \\Delta$ F/F. \\Delta$F/F = (Signal - Scaled Background) / Scaled Background 'df': Subtracted signal is output as \\Delta$ F. \\Delta$F = Signal - Scaled Background 'artifactremoval': Automatically detect and remove artifacts from the subtracted data stream. Set to 1 (Default: 0) to automatically detect and remove large artifacts from the subtracted data prior to filtering. Note: If \u2018artifactremoval\u2019 is set to 1, identified artifacts will be added to a table with artifact indexes. In the subtracted signal stream, artifacts will be replaced with the mean of the signal in the subtracted data stream for compatibility with filtering. An additional field with artifacts replaced by NaNs will be added to the data structure. An additional field with the non-adjusted subtracted signal (sigsubraw) will also be appended to the data structure. Optional filter parameters: Manually adjust the filter to be applied to the subtracted data stream. 'filtertype': Specify the filter type to apply to the data stream. Options are 'bandpass' (default), 'highpass', 'lowpass', or 'nofilter'. 'padding': By default, padding will be applied to the data stream prior to filtering. To override and skip padding, set to 0 (default: 1). 'paddingperc': Specify the percentage of data length to be used for padding. By default, 10% (0.1) of the data stream will be used for padding. Note that the padding percentage must be set to a minimum of 10% and a maximum of 100%. 'filterorder': Order to be used for the applied filter. By default, the order is set to 3. 'highpasscutoff': Frequency in Hz to use for the high-pass filter cutoff. Default: 2.286. 'lowpasscutoff': Frequency in Hz to use for the low-pass filter cutoff. Default: 0.0051. OUTPUTS: data: The data structure appended with fields for: params.subtractFPdata : All parameters used by the function. baqscaled : The scaled background stream. baqscalingfactor : The background scaling factor. sigsub : The subtracted signal. sigfilt : The subtracted and filtered signal. Code example: Subtraction with default scaling and filter parameters","title":"Subtract and Filter Fiber Photometry Data"},{"location":"userguide/signalprocessing/#plot-fiber-photometry-data-streams","text":"Visualization of the raw and processed data streams is essential to validate the quality of data for every animal and recording session. Further, visual inspection is critical to confirm the consistency and efficacy of your signal processing parameters.","title":"Plot Fiber Photometry Data Streams"},{"location":"userguide/signalprocessing/#plot-traces","text":"To facilitate visualization across all stages of signal processing, PASTa includes a basic plot function that generates a figure with the raw signal (figure panel A), raw background (figure panel B), raw signal with overlaid scaled background (figure panel C), subtracted signal (figure panel D), and subtracted and filtered signal traces (figure panel E). This function can be used in a loop to customize plots and save outputs for each recording session. plotTraces. Example output of the plotTraces function, which generates a tiled layout with the following fiber photometry data streams for a single recording session (VTA dopamine activity, GCaMP6f): A) raw signal (465nm), B) raw background (405nm), C) raw signal overlaid with scaled background, D) subtracted signal, and E) subtracted and filtered signal. Use the function plotTraces to generate the tiled layout with raw signal, raw background, raw signal with overlaid scaled background, subtracted signal, and subtracted and filtered signal for each session. REQUIRED INPUTS: When calling the function, users must specify: - data: The full data structure containing all session data output from the subtractFPdata function. fileindex: The file number to plot. Specify as an integer corresponding to the row in the data structure with the session to be plotted. maintitle: The main title to be displayed on the figure. OPTIONAL INPUTS: Automatically save the plot to a specified file path. 'saveoutput': Set to 1 to automatically save the created figure (default: 0). 'outputfiletype': Manually specify the file type of the saved figure. Available options are png, jpg, tiff, eps, and pdf (default: \u2018png\u2019). 'plotfilepath': Specify the path to save the figure to, including the full path from root directory to the figure folder and ending in the specific filename. OUTPUT: A tiled layout figure containing plots of the raw signal (sig), raw background (baq), raw signal with overlaid scaled background (sig and baqscaled), subtracted (sigsub), and subtracted and filtered signal (sigfilt). Optional: Use the plotTraces function in a loop to automatically plot all sessions, modify plots to add relevant experimental markets, and output the plots to the desired location. For each session in the data structure, generate the maintitle for the plot. Call the function plotTraces and assign the output figure to an object. Add any desired markers (e.g., session start indicator or injection time point line) to each plot tile, adjust the size of the plot, prepare a unique file path, and save the plot to the specified plot file path. Code example: plotTraces","title":"Plot Traces"},{"location":"userguide/signalprocessing/#review-trace-plots","text":"Inspect the stream plots for data quality, including the individual stages of signal processing (background scaling, subtraction, subtraction and filtering). - Identify any quality issues in the raw data stream and visually confirm a strong signal to noise ratio. - Inspect the raw data streams for large artifacts and observe the efficacy of the subtraction and filtration for removing artifacts. - Identify anomalies with signal processing and visually inspect the efficacy of the background scaling method, subtraction, and filtering for maintaining the shape of the signal stream while removing common artifacts and photobleaching effects.","title":"Review Trace Plots"},{"location":"userguide/signalprocessing/#plot-ffts","text":"To facilitate inspection of signal quality and signal processing effects, PASTa includes two functions for visualizing the frequency spectra of raw and processed data streams \u2013 plotFFTpower and plotFFTmag (see figure below). Examining the frequency domain may help diagnose noise sources, evaluate filter performance, and assess the preservation of biologically relevant signal content. plotFFTs. Example output of the frequency domain representation of fiber photometry streams for a single recording session (VTA GCaMP6f) created by the functions plotFFTpower and plotFFTmag . Top to bottom: FFT power spectral density (left) and magnitude plots (right) show the frequency content on a log scale of: A) raw signal (465nm channel), B) raw background (405nm channel; control), C) overlaid raw signal and raw background, D) overlaid raw signal with scaled background, E) background subtracted signal, and F) background subtracted and filtered signal. These plots visualize how preprocessing steps reduce high frequency noise while preserving low frequency neural signal components. Use the function plotFFTpower to plot the frequency-power plots and/or the function plotFFTmag to plot the frequency-magnitude plots for the FFTs of the raw signal, raw background, raw signal with overlaid scaled background, subtracted signal, and subtracted and filtered signal streams. REQUIRED INPUTS: When calling the function, users must specify: - data: The full data structure containing all session data output from the subtractFPdata function. - fileindex: The file number to plot. Specify as an integer corresponding to the row in the data structure with the session to be plotted. - maintitle: The main title to be displayed on the figure. - fsfieldname: The name of the field containing the sampling rate of the streams (e.g., 'fs' ). OPTIONAL INPUTS: - 'xmax': Adjust the x-axis (frequency) max value to aid visualization. Set 'xmax' to the desired frequency in Hz, or to 'actual' to plot all available frequencies. By default, plotFFTpower and plotFFTmag will plot power by frequency up to 100 Hz, excluding all values for frequencies above 100 Hz. Optional: Automatically save the plot to a specified file path. - 'saveoutput': Set to 1 to automatically save the created figure (default: 0). - 'outputfiletype': Manually specify the file type of the saved figure. Available options are png, jpg, tiff, eps, and pdf (default: 'png' ). - 'plotfilepath': Specify the path to save the figure to, including the full path from root directory to the figure folder and ending in the specific filename. Optional: Use the plotFFTpower or plotFFTmag function in a loop to automatically plot the FFTs of all sessions and output the plots to the desired location. - For each session in the data structure, generate the maintitle for the plot. Call the function plotFFTpower or plotFFTmag and assign the output figure to an object. Adjust the size of the plot, prepare a unique file path, and save the plot to the specified plot file path. Code example: plotFFTpower and plotFFTmag","title":"Plot FFTs"},{"location":"userguide/signalprocessing/#review-fft-plots","text":"Inspect the FFT stream plots for data quality, including the individual stages of signal processing (background scaling, subtraction, subtraction and filtering). - Identify any quality issues in the raw data stream and visually confirm a strong signal to noise ratio. - Inspect the raw data streams for large artifacts and observe the efficacy of the subtraction and filtration for removing artifacts. - Identify anomalies with signal processing and visually inspect the efficacy of the background scaling method, subtraction, and filtering for maintaining the shape of the signal stream while removing common artifacts and photobleaching effects.","title":"Review FFT Plots"},{"location":"userguide/signalprocessing/#normalization","text":"Normalization converts the filtered signal to Z score. While normalization is optional, it often facilitates comparisons across sessions and between treatment groups. The best method of normalization will depend heavily on the design of the experiment and individual recording session structure. Normalization of fiber photometry data is a critical step that facilitates meaningful comparisons across time, within and between sessions, and between subjects. Choosing an appropriate normalization strategy can strongly influence both the shape and interpretation of neural signals. One widely used approach is z-score normalization, which converts \u0394F/F traces into units of standard deviation (SD) from the mean. This standardization allows for intuitive interpretation of the data (e.g., effect size relative to variability) and enables cross-session and cross-subject comparisons by placing the processed signal on a common scale. However, z-score normalization is not without tradeoffs and can alter the shape and detectability of signals depending on the method of application ( Wallace et al, 2025 ). Because it is a linear transformation, the effects on the signal depend on how the mean and SD are defined. Whole-session z-scoring, a commonly used approach, centers the entire stream at zero and scales it based on total session variance. While this facilitates comparisons within and between sessions, it can obscure absolute changes in fluorescence. For instance, a pharmacological treatment that increases the frequency or amplitude of transient events may inflate session-wide variance, leading to a compression of z-scored signals and masking of event-related dynamics. Conversely, treatments that suppress activity may reduce session variance, exaggerating minor fluctuations in the normalized data stream. Similarly, trial-based paradigms with strong evoked responses may see signal distortion if those large events dominate the normalization parameters. To address these potential issues, normalization can instead be based on a stable pre-treatment or pre-trial baseline period within the session, preserving sensitivity to treatment or task-induced changes in the signal. Alternatively, users may select a custom normalization window (such as the first and last few minutes of a session) to avoid early session transients or anchor the analysis to a consistent resting state. In some cases, using raw \u0394F or \u0394F/F values without additional z-scoring may be preferable, particularly if the baseline is unavailable or unstable. PASTa provides multiple normalization options, including whole-session, baseline, and fully custom z-score methods, enabling users to tailor normalization to their experimental design. To support transparent decision making, PASTa also includes a visualization function that outputs a tiled layout of multiple normalized traces from a session to help users evaluate the impact of different normalization strategies. Importantly, transient event detection functions in PASTa are compatible with both \u0394F and \u0394F/F streams, giving users flexibility in how they process and analyze their data while preserving interpretability. PASTa includes multiple normalization options. Whole session normalization uses the mean and SD of the whole session. Session baseline uses the mean and SD from a specified session baseline, which may be useful in cases where a drug is delivered mid session, experimental events occur mid session, or other treatments may result in a sustained shift that could bias signal normalization. If additional options are required, please let us know! Normalization examples. A) Normalization (Z Score) to whole session mean and SD. B) Normalization (Z Score) to a 3-min session baseline mean and SD.","title":"Normalization"},{"location":"userguide/signalprocessing/#option-1-whole-session-z-score","text":"Normalize (Z-score) the subtracted and filtered signal to whole session mean and standard deviation. Use the normSession function to Z-score the subtracted and filtered signal stream. REQUIRED INPUTS: When calling the function, specify: data: The full data structure containing all session data output from the subtractFPdata function. streamfieldname: The name of the field containing the stream to be normalized (e.g., 'sigfilt' ). OUTPUT: data: The full data structure with the normalized data appended to an additional field. The field name will be specified as the streamfieldname concatenated with 'z_normsession'. For example, if 'sigfilt' is the specified stream, the output normalized stream will be named 'sigfiltz_normsession'.","title":"Option 1: Whole Session Z-Score"},{"location":"userguide/signalprocessing/#option-2-session-baseline-z-score","text":"Normalize (Z-score) to session baseline mean and standard deviation. First, prepare fields in the data structure containing indexes for the desired baseline period start and end points for each session. These indexes should be in sample number. Use the normBaseline function to Z-score the subtracted and filtered signal stream to the mean and standard deviation of the desired baseline period. REQUIRED INPUTS: When calling the function, specify: data: The full data structure containing all session data output from the subtractFPdata function. streamfieldname: The name of the field containing the stream to be normalized (e.g., 'sigfilt' ). BLstartfieldname: The name of the field containing the baseline period start indices for each session. BLendfieldname: The name of the field containing the baseline period end indices for each session. OUTPUT: data: The full data structure with the normalized data appended to an additional field. The field name will be specified as the streamfieldname concatenated with 'z_normbaseline'. For example, if 'sigfilt' is the specified stream, the output normalized stream will be named 'sigfiltz_normbaseline'.","title":"Option 2: Session Baseline Z-Score"},{"location":"userguide/signalprocessing/#option-3-custom-session-period-z-score","text":"Normalize (Z-score) to custom session period mean and standard deviation Prepare a field in the data structure containing the portions of the stream to be used for the calculating of the mean and standard deviation for normalization. This could include any portions of the stream desired by the user and should be one combined array. For example, users may want to normalize both pre-trial and post-trial baselines. The pre- and post-trial stream data can be appended to one array and used for normalization of the whole session. Use the normCustom function to Z-score the subtracted and filtered signal stream to the mean and standard deviation of the custom period. REQUIRED INPUTS: When calling the function, specify: data: The full data structure containing all session data output from the subtractFPdata function. streamfieldname: The name of the field containing the stream to be normalized (e.g., 'sigfilt'). customstreamfieldname: The name of the field containing the custom cut portion of the full stream to be used for the mean and standard deviation in Z-score calculations. OUTPUT: data: The full data structure with the normalized data appended to an additional field. The field name will be specified as the fullstreamfieldname concatenated with 'z_normcustom'. For example, if 'sigfilt' is the specified stream, the output normalized stream will be named 'sigfiltz_normcustom'. CODE EXAMPLE:","title":"Option 3: Custom Session Period Z-Score"},{"location":"userguide/signalprocessing/#plot-normalized-data-streams","text":"Plot the normalized data streams using the plotNormTraces function. As normalization can affect the shape of the signal depending on the session and/or experimental design, it may be useful to plot and compare multiple methods of normalization, such as whole session vs session baseline. plotNormTraces Normalized trace plot of a single recording session of VTA dopamine activity (GCaMP6f). Subtracted and filtered data are Z-scored to the whole session mean and standard deviation. Use the plotNormTraces function to generate a figure with plots of all normalized data streams. As many streams as desired can be input to allow for easy visual comparison. REQUIRED INPUTS: When calling the function, specify: data: The full data structure containing all session data including all normalized streams to be plotted. fileindex: The file number to plot. Specify as an integer corresponding to the row in the data structure with the session to be plotted. streamfieldnames: A cell array containing the names of the normalized streams to be plotted (e.g., {'sigfiltz_normsession', 'sigfiltz_normbaseline'}). fsfieldname: The name of the field containing the sampling rate of the streams (e.g., 'fs'). maintitle: The main title to be displayed on the figure. streamtitles: A cell array containing the desired titles for each subplot, corresponding to each stream in the streamfieldnames input. The titles should indicate which method of normalization was used for each stream provided (e.g., {'Normalized to Whole Session', 'Normalized to Session Baseline'}). OPTIONAL INPUTS: Automatically save the plot to a specified file path. - Set the optional input 'saveoutput' to 1. - Manually specify the file type to save the figure as (outputfiletype; default 'png'). Available options are png, jpg, tiff, eps, and pdf. - Use the optional input 'plotfilepath' to provide the full path for the figure including the full path from root directory to plot folder, ending in the specific filename. CODE EXAMPLE: After signal processing is complete, continue to Transient Analysis .","title":"Plot Normalized Data Streams"},{"location":"userguide/transientanalysis/","text":"Transient Detection and Quantification Transient event analysis is a widely used method for quantifying rapid, phasic changes in neural activity or neuromodulator release, and is a central analytic approach not only in fiber photometry but also in other techniques such as fast-scan cyclic voltammetry and electrophysiology [15, 30]. In fiber photometry, transients are interpreted as population-level bursts of neural activity or neurotransmitter fluctuations and can be characterized by their frequency, amplitude, duration, temporal alignment with behavioral or experimental events, and their occurrence patterns over time within a session. Given the diversity of measured characteristics and abundance of data per photometry recording, identifying changes in transient shape and occurrence is optimal for detecting treatment effects, internal state transitions, or gradual changes in neural dynamics. PASTa implements a flexible and robust framework for transient detection, designed to accommodate a wide range of experimental designs and sensor types. Previous tools and packages have used a sliding window approach, where all values above an absolute threshold are counted as peaks. Here, we present a method of peak detection where each peak is compared to a local baseline and amplitude is calculated and compared to a threshold to determine inclusion. This allows for consistent parameters across the session and reliable detection of individual events despite signal absolute value fluctuation. The core detection algorithm first identifies local peaks in the signal. For each peak, a pre-peak baseline is estimated, and the event amplitude is calculated as the difference between the peak and this baseline. The amplitude is then compared to a user-defined threshold to determine inclusion. This method is advantageous over simpler prominence or absolute value criteria as it accounts for local fluctuations in baseline, which can shift over time. In contrast, fixed-amplitude or prominence methods can be overly sensitive to slow drifts in the signal, potentially underestimating events during high-variability periods or overestimating events when variability is low [24]. Threshold Setting To detect transient events, PASTa first identifies all peaks in the data stream, finds the pre-peak baseline for each peak to calculate peak amplitude, and then compares the amplitude to the specified threshold for inclusion. Transients are detected as peaks with a greater amplitude than the specified threshold. Threshold Considerations We recommend users set the transient detection threshold at 2.6 standard deviations (SDs), which reflects a high-confidence p-value (< .01) [24] . This threshold is conservative, only detecting spontaneous events that significantly stand out from the fluctuation of the signal. Setting the threshold lower will result in detection of greater number of events with smaller amplitudes, while setting thresholds higher will result in detection of fewer events with greater amplitudes (see Fig. 7 for a comparison across thresholds). Determining the threshold for transient inclusion is critical to ensure that effects are accurately characterized. Thresholds should be set with consideration of the experimental paradigm. Certain pharmacological treatments may affect the frequency and amplitude of transients, therefore treated vs control recordings may produce vastly different whole session mean and SDs. In these cases, we recommend for users to define thresholds based on subsections of the recording session (e.g., baseline or intertrial interval periods). Similarly, the normalization method will affect which transients are included. Specific normalization methods can compress transient height (e.g., with systemic drug injection or stimulation recordings normalized to whole session mean and SD), making it harder for events to reach threshold compared to control treatments [24]. In these cases, normalizing to a baseline or detecting transients in \uf044F/F may be preferable. Transient threshold comparison. Comparison of transient detection with amplitude thresholds of 2, 2.6, and SDs from a single VTA dopamine GCaMP6f recording session. The subtracted and filtered data stream was Z-scored to the whole session mean and SD prior to transient detection. Note that each lower threshold includes the events detected by the higher thresholds (e.g., 2 SD includes the events detected by both 2.6 and 3 SD thresholds). A) Normalized trace plot showing detected transients for each threshold. B) Whole session transient count at each threshold. C) Mean transient amplitude (Z-score) at each threshold. Prepare Thresholds Prior to detecting session transients, users must prepare the thresholds to be used for event detection for each recording session. If the processed signal used for transient analysis is normalized to a Z-score, thresholds can be set in standard deviation units. If the processed signal used for transient analysis is in \u0394F or \u0394F/F, thresholds must be calculated for each session based on the standard deviation of the stream. Users must add a field to the data structure with the desired transient amplitude thresholds for each recording session. Option 1: Normalized (Z-Score) Data Streams For Z-scored data, set the threshold in SD units. We recommend a threshold of 2.6 SDs. Option 2: Non-Normalized (\u0394F or \u0394F/F) Data Streams Calculate the equivalent of 2.6 SDs (or other chosen threshold) in the units of the stream to be used for transient detection and quantification. For example, if using \u0394F/F, calculate the SD of the \u0394F/F stream and multiply it by 2.6. Code example: Detect and Quantify Transients Transient detection relies on the comparison of the maximum value of each peak in the data stream to a local baseline to determine amplitude. PASTa includes three options for transient baselines: baseline minimum, baseline mean, or local minimum. For all transient detection functions, event baseline is determined by a specified window before the peak maximum location. By default, the window spans 800ms to 100ms before the peak maximum. The baseline minimum function uses the minimum value in the window as the pre-peak baseline, while the baseline mean function uses the mean of the window. The local minimum option uses the last local minimum in the baseline window as the pre-peak baseline. Peak amplitude is calculated as the maximum peak value minus the baseline value. For each peak, the amplitude is compared to the specified threshold to determine inclusion. Transients that pass the amplitude threshold are then quantified for further analyses of transient dynamics (see figure below). For each transient event, the output includes the maximum peak index, maximum peak value, pre-peak baseline index (center of the window if using the baseline mean function), pre-peak baseline value, and peak amplitude (max peak \u2013 baseline). To more finely characterize events, temporal variables for each event are determined at half height by default. Users can adjust the specified quantification height if desired. The rise phase of each transient event is characterized by the rise start index, number of samples from rise start to maximum peak, and millisecond duration of the rise period. To characterize the fall period, the function identifies the first index within 2000 milliseconds after the peak maximum that is equal to the quantification height value; however, the post peak fall window length can be adjusted if desired. The fall phase of each event is then quantified by the fall end index, number of samples from peak maximum to fall end, and millisecond duration of the fall period. To combine rise and fall dynamics, the whole transient width in samples and milliseconds are included, as well as total area under the curve (AUC). To enable analysis of transient event dynamics, the interevent interval between each transient is included in both samples and milliseconds. Finally, transients are identified as compound events if the maximum peak location falls within +/- 2000 milliseconds of another transient event. The window for compound event identification can be adjusted if desired. Default parameters for baseline window size, post peak maximum fall window, quantification height, compound event window size are included but users can override defaults if desired to customize the transient analysis. Transient detection and quantification example. All peaks (dark blue) in the data stream are identified and compared to a baseline (light blue), which can be determined by a pre-peak window mean (default), pre-peak window minimum, or the pre-peak local minimum. Amplitude (green) is calculated as the difference between the maximum peak value and the baseline, and events with amplitudes greater than the set threshold (red) are included. Transient events are quantified by amplitude (green), rise duration from half-height (pink), fall duration from half-height (purple), and half-height area under the curve (yellow). Find Transients Use the findTransients function to detect and quantify transient events in the desired data stream. REQUIRED INPUTS: When calling the findTransients function, specify: data: The full data structure containing all session data. The data structure must include (at a minimum) fields with the stream to be analyzed for transient events, the sampling rate of the data stream, and the user determined transient threshold. addvariablesfieldnames : A cell array containing the field names of all subject metadata variables from the main data structure to add to the transient data output. For example, Subject ID, block folder name, treatment group, treatment condition, or any other experimentally relevant variables, as well as the params field with the parameters used by all other functions in the pipeline. streamfieldname: The name of the field containing the full stream to be analyzed for transient events (typically the subtracted and filtered data stream 'sigfilt' or the normalized subtracted and filtered data stream such as 'sigfiltz_normSession'). thresholdfieldname: The name of the field containing the amplitude threshold for transient inclusion. Be sure to specify the threshold in the same units as the specified stream in streamfieldname . __fsfieldname:__The name of the field containing the sampling rate of the stream (typically 'fs'). OPTIONAL INPUTS: __'bltype':__Specify the pre-transient baseline method. Available options are: 'blmean' (default): Pre-peak baselines are set to the mean value of the baseline window. 'blmin': Pre-peak baselines are set to the minimum value in the baseline window. 'localmin': Pre-peak baselines are set to the last local minimum value in the baseline window. Adjust baseline window size. By default, the baseline window is set to 1000ms to 100ms preceding the peak maximum. Users can adjust the baseline window by specifying number of milliseconds preceding the peak maximum to start and end the baseline window. 'blstartms': Set to the desired milliseconds pre-peak to adjust the start of the baseline window. 'blendms': Set to the desired milliseconds pre-peak to adjust the end of the baseline window. 'posttransientms': Adjust the post-transient fall window size in milliseconds. By default, the post-transient window to search for the fall index and value is set to 2000 ms. 'quantificationheight': Set the quantification height at which to determine the rise, fall, and area variables for each transient. By default, transient temporal dynamics are quantified at half height (0.5). Note that the input must fall between 0 and 1. Adjust the output of cut transient event data streams. By default, findTransients will output a table with the cut data streams from 5 seconds before the peak to 8 seconds after the peak for each transient to allow for plotting and additional analysis. Set optional inputs: 'outputtransientdata': Set to 0 to skip the output of cut transient data streams. 'outputpremaxS': Set to desired number of seconds pre-transient peak to include in the cut data streams. Default: 5. 'outputpostmaxS': Set to desired number of seconds post-transient peak to include in the cut data streams. Default: 8. OUTPUT: findTransients returns the separate data structure transientdata with each session as a row including the variables specified in addvariablesfieldnames , all parameters used in the analysis ( params ), the tables of quantified transient events ( transientquantification ), and the cut transient event data stream relative indexes ( transientstreamlocs) and data (transientstreamdata ). Code example: Whole session transient detection with baseline minimum and default inputs. Code example: Whole session transient detection with baseline mean and shortened pre-peak baseline window. Code example: Whole session transient detection with local minimum and 25% quantification height. Transient Quantification Variables PASTa outputs multiple features for each transient event that can be quantitatively analyzed and compared. The function findTransients automatically calculate numerous variables for each transient to characterize amplitude and event duration. The following variables are included for each transient event in the transientquantification table: maxloc: The index (sample number) of the max peak location maxval: The stream value of the max peak location. Note: this variable is in the units of the data stream used to detect transient events blstartloc: The index (sample number) of the start of the pre-peak baseline window blendloc: The index (sample number) of the end of the pre-peak baseline window blloc: The index of the baseline value. _Note: if the baseline mean ('blmean') option is used, this reflects the middle of the baseline window. If the baseline minimum ('blmin') or local minimum ('localmin') options are used, this will be the exact index of the identified baseline point blval: The value of the pre-peak baseline. Note: this variable is in the units of the data stream used to detect transient events amp: The amplitude of the event from the pre-peak baseline to the max peak (maxval - blval). Note: the amplitude of all events will be at least the value of the set threshold (reccomended 2.6 SDs) quantheightval: The value of the point corresponding to the quantification height (default half-height). Note: this variable is in the units of the data stream used to detect transient events risestartloc: The index (sample number) of the start of the rise phase, measured from the peak to quantification height (default half height) risesamples: The number of samples between the start of the rise phase to the transient peak (maxloc - risestartloc) risems: The number of milliseconds from the start of the rise phase to the transient peak (risesamples / fs (sampling rate in Hz)) fallendloc: The index (sample number) of the end of the fall phase, measured from the peak to quantification height (default half height) fallsamples: The number of samples between the transient peak and the end of the fall phase (fallendloc - maxlox) fallms: The number of milliseconds from the transient peak to the end of the fall phase (fallsamples / fs (sampling rate in Hz)) widthsamples: The number of samples between the start of the rise phase and the end of the fall phase (fallendloc - risestartloc) widthms: The number of milliseconds from the start of the rise phase and the end of the fall phase (widthsamples / fs (sampling rate in Hz)) AUC: The total area under the curve from the start of the rise phase to the end of the fall phase (determined using the trapezoidal method) IEIsamples: The total number of samples from the peak of the previous event to the peak of the current event (current event maxloc - previous event max loc) IEIms: The number of milliseconds between the peak of the previous event and the peak of the current event (IEIsamples / fs (sampling rate in Hz)) IEIs: The number of seconds between the peak of the previous event and the peak of the current event (IEIsamples / fs (sampling rate in Hz) / 60) compoundeventnum: If events occur within the compound event window, this variable will include an ID starting at 1 for each event within the window. If no other events occur within the compound event window, this will be set to 0. Output Example - Adding Transients to the Data Structure: Output of findSessionTransients, which is a sub-structure under the field sessiontransients added to the main data structure. Output Example - Function Inputs: Inputs passed to the findSessionTransients functions are output to the table Inputs within the sessiontransients field of the main data structure. Output Example - Transient Quantification: Quantification of individual transient events is output to the table transientquantification within the sessiontransients field of the main data structure. Each transient is in a separate row with a unique transient ID. Output Example - Individual Transient Trace Indexes: For individual transient traces, transient stream locations of peak and baseline indexes and values are added to the table transientstreamlocs . Each transient is in a separate row. Output Example - Individual Transient Traces: Actual stream values for individual transient traces, which are cut from the start of the baseline window to the end of the post peak period, and output to the table transientstreamdata . Each transient is in a separate row, and traces are spatially aligned consistently with the transientstreamlocs for easy plotting and analysis. Bin Transients To increase the resolution of transient analysis, it may be advantageous to identify change in transient events across the session, or during specific discrete events like during or between trials. After transient events are identified, users can divide transient events into bins based on time or specific event indexes. The function binTransients will loop through each session in the data structure and add the variable 'Bin' to the transient quantification tables output by findTransients. The total number of bins will be determined for each session based on the session length and the bin length. For each bin, the start and end time sample index will be determined. All transient events within the start and end indexes will be labeled with the discrete bin number to enable analysis of transient dynamics across time. Use the binTransients function to divide transients into bins. By default, transients will be divided into 5-minute time bins. The number of bins will be determined automatically based on the whole session length. Users can override the defaults to adjust the number of minutes per bin, manually set the total number of bins, or manually pass bin start and end indexes to determine bins based on relevant time epochs. REQUIRED INPUTS: When calling the binTransients function, specify: - transientdata: The output transient data structure from the findTransients function ('transientdata'). This structure must include at a minimum the 'params.findTransients' and 'transientquantification' fields. OPTIONAL INPUTS: Adjust bin settings with optional inputs: 'binlengthmins': Adjust bin length (minutes). Set to desired number of minutes per bin. Default: 5. Note: adjusting the bin length will also change the name of the output field in the transient quantification table. For example, if 'binlengthmins' is set to 3, the output field will be named 'Bin_3mins'. This facilitates comparisons across different bin lengths if desired. 'nbinsoverride': Manually set the total number of bins. This option may be useful if there is minor variance in recording length, but users want to ensure the same number of total bins across sessions. Set to desired total number of bins. If set to anything other than 0, the override will be applied. Default: 0. Note: any events outside the final bin end index will not be assigned a bin number. 'manuallydefinebins'': Manually define bin start and end indexes, rather than defining bins based on time increments. If this option is set to TRUE or 1, users must pass additional inputs 'binstartfieldname' and 'binendfieldname' to define the beginning and end of each bin. Note: If custom bin specification is necessary, bin start and end indexes should be set up prior to calling the binTransients function. Bin start and end indexes are typically determined based on trial start and end epochs, specific events like injection, or behavioral events time locked to the recording data. 'binstartfieldname': The name of the field in data that contains the array of bin start indexes for each session. Bin start indexes should be specified in sample number. This input is required if 'manuallydefinebins' is set to TRUE or 1. 'binendfieldname': The name of the field in data that contains the array of bin end indexes for each session. Bin start indexes should be specified in sample number. This input is required if 'manuallydefinebins' is set to TRUE or 1. OUTPUT: For each session, the Bin variable will be added to the transient quantification table. The field name for Bin is specified as 'Bin_(binlengthmins)_mins'. For example, with the function default bin length of 5 minutes, the bin assignment field will be named 'Bin_5mins'. This facilitates comparisons across different bin lengths if desired. Adjusting the bin length will change the name of the output field in the transient quantification table. For example, if 'binlengthmins' is set to 3, the output field will be named 'Bin_3mins'. If the 'manuallydefinebins' optional input is utilized, the bin assignment field name will be specified as 'Bin_Custom'. Export Individual Transient Events After transients are identified, quantified, and processed, users may want to conduct statistical analysis and generate summary plots in other computational programs (e.g., R). To facilitate this transfer, users can combine transient events from each session into a single composite table with relevant subject and session level variables and output the table to a csv file. Use the exportTransients function to create a single table with individual session transient quantification tables and specified subject and session variables. REQUIRED INPUTS: When calling the exportTransients function, specify: transientdata: The output transient data structure from the findTransients function. This structure must include the 'transientquantification' field. exportfilepath: The full path to the location at which to save the output csv file. addvariablesfieldnames: A cell array containing the field names of subject and session variables to include with the transient quantification for each file. These variables will be added to every row of the output table. At a minimum, this should include the SubjectID. If multiple sessions per subject are included in the analysis, ensure a session ID variable is also included. OPTIONAL INPUTS: 'exportfilename' : Specify the file name to save the output as. By default, the file name will be generated as 'TransientQuantification_AllSessionExport_DAY-MONTH-YEAR.csv'. To manually specify the name of the output file, input the desired name as a string, ending in .csv . OUTPUT: If passed into an object, exportTransients will return the compiled table of transients for all sessions in the data structure into a table ( alltransients ). The compiled table of transients for all sessions will be saved as a .csv file under the name and file path specified. Plot Transient Events To visualize the detected transients and review for quality control, plot the data streams and overlay markers to identify the individual transient events. PASTa includes multiple options to assist in the visualization of transient events, with functions to plot the whole session data stream trace with all identified transient events (figure panel A), data stream trace with identified transient events by bin (figure panel B), overlaid individual transient event traces for the whole stream (figure panel C), and overlaid individual transient event traces by bin (figure panel D). We recommend plotting transient events as many ways as possible for quality verification and interpretation. Transient event plots. Example outputs from transient plot functions for a single fiber photometry recording session (VTA dopamine activity, GCaMP6f). The subtracted and filtered signal was Z-scored to the whole session mean and SD prior to transient detection. The threshold for transient event detection was set at 2.6 SDs. A) Whole recording session trace (green) with identified transient events (blue circles) generated by the function plotTransients . B) Recording session trace divided into five 2-minute bins with detected transients (blue circles), generated by the function plotTransientBins . C) Overlaid traces of all individual transients in the session aligned to peak maxima, generated by the function plotTransientTraces . D) Overlaid aligned transient traces grouped by session bin, generated by the function plotTransientBinTraces . Plot Whole Session Trace with Transient Events Use the plotTransients function to plot the whole session data stream trace with overlaid transient event markers. REQUIRED INPUTS: When calling the function, users must specify: data: The full data structure containing all session data including all normalized streams to be plotted fileindex: The file number to plot (specify as an integer corresponding to the row in the data structure with the session to be plotted) streamfieldname: The name of the field containing the stream to be plotted. This should be the same as the stream used for transient event detection and quantification input to the findTransients function fsfieldname: The name of the field containing the sampling rate of the streams (typically 'fs' ) transientdata: The output transient data structure from the findTransients function. This structure must include at a minimum the 'transientquantification' field with peak indexes in 'maxloc' maintitle: The main title to be displayed on the figure, specified as a string OPTIONAL INPUTS: Automatically save the plot to a specified file path: 'saveoutput': Set the optional input 'saveoutput' to 1 outputfiletype: Manually specify the file type to save the figure as (default 'png' ). Available options are png, jpg, tiff, eps, and pdf 'plotfilepath': The output path for the figure including the full path from root directory to plot folder, ending in the specific filename OUTPUT: A plot of the whole session stream with transient events marked with circles (see figure panel A above). Plot Session Trace with Transient Events by Bin Use the plotTransientBins function to plot the data stream trace with overlaid transient event markers by bin. REQUIRED INPUTS: When calling the function, users must specify: data: The full data structure containing all session data including all normalized streams to be plotted fileindex: The file number to plot (specify as an integer corresponding to the row in the data structure with the session to be plotted) streamfieldname: The name of the field containing the stream to be plotted. This should be the same as the stream used for transient event detection and quantification input to the findTransients function transientdata: The output transient data structure from the findTransients function. This structure must include at a minimum the 'transientquantification' field with peak indexes in 'maxloc' . binfieldname: The name of the field in transientdata under transientquantification that contains the bin IDs for each transient event (e.g., 'Bin_5min'). maintitle: The main title to be displayed on the figure, specified as a string. OPTIONAL INPUTS: Automatically save the plot to a specified file path: 'saveoutput': Set the optional input 'saveoutput' to 1. outputfiletype: Manually specify the file type to save the figure as (default 'png' ). Available options are png, jpg, tiff, eps, and pdf. 'plotfilepath': The output path for the figure including the full path from root directory to plot folder, ending in the specific filename. OUTPUT: A tiled plot of the session stream trace with transient events marked with circles by bin (see figure panel B above). Plot Overlaid Transient Event Traces for the Whole Session Use the plotTransientTraces function to plot overlaid traces for all transient events in the session. REQUIRED INPUTS: When calling the function, users must specify: transientdata: The output transient data structure from the findTransients function. This structure must include the cut data streams for each transient in the field 'transientstreamdata' . fileindex: The file number to plot (specify as an integer corresponding to the row in the data structure with the session to be plotted) maintitle: The main title to be displayed on the figure, specified as a string. OPTIONAL INPUTS: Automatically save the plot to a specified file path: 'saveoutput': Set the optional input 'saveoutput' to 1. outputfiletype: Manually specify the file type to save the figure as (default 'png' ). Available options are png, jpg, tiff, eps, and pdf. 'plotfilepath': The output path for the figure including the full path from root directory to plot folder, ending in the specific filename. OUTPUT: A plot of overlaid transient event traces from the whole session (see figure panel C above). Plot Overlaid Transient Event Traces by Bin Use the plotTransientTraceBins function to plot overlaid traces for transient events by bin. REQUIRED INPUTS: When calling the function, users must specify: transientdata: The output transient data structure from the findTransients function. This structure must include the cut data streams for each transient in the field 'transientstreamdata' . fileindex: The file number to plot (specify as an integer corresponding to the row in the data structure with the session to be plotted) binfieldname: The name of the field in transientdata under transientquantification that contains the bin IDs for each transient event (e.g., 'Bin_5min'). maintitle: The main title to be displayed on the figure, specified as a string. OPTIONAL INPUTS: Automatically save the plot to a specified file path: 'saveoutput': Set the optional input 'saveoutput' to 1. outputfiletype: Manually specify the file type to save the figure as (default 'png' ). Available options are png, jpg, tiff, eps, and pdf. 'plotfilepath': The output path for the figure including the full path from root directory to plot folder, ending in the specific filename. OUTPUT: A plot of overlaid transient event traces by bin (see figure panel D above). Summarize Transient Events After transient events have been detected and quantified, users may want to summarize transient quantification for the whole session, or by bin. Overall session values can be calculated from the transientquantification tables output by the findTransients function for each subject using the summarizeTransients and summarizeBinTransients functions. Summarize Whole Session Transient Events Use the summarizeTransients function to calculate overall session values and means for transient quantification variables for each recording session in the analysis. REQUIRED INPUTS: When calling the summarizeTransients function, users must specify: transientdata: The output transient data structure from the findTransients function. This structure must include at a minimum the 'params.findTransients' and 'transientquantification' fields. OUTPUT: The function will return the transientdata structure with the added field 'transientsummary_session' containing the average transient quantification output. Included quantification variables are: freq: Total number of transient events identified in the session freqpermin: Rate of transient events per minute in the session (total number of transient events / session length in minutes) freqhz: Rate of transient events per second in the session (total number of transient events / session length in seconds) maxval: Mean of transient event peak maximum values blval : Mean of pre-transient event baseline values amp: Mean of transient event amplitudes (maxval \u2013 blval) quantheightval: Mean of transient event values at the quantification height (default half height) risesamples: Mean number of rise phase samples from quantification height to transient event peak maximum value (maxloc \u2013 risestartloc) risems: Mean rise phase length (milliseconds) from quantification height to transient event peak maximum value ((maxloc \u2013 risestartloc) / fs (sampling rate in Hz)) fallsamples: Mean number of fall phase samples from transient event peak maximum value to quantification height to (fallendloc \u2013 maxloc) fallms: Mean fall phase length (milliseconds) from quantification height to transient event peak maximum value (fallendloc \u2013 maxloc) ((maxloc \u2013 risestartloc) / fs (sampling rate in Hz)) widthsamples: Mean number of samples from quantification height at rise start to quantification height at fall end (fallendloc \u2013 risestartloc) widthms: Mean transient width length (milliseconds) at quantification height from rise start to fall end ((fallendloc \u2013 risestartloc) / fs (sampling rate in Hz)) __AUC: Mean area under the curve from rise start to fall end IEIsamples: Mean number of samples between transient event maximum peak locations (Interevent intervals; maxloc2 \u2013 maxloc1) IEIms: Mean time (milliseconds) between transient event maximum peak locations ((maxloc2 \u2013 maxloc1) / fs (sampling rate in Hz)) IEIs: Mean time (seconds) between transient event maximum peak locations ((maxloc2 \u2013 maxloc1) / (fs / 60)) compoundeventtotal: Total number of compound transient events identified in the session Note: For all transient variables reflected the actual value of the data stream (maxval, blval, and quantheightval) that this will be in the same units as the data stream input for transient detection. For example, if normalized streams are used for transient detected then values will be in Z-score, but if the non-normalized \u0394F/F is used then value will be in \u0394F/F. Summarize Transient Events by Bin Use the summarizeBinTransients function to calculate bin values and means for transient quantification variables for each recording session in the analysis. REQUIRED INPUTS: When calling the summarizeTransients function, users must specify: transientdata: The output transient data structure from the findTransients function. This structure must include at a minimum the 'params.findTransients' and 'transientquantification' fields. binfieldname: The name of the field in transientdata under transientquantification that contains the bin IDs for each transient event (e.g., 'Bin_5min'). OUTPUT: The function will return the transientdata structure with the added field 'transientsummary ' containing the average transient quantification output by time bin. Quantification variables (see above, Summarize Whole Session Transient Events) will be returned for each bin, with the bin ID in the first column of the table under the field name specified by _binfieldname .","title":"Transient Analysis"},{"location":"userguide/transientanalysis/#transient-detection-and-quantification","text":"Transient event analysis is a widely used method for quantifying rapid, phasic changes in neural activity or neuromodulator release, and is a central analytic approach not only in fiber photometry but also in other techniques such as fast-scan cyclic voltammetry and electrophysiology [15, 30]. In fiber photometry, transients are interpreted as population-level bursts of neural activity or neurotransmitter fluctuations and can be characterized by their frequency, amplitude, duration, temporal alignment with behavioral or experimental events, and their occurrence patterns over time within a session. Given the diversity of measured characteristics and abundance of data per photometry recording, identifying changes in transient shape and occurrence is optimal for detecting treatment effects, internal state transitions, or gradual changes in neural dynamics. PASTa implements a flexible and robust framework for transient detection, designed to accommodate a wide range of experimental designs and sensor types. Previous tools and packages have used a sliding window approach, where all values above an absolute threshold are counted as peaks. Here, we present a method of peak detection where each peak is compared to a local baseline and amplitude is calculated and compared to a threshold to determine inclusion. This allows for consistent parameters across the session and reliable detection of individual events despite signal absolute value fluctuation. The core detection algorithm first identifies local peaks in the signal. For each peak, a pre-peak baseline is estimated, and the event amplitude is calculated as the difference between the peak and this baseline. The amplitude is then compared to a user-defined threshold to determine inclusion. This method is advantageous over simpler prominence or absolute value criteria as it accounts for local fluctuations in baseline, which can shift over time. In contrast, fixed-amplitude or prominence methods can be overly sensitive to slow drifts in the signal, potentially underestimating events during high-variability periods or overestimating events when variability is low [24].","title":"Transient Detection and Quantification"},{"location":"userguide/transientanalysis/#threshold-setting","text":"To detect transient events, PASTa first identifies all peaks in the data stream, finds the pre-peak baseline for each peak to calculate peak amplitude, and then compares the amplitude to the specified threshold for inclusion. Transients are detected as peaks with a greater amplitude than the specified threshold.","title":"Threshold Setting"},{"location":"userguide/transientanalysis/#threshold-considerations","text":"We recommend users set the transient detection threshold at 2.6 standard deviations (SDs), which reflects a high-confidence p-value (< .01) [24] . This threshold is conservative, only detecting spontaneous events that significantly stand out from the fluctuation of the signal. Setting the threshold lower will result in detection of greater number of events with smaller amplitudes, while setting thresholds higher will result in detection of fewer events with greater amplitudes (see Fig. 7 for a comparison across thresholds). Determining the threshold for transient inclusion is critical to ensure that effects are accurately characterized. Thresholds should be set with consideration of the experimental paradigm. Certain pharmacological treatments may affect the frequency and amplitude of transients, therefore treated vs control recordings may produce vastly different whole session mean and SDs. In these cases, we recommend for users to define thresholds based on subsections of the recording session (e.g., baseline or intertrial interval periods). Similarly, the normalization method will affect which transients are included. Specific normalization methods can compress transient height (e.g., with systemic drug injection or stimulation recordings normalized to whole session mean and SD), making it harder for events to reach threshold compared to control treatments [24]. In these cases, normalizing to a baseline or detecting transients in \uf044F/F may be preferable. Transient threshold comparison. Comparison of transient detection with amplitude thresholds of 2, 2.6, and SDs from a single VTA dopamine GCaMP6f recording session. The subtracted and filtered data stream was Z-scored to the whole session mean and SD prior to transient detection. Note that each lower threshold includes the events detected by the higher thresholds (e.g., 2 SD includes the events detected by both 2.6 and 3 SD thresholds). A) Normalized trace plot showing detected transients for each threshold. B) Whole session transient count at each threshold. C) Mean transient amplitude (Z-score) at each threshold.","title":"Threshold Considerations"},{"location":"userguide/transientanalysis/#prepare-thresholds","text":"Prior to detecting session transients, users must prepare the thresholds to be used for event detection for each recording session. If the processed signal used for transient analysis is normalized to a Z-score, thresholds can be set in standard deviation units. If the processed signal used for transient analysis is in \u0394F or \u0394F/F, thresholds must be calculated for each session based on the standard deviation of the stream. Users must add a field to the data structure with the desired transient amplitude thresholds for each recording session.","title":"Prepare Thresholds"},{"location":"userguide/transientanalysis/#option-1-normalized-z-score-data-streams","text":"For Z-scored data, set the threshold in SD units. We recommend a threshold of 2.6 SDs.","title":"Option 1: Normalized (Z-Score) Data Streams"},{"location":"userguide/transientanalysis/#option-2-non-normalized-f-or-ff-data-streams","text":"Calculate the equivalent of 2.6 SDs (or other chosen threshold) in the units of the stream to be used for transient detection and quantification. For example, if using \u0394F/F, calculate the SD of the \u0394F/F stream and multiply it by 2.6. Code example:","title":"Option 2: Non-Normalized (\u0394F or \u0394F/F) Data Streams"},{"location":"userguide/transientanalysis/#detect-and-quantify-transients","text":"Transient detection relies on the comparison of the maximum value of each peak in the data stream to a local baseline to determine amplitude. PASTa includes three options for transient baselines: baseline minimum, baseline mean, or local minimum. For all transient detection functions, event baseline is determined by a specified window before the peak maximum location. By default, the window spans 800ms to 100ms before the peak maximum. The baseline minimum function uses the minimum value in the window as the pre-peak baseline, while the baseline mean function uses the mean of the window. The local minimum option uses the last local minimum in the baseline window as the pre-peak baseline. Peak amplitude is calculated as the maximum peak value minus the baseline value. For each peak, the amplitude is compared to the specified threshold to determine inclusion. Transients that pass the amplitude threshold are then quantified for further analyses of transient dynamics (see figure below). For each transient event, the output includes the maximum peak index, maximum peak value, pre-peak baseline index (center of the window if using the baseline mean function), pre-peak baseline value, and peak amplitude (max peak \u2013 baseline). To more finely characterize events, temporal variables for each event are determined at half height by default. Users can adjust the specified quantification height if desired. The rise phase of each transient event is characterized by the rise start index, number of samples from rise start to maximum peak, and millisecond duration of the rise period. To characterize the fall period, the function identifies the first index within 2000 milliseconds after the peak maximum that is equal to the quantification height value; however, the post peak fall window length can be adjusted if desired. The fall phase of each event is then quantified by the fall end index, number of samples from peak maximum to fall end, and millisecond duration of the fall period. To combine rise and fall dynamics, the whole transient width in samples and milliseconds are included, as well as total area under the curve (AUC). To enable analysis of transient event dynamics, the interevent interval between each transient is included in both samples and milliseconds. Finally, transients are identified as compound events if the maximum peak location falls within +/- 2000 milliseconds of another transient event. The window for compound event identification can be adjusted if desired. Default parameters for baseline window size, post peak maximum fall window, quantification height, compound event window size are included but users can override defaults if desired to customize the transient analysis. Transient detection and quantification example. All peaks (dark blue) in the data stream are identified and compared to a baseline (light blue), which can be determined by a pre-peak window mean (default), pre-peak window minimum, or the pre-peak local minimum. Amplitude (green) is calculated as the difference between the maximum peak value and the baseline, and events with amplitudes greater than the set threshold (red) are included. Transient events are quantified by amplitude (green), rise duration from half-height (pink), fall duration from half-height (purple), and half-height area under the curve (yellow).","title":"Detect and Quantify Transients"},{"location":"userguide/transientanalysis/#find-transients","text":"Use the findTransients function to detect and quantify transient events in the desired data stream. REQUIRED INPUTS: When calling the findTransients function, specify: data: The full data structure containing all session data. The data structure must include (at a minimum) fields with the stream to be analyzed for transient events, the sampling rate of the data stream, and the user determined transient threshold. addvariablesfieldnames : A cell array containing the field names of all subject metadata variables from the main data structure to add to the transient data output. For example, Subject ID, block folder name, treatment group, treatment condition, or any other experimentally relevant variables, as well as the params field with the parameters used by all other functions in the pipeline. streamfieldname: The name of the field containing the full stream to be analyzed for transient events (typically the subtracted and filtered data stream 'sigfilt' or the normalized subtracted and filtered data stream such as 'sigfiltz_normSession'). thresholdfieldname: The name of the field containing the amplitude threshold for transient inclusion. Be sure to specify the threshold in the same units as the specified stream in streamfieldname . __fsfieldname:__The name of the field containing the sampling rate of the stream (typically 'fs'). OPTIONAL INPUTS: __'bltype':__Specify the pre-transient baseline method. Available options are: 'blmean' (default): Pre-peak baselines are set to the mean value of the baseline window. 'blmin': Pre-peak baselines are set to the minimum value in the baseline window. 'localmin': Pre-peak baselines are set to the last local minimum value in the baseline window. Adjust baseline window size. By default, the baseline window is set to 1000ms to 100ms preceding the peak maximum. Users can adjust the baseline window by specifying number of milliseconds preceding the peak maximum to start and end the baseline window. 'blstartms': Set to the desired milliseconds pre-peak to adjust the start of the baseline window. 'blendms': Set to the desired milliseconds pre-peak to adjust the end of the baseline window. 'posttransientms': Adjust the post-transient fall window size in milliseconds. By default, the post-transient window to search for the fall index and value is set to 2000 ms. 'quantificationheight': Set the quantification height at which to determine the rise, fall, and area variables for each transient. By default, transient temporal dynamics are quantified at half height (0.5). Note that the input must fall between 0 and 1. Adjust the output of cut transient event data streams. By default, findTransients will output a table with the cut data streams from 5 seconds before the peak to 8 seconds after the peak for each transient to allow for plotting and additional analysis. Set optional inputs: 'outputtransientdata': Set to 0 to skip the output of cut transient data streams. 'outputpremaxS': Set to desired number of seconds pre-transient peak to include in the cut data streams. Default: 5. 'outputpostmaxS': Set to desired number of seconds post-transient peak to include in the cut data streams. Default: 8. OUTPUT: findTransients returns the separate data structure transientdata with each session as a row including the variables specified in addvariablesfieldnames , all parameters used in the analysis ( params ), the tables of quantified transient events ( transientquantification ), and the cut transient event data stream relative indexes ( transientstreamlocs) and data (transientstreamdata ). Code example: Whole session transient detection with baseline minimum and default inputs. Code example: Whole session transient detection with baseline mean and shortened pre-peak baseline window. Code example: Whole session transient detection with local minimum and 25% quantification height.","title":"Find Transients"},{"location":"userguide/transientanalysis/#transient-quantification-variables","text":"PASTa outputs multiple features for each transient event that can be quantitatively analyzed and compared. The function findTransients automatically calculate numerous variables for each transient to characterize amplitude and event duration. The following variables are included for each transient event in the transientquantification table: maxloc: The index (sample number) of the max peak location maxval: The stream value of the max peak location. Note: this variable is in the units of the data stream used to detect transient events blstartloc: The index (sample number) of the start of the pre-peak baseline window blendloc: The index (sample number) of the end of the pre-peak baseline window blloc: The index of the baseline value. _Note: if the baseline mean ('blmean') option is used, this reflects the middle of the baseline window. If the baseline minimum ('blmin') or local minimum ('localmin') options are used, this will be the exact index of the identified baseline point blval: The value of the pre-peak baseline. Note: this variable is in the units of the data stream used to detect transient events amp: The amplitude of the event from the pre-peak baseline to the max peak (maxval - blval). Note: the amplitude of all events will be at least the value of the set threshold (reccomended 2.6 SDs) quantheightval: The value of the point corresponding to the quantification height (default half-height). Note: this variable is in the units of the data stream used to detect transient events risestartloc: The index (sample number) of the start of the rise phase, measured from the peak to quantification height (default half height) risesamples: The number of samples between the start of the rise phase to the transient peak (maxloc - risestartloc) risems: The number of milliseconds from the start of the rise phase to the transient peak (risesamples / fs (sampling rate in Hz)) fallendloc: The index (sample number) of the end of the fall phase, measured from the peak to quantification height (default half height) fallsamples: The number of samples between the transient peak and the end of the fall phase (fallendloc - maxlox) fallms: The number of milliseconds from the transient peak to the end of the fall phase (fallsamples / fs (sampling rate in Hz)) widthsamples: The number of samples between the start of the rise phase and the end of the fall phase (fallendloc - risestartloc) widthms: The number of milliseconds from the start of the rise phase and the end of the fall phase (widthsamples / fs (sampling rate in Hz)) AUC: The total area under the curve from the start of the rise phase to the end of the fall phase (determined using the trapezoidal method) IEIsamples: The total number of samples from the peak of the previous event to the peak of the current event (current event maxloc - previous event max loc) IEIms: The number of milliseconds between the peak of the previous event and the peak of the current event (IEIsamples / fs (sampling rate in Hz)) IEIs: The number of seconds between the peak of the previous event and the peak of the current event (IEIsamples / fs (sampling rate in Hz) / 60) compoundeventnum: If events occur within the compound event window, this variable will include an ID starting at 1 for each event within the window. If no other events occur within the compound event window, this will be set to 0. Output Example - Adding Transients to the Data Structure: Output of findSessionTransients, which is a sub-structure under the field sessiontransients added to the main data structure. Output Example - Function Inputs: Inputs passed to the findSessionTransients functions are output to the table Inputs within the sessiontransients field of the main data structure. Output Example - Transient Quantification: Quantification of individual transient events is output to the table transientquantification within the sessiontransients field of the main data structure. Each transient is in a separate row with a unique transient ID. Output Example - Individual Transient Trace Indexes: For individual transient traces, transient stream locations of peak and baseline indexes and values are added to the table transientstreamlocs . Each transient is in a separate row. Output Example - Individual Transient Traces: Actual stream values for individual transient traces, which are cut from the start of the baseline window to the end of the post peak period, and output to the table transientstreamdata . Each transient is in a separate row, and traces are spatially aligned consistently with the transientstreamlocs for easy plotting and analysis.","title":"Transient Quantification Variables"},{"location":"userguide/transientanalysis/#bin-transients","text":"To increase the resolution of transient analysis, it may be advantageous to identify change in transient events across the session, or during specific discrete events like during or between trials. After transient events are identified, users can divide transient events into bins based on time or specific event indexes. The function binTransients will loop through each session in the data structure and add the variable 'Bin' to the transient quantification tables output by findTransients. The total number of bins will be determined for each session based on the session length and the bin length. For each bin, the start and end time sample index will be determined. All transient events within the start and end indexes will be labeled with the discrete bin number to enable analysis of transient dynamics across time. Use the binTransients function to divide transients into bins. By default, transients will be divided into 5-minute time bins. The number of bins will be determined automatically based on the whole session length. Users can override the defaults to adjust the number of minutes per bin, manually set the total number of bins, or manually pass bin start and end indexes to determine bins based on relevant time epochs. REQUIRED INPUTS: When calling the binTransients function, specify: - transientdata: The output transient data structure from the findTransients function ('transientdata'). This structure must include at a minimum the 'params.findTransients' and 'transientquantification' fields. OPTIONAL INPUTS: Adjust bin settings with optional inputs: 'binlengthmins': Adjust bin length (minutes). Set to desired number of minutes per bin. Default: 5. Note: adjusting the bin length will also change the name of the output field in the transient quantification table. For example, if 'binlengthmins' is set to 3, the output field will be named 'Bin_3mins'. This facilitates comparisons across different bin lengths if desired. 'nbinsoverride': Manually set the total number of bins. This option may be useful if there is minor variance in recording length, but users want to ensure the same number of total bins across sessions. Set to desired total number of bins. If set to anything other than 0, the override will be applied. Default: 0. Note: any events outside the final bin end index will not be assigned a bin number. 'manuallydefinebins'': Manually define bin start and end indexes, rather than defining bins based on time increments. If this option is set to TRUE or 1, users must pass additional inputs 'binstartfieldname' and 'binendfieldname' to define the beginning and end of each bin. Note: If custom bin specification is necessary, bin start and end indexes should be set up prior to calling the binTransients function. Bin start and end indexes are typically determined based on trial start and end epochs, specific events like injection, or behavioral events time locked to the recording data. 'binstartfieldname': The name of the field in data that contains the array of bin start indexes for each session. Bin start indexes should be specified in sample number. This input is required if 'manuallydefinebins' is set to TRUE or 1. 'binendfieldname': The name of the field in data that contains the array of bin end indexes for each session. Bin start indexes should be specified in sample number. This input is required if 'manuallydefinebins' is set to TRUE or 1. OUTPUT: For each session, the Bin variable will be added to the transient quantification table. The field name for Bin is specified as 'Bin_(binlengthmins)_mins'. For example, with the function default bin length of 5 minutes, the bin assignment field will be named 'Bin_5mins'. This facilitates comparisons across different bin lengths if desired. Adjusting the bin length will change the name of the output field in the transient quantification table. For example, if 'binlengthmins' is set to 3, the output field will be named 'Bin_3mins'. If the 'manuallydefinebins' optional input is utilized, the bin assignment field name will be specified as 'Bin_Custom'.","title":"Bin Transients"},{"location":"userguide/transientanalysis/#export-individual-transient-events","text":"After transients are identified, quantified, and processed, users may want to conduct statistical analysis and generate summary plots in other computational programs (e.g., R). To facilitate this transfer, users can combine transient events from each session into a single composite table with relevant subject and session level variables and output the table to a csv file. Use the exportTransients function to create a single table with individual session transient quantification tables and specified subject and session variables. REQUIRED INPUTS: When calling the exportTransients function, specify: transientdata: The output transient data structure from the findTransients function. This structure must include the 'transientquantification' field. exportfilepath: The full path to the location at which to save the output csv file. addvariablesfieldnames: A cell array containing the field names of subject and session variables to include with the transient quantification for each file. These variables will be added to every row of the output table. At a minimum, this should include the SubjectID. If multiple sessions per subject are included in the analysis, ensure a session ID variable is also included. OPTIONAL INPUTS: 'exportfilename' : Specify the file name to save the output as. By default, the file name will be generated as 'TransientQuantification_AllSessionExport_DAY-MONTH-YEAR.csv'. To manually specify the name of the output file, input the desired name as a string, ending in .csv . OUTPUT: If passed into an object, exportTransients will return the compiled table of transients for all sessions in the data structure into a table ( alltransients ). The compiled table of transients for all sessions will be saved as a .csv file under the name and file path specified.","title":"Export Individual Transient Events"},{"location":"userguide/transientanalysis/#plot-transient-events","text":"To visualize the detected transients and review for quality control, plot the data streams and overlay markers to identify the individual transient events. PASTa includes multiple options to assist in the visualization of transient events, with functions to plot the whole session data stream trace with all identified transient events (figure panel A), data stream trace with identified transient events by bin (figure panel B), overlaid individual transient event traces for the whole stream (figure panel C), and overlaid individual transient event traces by bin (figure panel D). We recommend plotting transient events as many ways as possible for quality verification and interpretation. Transient event plots. Example outputs from transient plot functions for a single fiber photometry recording session (VTA dopamine activity, GCaMP6f). The subtracted and filtered signal was Z-scored to the whole session mean and SD prior to transient detection. The threshold for transient event detection was set at 2.6 SDs. A) Whole recording session trace (green) with identified transient events (blue circles) generated by the function plotTransients . B) Recording session trace divided into five 2-minute bins with detected transients (blue circles), generated by the function plotTransientBins . C) Overlaid traces of all individual transients in the session aligned to peak maxima, generated by the function plotTransientTraces . D) Overlaid aligned transient traces grouped by session bin, generated by the function plotTransientBinTraces .","title":"Plot Transient Events"},{"location":"userguide/transientanalysis/#plot-whole-session-trace-with-transient-events","text":"Use the plotTransients function to plot the whole session data stream trace with overlaid transient event markers. REQUIRED INPUTS: When calling the function, users must specify: data: The full data structure containing all session data including all normalized streams to be plotted fileindex: The file number to plot (specify as an integer corresponding to the row in the data structure with the session to be plotted) streamfieldname: The name of the field containing the stream to be plotted. This should be the same as the stream used for transient event detection and quantification input to the findTransients function fsfieldname: The name of the field containing the sampling rate of the streams (typically 'fs' ) transientdata: The output transient data structure from the findTransients function. This structure must include at a minimum the 'transientquantification' field with peak indexes in 'maxloc' maintitle: The main title to be displayed on the figure, specified as a string OPTIONAL INPUTS: Automatically save the plot to a specified file path: 'saveoutput': Set the optional input 'saveoutput' to 1 outputfiletype: Manually specify the file type to save the figure as (default 'png' ). Available options are png, jpg, tiff, eps, and pdf 'plotfilepath': The output path for the figure including the full path from root directory to plot folder, ending in the specific filename OUTPUT: A plot of the whole session stream with transient events marked with circles (see figure panel A above).","title":"Plot Whole Session Trace with Transient Events"},{"location":"userguide/transientanalysis/#plot-session-trace-with-transient-events-by-bin","text":"Use the plotTransientBins function to plot the data stream trace with overlaid transient event markers by bin. REQUIRED INPUTS: When calling the function, users must specify: data: The full data structure containing all session data including all normalized streams to be plotted fileindex: The file number to plot (specify as an integer corresponding to the row in the data structure with the session to be plotted) streamfieldname: The name of the field containing the stream to be plotted. This should be the same as the stream used for transient event detection and quantification input to the findTransients function transientdata: The output transient data structure from the findTransients function. This structure must include at a minimum the 'transientquantification' field with peak indexes in 'maxloc' . binfieldname: The name of the field in transientdata under transientquantification that contains the bin IDs for each transient event (e.g., 'Bin_5min'). maintitle: The main title to be displayed on the figure, specified as a string. OPTIONAL INPUTS: Automatically save the plot to a specified file path: 'saveoutput': Set the optional input 'saveoutput' to 1. outputfiletype: Manually specify the file type to save the figure as (default 'png' ). Available options are png, jpg, tiff, eps, and pdf. 'plotfilepath': The output path for the figure including the full path from root directory to plot folder, ending in the specific filename. OUTPUT: A tiled plot of the session stream trace with transient events marked with circles by bin (see figure panel B above).","title":"Plot Session Trace with Transient Events by Bin"},{"location":"userguide/transientanalysis/#plot-overlaid-transient-event-traces-for-the-whole-session","text":"Use the plotTransientTraces function to plot overlaid traces for all transient events in the session. REQUIRED INPUTS: When calling the function, users must specify: transientdata: The output transient data structure from the findTransients function. This structure must include the cut data streams for each transient in the field 'transientstreamdata' . fileindex: The file number to plot (specify as an integer corresponding to the row in the data structure with the session to be plotted) maintitle: The main title to be displayed on the figure, specified as a string. OPTIONAL INPUTS: Automatically save the plot to a specified file path: 'saveoutput': Set the optional input 'saveoutput' to 1. outputfiletype: Manually specify the file type to save the figure as (default 'png' ). Available options are png, jpg, tiff, eps, and pdf. 'plotfilepath': The output path for the figure including the full path from root directory to plot folder, ending in the specific filename. OUTPUT: A plot of overlaid transient event traces from the whole session (see figure panel C above).","title":"Plot Overlaid Transient Event Traces for the Whole Session"},{"location":"userguide/transientanalysis/#plot-overlaid-transient-event-traces-by-bin","text":"Use the plotTransientTraceBins function to plot overlaid traces for transient events by bin. REQUIRED INPUTS: When calling the function, users must specify: transientdata: The output transient data structure from the findTransients function. This structure must include the cut data streams for each transient in the field 'transientstreamdata' . fileindex: The file number to plot (specify as an integer corresponding to the row in the data structure with the session to be plotted) binfieldname: The name of the field in transientdata under transientquantification that contains the bin IDs for each transient event (e.g., 'Bin_5min'). maintitle: The main title to be displayed on the figure, specified as a string. OPTIONAL INPUTS: Automatically save the plot to a specified file path: 'saveoutput': Set the optional input 'saveoutput' to 1. outputfiletype: Manually specify the file type to save the figure as (default 'png' ). Available options are png, jpg, tiff, eps, and pdf. 'plotfilepath': The output path for the figure including the full path from root directory to plot folder, ending in the specific filename. OUTPUT: A plot of overlaid transient event traces by bin (see figure panel D above).","title":"Plot Overlaid Transient Event Traces by Bin"},{"location":"userguide/transientanalysis/#summarize-transient-events","text":"After transient events have been detected and quantified, users may want to summarize transient quantification for the whole session, or by bin. Overall session values can be calculated from the transientquantification tables output by the findTransients function for each subject using the summarizeTransients and summarizeBinTransients functions.","title":"Summarize Transient Events"},{"location":"userguide/transientanalysis/#summarize-whole-session-transient-events","text":"Use the summarizeTransients function to calculate overall session values and means for transient quantification variables for each recording session in the analysis. REQUIRED INPUTS: When calling the summarizeTransients function, users must specify: transientdata: The output transient data structure from the findTransients function. This structure must include at a minimum the 'params.findTransients' and 'transientquantification' fields. OUTPUT: The function will return the transientdata structure with the added field 'transientsummary_session' containing the average transient quantification output. Included quantification variables are: freq: Total number of transient events identified in the session freqpermin: Rate of transient events per minute in the session (total number of transient events / session length in minutes) freqhz: Rate of transient events per second in the session (total number of transient events / session length in seconds) maxval: Mean of transient event peak maximum values blval : Mean of pre-transient event baseline values amp: Mean of transient event amplitudes (maxval \u2013 blval) quantheightval: Mean of transient event values at the quantification height (default half height) risesamples: Mean number of rise phase samples from quantification height to transient event peak maximum value (maxloc \u2013 risestartloc) risems: Mean rise phase length (milliseconds) from quantification height to transient event peak maximum value ((maxloc \u2013 risestartloc) / fs (sampling rate in Hz)) fallsamples: Mean number of fall phase samples from transient event peak maximum value to quantification height to (fallendloc \u2013 maxloc) fallms: Mean fall phase length (milliseconds) from quantification height to transient event peak maximum value (fallendloc \u2013 maxloc) ((maxloc \u2013 risestartloc) / fs (sampling rate in Hz)) widthsamples: Mean number of samples from quantification height at rise start to quantification height at fall end (fallendloc \u2013 risestartloc) widthms: Mean transient width length (milliseconds) at quantification height from rise start to fall end ((fallendloc \u2013 risestartloc) / fs (sampling rate in Hz)) __AUC: Mean area under the curve from rise start to fall end IEIsamples: Mean number of samples between transient event maximum peak locations (Interevent intervals; maxloc2 \u2013 maxloc1) IEIms: Mean time (milliseconds) between transient event maximum peak locations ((maxloc2 \u2013 maxloc1) / fs (sampling rate in Hz)) IEIs: Mean time (seconds) between transient event maximum peak locations ((maxloc2 \u2013 maxloc1) / (fs / 60)) compoundeventtotal: Total number of compound transient events identified in the session Note: For all transient variables reflected the actual value of the data stream (maxval, blval, and quantheightval) that this will be in the same units as the data stream input for transient detection. For example, if normalized streams are used for transient detected then values will be in Z-score, but if the non-normalized \u0394F/F is used then value will be in \u0394F/F.","title":"Summarize Whole Session Transient Events"},{"location":"userguide/transientanalysis/#summarize-transient-events-by-bin","text":"Use the summarizeBinTransients function to calculate bin values and means for transient quantification variables for each recording session in the analysis. REQUIRED INPUTS: When calling the summarizeTransients function, users must specify: transientdata: The output transient data structure from the findTransients function. This structure must include at a minimum the 'params.findTransients' and 'transientquantification' fields. binfieldname: The name of the field in transientdata under transientquantification that contains the bin IDs for each transient event (e.g., 'Bin_5min'). OUTPUT: The function will return the transientdata structure with the added field 'transientsummary ' containing the average transient quantification output by time bin. Quantification variables (see above, Summarize Whole Session Transient Events) will be returned for each bin, with the bin ID in the first column of the table under the field name specified by _binfieldname .","title":"Summarize Transient Events by Bin"},{"location":"userguide/userguide/","text":"PASTa User Guide This user guide is meant to serve as a detailed step-by-step practical guide through preparing, processing, and analyzing fiber photometry data with the PASTa protocol. Each section includes descriptions of each step, provides examples of fiber photometry recordings to show the effect on the data, and provides details on the required functions. For specific function troubleshooting, see the Function Documentation page. Quick Links PASTa includes protocols for all stages of fiber photometry analysis. Here are quick links to access each section of the user guide: Data Preparation : Data organization, metadata preparation, fiber photometry data extraction, loading data into MATLAB, and cropping of fiber photometry streams. Signal Processing : Subtraction and filtering of raw data, normalization, and visualization of fiber photometry streams. Transient Event Detection and Quantification : Detection, quantification, and visualization of transient events.","title":"User Guide"},{"location":"userguide/userguide/#pasta-user-guide","text":"This user guide is meant to serve as a detailed step-by-step practical guide through preparing, processing, and analyzing fiber photometry data with the PASTa protocol. Each section includes descriptions of each step, provides examples of fiber photometry recordings to show the effect on the data, and provides details on the required functions. For specific function troubleshooting, see the Function Documentation page.","title":"PASTa User Guide"},{"location":"userguide/userguide/#quick-links","text":"PASTa includes protocols for all stages of fiber photometry analysis. Here are quick links to access each section of the user guide: Data Preparation : Data organization, metadata preparation, fiber photometry data extraction, loading data into MATLAB, and cropping of fiber photometry streams. Signal Processing : Subtraction and filtering of raw data, normalization, and visualization of fiber photometry streams. Transient Event Detection and Quantification : Detection, quantification, and visualization of transient events.","title":"Quick Links"}]}